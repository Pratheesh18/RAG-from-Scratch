{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5984ad56",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e62490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader , PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0debfb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 164 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 201 0 (offset 0)\n",
      "Ignoring wrong pointing object 207 0 (offset 0)\n",
      "Ignoring wrong pointing object 260 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: NLP_101.pdf\n",
      " Loaded 39 pages\n",
      "\n",
      "Processing: notes.pdf\n",
      " Loaded 69 pages\n",
      "\n",
      "Total documents loaded : 108\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a direectory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # Find all PDF files recursiveley\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            #Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\" Loaded {len(documents)} pages\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" Error : {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded : {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c43f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 0, 'page_label': '1', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NATURAL LANGUAGE PROCESSING 101Sarah Rodenbeck, Lead Research Data ScientistRosen Center for Advanced Computing\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 1, 'page_label': '2', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Agenda‚Ä¢NLPPrimer‚Ä¢Mechanics & Evolution of NLP‚Ä¢Getting started with NLP\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 2, 'page_label': '3', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Primer\\n3'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 3, 'page_label': '4', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Text data is everywhere!\\n4'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 4, 'page_label': '5', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Key Idea of NLP\\n5\\nWith a sufficiently large corpus of text data, models can learn the patterns of language'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 5, 'page_label': '6', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='When have you used NLP?\\n6'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can you do with NLP?\\n7\\nCore DomainDescriptionExampleText ClassificationGrouping documents into categoriesSpam FilterInformation ExtractionIdentifying information from text Automatic calendar event creation when times mentionedInformation RetrievalFinding relevant informationSearch EnginesQuestion Answering SystemsAnswering questions based on a natural language questionSiri/AlexaMachine Translation/SummarizationConverting a sequence of text to another with the same meaningGoogle Translate\\nNatural Language GenerationGenerate new text based on a promptChat GPT'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 7, 'page_label': '8', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can‚Äôt you do with NLP?\\n8\\nLogical Reasoning'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 8, 'page_label': '9', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can‚Äôt you do with NLP?\\n9\\nNLP doesn‚Äôt truly understand language!'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 9, 'page_label': '10', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Challenges of NLP -Ambiguity\\n10\\nThe animal didn‚Äôt cross the street because itwas too tired.------versus ------The animal didn‚Äôt cross the street because itwas too wide.\\nI ranto the store because we ranout of bread.Can I runsomething past you?That house is really rundown.\\nI lovetaking tests \\nüôÑ'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 10, 'page_label': '11', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Challenges of NLP -Language\\n11'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 11, 'page_label': '12', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Ethics\\n12'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 12, 'page_label': '13', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content=\"NLP Ethics\\n13\\n¬°Have our algorithms been tested on diverse data?¬°Are our algorithms equally performant on all groups?\\nAccountability¬°How are we holding ourselves accountable if AI makes a mistake?¬°What recourse is available and how do we ensure the issue doesn't happen again?\\nFairness\"),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 13, 'page_label': '14', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content=\"NLP Ethics\\n14\\nTransparency\\nEthics\\n¬°Are we transparent about how we're using AI?¬°Do we allow outside researchers or watchdogs to examine our use of AI?¬°Are the applications we're using AI for ethical?\"),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 14, 'page_label': '15', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Mechanics & Evolution\\n15'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 15, 'page_label': '16', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Quick Review of Key ML Terms\\n16\\n‚Ä¢Unsupervised Learning: neural network used patterns in unlabeled data, e.g., clustering‚Ä¢SupervisedLearning:Labelleddatausedtohelpthemodel‚Äúlearn‚Äùhowtodoaparticulartask, e.g., classification‚Ä¢Transferlearning:Reusinggeneralinformationlearned from a previous task for a new task; speeds up training and reduces data requirements‚Ä¢Pre-training:Generallearning‚Ä¢Fine-tuning: Tweaking the pre-trained model for a downstream task'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 16, 'page_label': '17', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='EvolutionofNLP\\n17\\n1950s-1990sSymbolic NLPStatistical NLPNeural NLP1990s-20102010s-Present\\n‚Ä¢Expert rule-based systems hand-coded by linguists‚Ä¢Keyachievements:‚Ä¢Georgetown Experiment‚Ä¢ELIZA\\n‚Ä¢Use similarities between words to compete tasks‚Ä¢Key achievements:‚Ä¢Statistical Machine Translation‚Ä¢Latent Semantic Indexing/TFIDF‚Ä¢FirstuseofNNforlanguagemodelling\\n‚Ä¢Rapid advancement in NLP thanks to more data and hardware‚Ä¢Keyachievements:‚Ä¢Word embeddings‚Ä¢AttentionandTransformer‚Ä¢Large language models'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 17, 'page_label': '18', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n18\\nEmbedding ideally captures:‚Ä¢Meaning of words‚Ä¢Similarities/differences between words‚Ä¢Contextual meaning of words'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 18, 'page_label': '19', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n19\\n‚ÄúYou shall know a word by the company it keeps‚Äù ‚Ä¢A word‚Äôs meaning can be understood based on the words it frequently appears close to‚Ä¢Use the many contexts of a word to build up its representation'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 19, 'page_label': '20', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n20\\nHow are embeddings actually created?‚Ä¢Unsupervised training on large corpus of text‚Ä¢Randomly initialized vectors for each word in corpus‚Ä¢Train to maximize similarity (dot product) of target and context word vectors (Word2Vec)‚Ä¢Addglobal statistics about corpus (co-occurrence probabilities) to improve embeddings (GloVe)'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 20, 'page_label': '21', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n21\\nWord2Vec/GloVeEmbeddings Capture:√ºMeaning of words√ºSimilarities/differences between wordsqContextual meaning of words'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 21, 'page_label': '22', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n22\\n‚ÄúAfter stealing money from the bank vault, the bank robberwas seen fishing on the Mississippi river bank.‚Äù\\nHow are conditionalembeddings actually created?‚Ä¢Unsupervised pre-training on large corpus of text‚Ä¢Runpre-processedtextthroughthepre-trained model to dynamically generate embeddings for each word √†‚Äúfine-tuning‚Äù the embeddings‚Ä¢ELMo/BERT/other conditional embeddings satisfy all ofour requirements!\\nEach use of ‚Äúbank‚Äù has a different embedding'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 22, 'page_label': '23', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n23\\nAttributes of text data:‚Ä¢Word orderencodesmeaning‚Ä¢The most relevant information for understanding a word may be near or far away‚Ä¢Words havedifferentialimportance\\nThe animal didn‚Äôt cross the street because itwas too tired.------versus ------The animal didn‚Äôt cross the street because itwas too wide.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 23, 'page_label': '24', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n24\\nRecurrent Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 24, 'page_label': '25', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n25\\n1stattempt: RNNs and LSTMs Key Features:√ºWord order encodes meaning¬ßThe most relevant information for understanding a word may be near or far awayqWords have differential importance\\nLong Short-Term Memory'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 25, 'page_label': '26', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n26\\nAttention\\nTo predict a word, use only the most relevant parts of the input text'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 26, 'page_label': '27', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n27\\nAttentionSelf-attention: relating different positions of a single sequence to itself to compute attentionlProcesses each word in the input one at a time (query) by looking at all other words in the input sequence (keys) for clues that can help the model learn a better encoding for the query (values)'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 27, 'page_label': '28', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n28\\n2ndAttempt:Transformers‚Ä¢‚ÄùAttentionisallyouNeed‚Äù:SeminalNLPpaperthat presented SOTA results by only using attention mechanisms withoutrecurrence‚Ä¢Basis ofBERTandGPTSOTAmodels‚Ä¢Manytimesfasterandparallelizable‚Ä¢Addressestheissueofdifferentialimportance'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 28, 'page_label': '29', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n29\\nBERT Model‚Ä¢Bidirectional Encoder Representations from Transformers‚Ä¢Uses both left and right context for training (bi-directional)‚Ä¢Language representation model (pre-trained) that can be fine-tuned for a variety of NLP tasks‚Ä¢Basedontransformer architecture'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 29, 'page_label': '30', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n30\\nGPT‚Ä¢Generative Pre-trained Transformer‚Ä¢Uni-directional‚Ä¢Draws from corpus of information to generate best results for query‚Ä¢Based ontransformer architecture'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 30, 'page_label': '31', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n31\\nBERT                          vs                          GPTPros‚Ä¢SuitableforawiderangeofNLPtasks‚Ä¢Canbeadaptedtoaspecificdomain/taskandcanlearnnewinformationthroughfine-tuning‚Ä¢Open-source modelCons‚Ä¢Requires more effort to develop a model\\nPros‚Ä¢Suitable for a wide range of tasks‚Ä¢Lower barriertoentrybecause nofine-tuning required‚Ä¢TrainedonmassivecorpusofinformationCons‚Ä¢Cannot be fine-tuned or learn anything new'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 31, 'page_label': '32', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Review: Key Terms\\n32\\n‚Ä¢Embedding: way to numerically represent the meaning of a word, sentence, paragraph, etc. ‚Ä¢Language Model: probabilistic model of words and phrases in a language‚Ä¢Transformers: Architecture based on attention mechanisms‚Ä¢Representation Learning: Based on pattern discovery‚Ä¢Generative AI: Utilizes knowledge to generate data/information'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 32, 'page_label': '33', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n33'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 33, 'page_label': '34', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='GettingStarted\\n34\\n‚Ä¢User-friendly resource to help you get started with NLP‚Ä¢Transformers python package‚Ä¢Models/datasets for variety of different tasks'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 34, 'page_label': '35', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n35\\nGLUE Benchmark includes many tasks to assess general language understanding‚Ä¢LinguisticAcceptability‚Ä¢Paraphrasing‚Ä¢Semantic Similarity‚Ä¢Question-Answering‚Ä¢Sentiment‚Ä¢And more!'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 35, 'page_label': '36', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n36\\nNLP in 3 easy steps\\nHuggingFaceTutorial: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb\\nLoad pre-trained model and dataTokenize and pre-process dataFine-tune model and save checkpoint'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Practical Tips¬ßNewer NLP approaches generally don‚Äôt require much manual preprocessing (e.g.older methods like stop word removal and  stemming/lemmatization are not usually needed). ¬ßThere are a lot of specialized pre-trained models out there (e.g., BERT pre-trained models available for twitter data, medical data, etc.) that are an easy win in boosting performance ‚Äìresearch and experiment whenever possibleGeneral ML:¬ßAs withanyML‚Äìgarbage in, garbage out! Take the time to ensure sufficient data quality¬ßThe ‚Äúbest‚Äù new model may not be the best for you ‚Äìkeep in mind the benefits of using a more established model with more support and try these first ¬ßGPUs not strictly necessary if you are only fine-tuning or doing inference, but will definitely speedup tasks\\n37'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 37, 'page_label': '38', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Resources‚Ä¢Stanford CS224N NLP with Deep Learning Course: https://youtu.be/rmVRLeJRkl4‚Ä¢Variety of excellent explainers on key concepts/architectures: https://jalammar.github.io/\\n38'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 38, 'page_label': '39', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='THANK YOUContact: rcac-help@purdue.edu\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 0, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Introduction to Machine Learning Class Notes\\nHuy Nguyen\\nPhD Student, Human-Computer Interaction Institute\\nCarnegie Mellon University'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Contents\\nPreface 3\\n1 MLE and MAP 4\\n1.1 MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Bayesian learning and MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Nonparametric models: KNN and kernel regression 7\\n2.1 Bayes decision rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.3 K-nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.4 Local Kernel Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3 Linear Regression 11\\n3.1 Basic linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.2 Multivariate and general linear regression . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3 Regularized least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3.2 Connection to MLE and MAP . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Logistic Regression 16\\n4.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.2 Training logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n5 Naive Bayes ClassiÔ¨Åer 20\\n5.1 Gaussian Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n5.2 Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.3 Text classiÔ¨Åcation: bag of words model . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.4 Generative vs Discriminative Classifer . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6 Neural Networks and Deep Learning 25\\n6.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n6.2 Training a neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6.2.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6.2.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n6.3 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n6.3.1 Convolutional Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n6.3.2 Pooling layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n6.3.3 Fully Connected Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7 Support Vector Machine 33\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n7.2 Primal form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2.1 Linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2.2 Non linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3 Dual representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3.1 Linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3.2 Transformation of inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n7.3.3 Kernel tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n7.3.4 Non linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4 Other topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4.1 Why do SVMs work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4.2 Multi-class classiÔ¨Åcation with SVM . . . . . . . . . . . . . . . . . . . . . 41\\n8 Ensemble Methods and Boosting 42\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n8.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.2.1 What Œ±t to choose for hypothesis ht? . . . . . . . . . . . . . . . . . . . . 45\\n8.2.2 Show that training error converges to 0 . . . . . . . . . . . . . . . . . . . 46\\n9 Principal Component Analysis 49\\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n9.2 PCA algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n9.3 PCA applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n9.3.1 Eigenfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n9.3.2 Image compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n9.4 Shortcomings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n10 Hidden Markov Model 54\\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n10.2 Inference in HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n10.2.1 What is P (qt =si)? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n10.2.2 What is P (qt =si |o1o2...o t)? . . . . . . . . . . . . . . . . . . . . . . . 57\\n10.2.3 What is arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t)? . . . . . . . . . . . . . . . . . 57\\n11 Reinforcement Learning 59\\n11.1 Markov decision process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n11.2 Reinforcement learning - No action . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n11.2.1 Supervised RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n11.2.2 Certainty-Equivalence learning . . . . . . . . . . . . . . . . . . . . . . . . 62\\n11.2.3 Temporal diÔ¨Äerence learning . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n11.3 Reinforcement learning with action - Policy learning . . . . . . . . . . . . . . . . 64\\n12 Generalization and Model Selection 66\\n12.1 True risk vs Empirical risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n12.2 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 3, 'page_label': '3', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Preface\\nThis is the class notes I took for CMU‚Äôs 10701: Introduction to Machine Learning in Fall 2018.\\nThe goal of this document is to serve as a quick review of key points from each topic covered in\\nthe course. A more comprehensive note collection for beginners is available at UPenn‚Äôs CIS520:\\nMachine Learning.\\nIn this document, each chapter typically covers one machine learning methodology and contains\\nthe followings:\\n‚Ä¢ DeÔ¨Ånition - deÔ¨Ånition of important concepts.\\n‚Ä¢ Diving in the Math - mathematical proof for a statement / formula.\\n‚Ä¢ Algorithm - the steps to perform a common routine / subroutine.\\nIntertwined with these components are transitional text (as I Ô¨Ånd them easier to review than\\nbullet points), so the document as a whole ends up looking like a mini textbook. While there\\nare already plenty of ML textbooks out there, I am still interested in writing up something\\nthat stays closest to the content taught by Professor Ziv Bar-Joseph and Pradeep Ravikumar.\\nI would also like to take this opportunity to thank the two professors for their guidance.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 1\\nMLE and MAP\\n1.1 MLE\\nDeÔ¨Ånition 1: (Likelihood function and MLE)\\nGivenn data points x1,x 2,...,x n we can deÔ¨Åne the likelihood of the data given the model\\nŒ∏ (usually a collection of parameters) as follows.\\nÀÜP (dataset |Œ∏) =\\nn‚àè\\nk=1\\nÀÜP (xk |Œ∏). (1.1)\\nThe maximum likelihood estimate (MLE) of Œ∏ is\\nÀÜŒ∏MLE = arg max\\nŒ∏\\nÀÜP (dataset |Œ∏). (1.2)\\nTo determine the values for the parameters in Œ∏, we maximize the probability of generating the\\nobserved samples. For example, let Œ∏ be the model of a coin Ô¨Çip (so Œ∏ = {P (Head) =q}), then\\nthe best assignment (MLE) for Œ∏ in this case is ÀÜŒ∏ =\\n{\\nÀÜq = # heads\\n# samples\\n}\\n.\\nDiving in the Math 1 - MLE for binary variable\\nFor a binary random variable A with P (A = 1) =q, we show that ÀÜq = # 1\\n# samples .\\nAssume we observe n samples x1,x 2,...,x n with n1 heads and n2 tails.\\nThen, the likelihood function is\\nP (D |Œ∏) =\\nn‚àè\\ni=1\\nP (xi |Œ∏) =qn1(1 ‚àíq)n2.\\nWe now Ô¨Ånd ÀÜq that maximizes this likelihood function, i.e., ÀÜq = arg max\\nq\\nqn1(1 ‚àíq)n2.\\nTo do so, we set the derivative to 0:\\n0 = ‚àÇ\\n‚àÇqqn1(1 ‚àíq)n2 =n1qn1‚àí1(1 ‚àíq)n2 ‚àíqn1n2(1 ‚àíq)n2‚àí1,\\nwhich is equivalent to\\nqn1‚àí1(1 ‚àíq)n2‚àí1(n1(1 ‚àíq) ‚àíqn2) = 0,\\nwhich yields\\nn1(1 ‚àíq) ‚àíqn2 = 0 ‚áî q = n1\\nn1 +n2\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 5, 'page_label': '5', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='When working with products, probabilities of entire datasets often get too small. A possible\\nsolution is to use the log of probabilities, often termed log likelihood 1:\\nlog ÀÜP (dataset |M) = log\\nn‚àè\\nk=1\\nÀÜP (xk |M) =\\nn‚àë\\nk=1\\nlog ÀÜP (xk |M). (1.3)\\nIn this case, the algorithm for MLE is as follows.\\nAlgorithm 1: (Finding the MLE)\\nGivenn data points x1,x 2,...,x n and a model Œ∏ represented by an expression for P (X |Œ∏),\\nperform the following steps:\\n1. Compute the log-likelihood\\nL = log\\nn‚àè\\ni=1\\nP (xi |Œ∏) =\\nn‚àë\\ni=1\\nlogP (xi |Œ∏).\\n2. For each parameter Œ≥ in Œ∏, Ô¨Ånd the solution(s) to the equation ‚àÇL\\n‚àÇŒ≥ = 0.\\n3. The solution ÀÜŒ≥ that satisÔ¨Åes ‚àÇ2L\\n‚àÇÀÜŒ≥2 ‚â§ 0 is the MLE of Œ≥.\\n1.2 Bayesian learning and MAP\\nWe Ô¨Årst note the Bayes formula\\nP (A |B) = P (B |A)P (A)\\nP (B) = P (B |A)P (A)‚àë\\nA\\nP (B |A)P (A)\\n. (1.4)\\nIn Bayesian learning, prior information is encoded as a distribution over possible values of\\nparameters P (M). Using (1.4), we get an updated posterior distribution over parameters. To\\nderive the estimate of true parameter, we choose the value that maximizes posterior probability.\\nDeÔ¨Ånition 2: (MAP)\\nGiven a dataset and a modelM with priorP (M), the maximum a posteriori (MAP) estimate\\nof M is\\nÀÜŒ∏MAP = arg max\\nŒ∏\\nP (Œ∏ | dataset) = arg max\\nŒ∏\\nP (dataset |Œ∏)P (Œ∏). (1.5)\\n1Note that because log t is monotonous on R, maximizing log t is the same as maximizing t.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 6, 'page_label': '6', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='If we only have very few samples, MLE may not yield accurate results, so it is useful to take into\\naccount prior knowledge. When the number of samples gets large, the eÔ¨Äect of prior knowledge\\nwill diminish.\\nSimilar to MLE, we have the following algorithm for MAP.\\nAlgorithm 2: (Finding the MAP)\\nGivenn data pointsx1,x 2,...,x n, a model Œ∏ represented by an expression forP (X |Œ∏), and\\nthe prior knowledge P (Œ∏), perform the following steps:\\n1. Compute the log-likelihood\\nL = logP (Œ∏) ¬∑\\nn‚àè\\ni=1\\nP (xi |Œ∏) = logP (Œ∏) +\\nn‚àë\\ni=1\\nlogP (xi |Œ∏).\\n2. For each parameter Œ≥ in Œ∏, Ô¨Ånd the solution(s) to the equation ‚àÇL\\n‚àÇŒ≥ = 0.\\n3. The solution ÀÜŒ≥ that satisÔ¨Åes ‚àÇ2L\\n‚àÇÀÜŒ≥2 ‚â§ 0 is the MAP of Œ≥.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 2\\nNonparametric models: KNN and\\nkernel regression\\n2.1 Bayes decision rule\\nClassiÔ¨Åcation is the task of predicting a (discrete) output label given the input data. The\\nperformance of any classiÔ¨Åcation algorithm depends on two factors: (1) the parameters are\\ncorrect, and (2) the underlying assumption holds. The most optimal algorithm is called the\\nBayes decision rule.\\nAlgorithm 3: (Bayes decision rule)\\nIf we know the conditional probability P (x |y) and class prior P (y), use (1.4) to compute\\nP (y =i |x) = P (x |y =i)P (y =i)\\nP (x) ‚àùP (x |y =i)P (y =i) =qi(x) (2.1)\\nand qi(x) to select the appropriate class. Choose class 0 if q0(x) > q1(x) and 1 otherwise.\\nIn general choose the class ÀÜc = arg max\\nc\\n{qc(x)}.\\nBecause our decision is probabilistic, there is still chance for error. The Bayes error rate (risk)\\nof the data distribution is the probability an instance is misclassiÔ¨Åed by the Bayes decision rule.\\nFor binary classiÔ¨Åcation, the risk for sample x is\\nR(x) = min{P (y = 0 |x),P (y = 1 |x)}. (2.2)\\nIn other words, if P (y = 0 |x)> P(y = 1 |x), then we would pick the label 0, and the risk is\\nthe probability that the actual label is 1, which is P (y = 1 |x).\\nWe can also compute the expected risk - the risk for the entire range of values of x:\\nE[r(x)] =\\n‚à´\\nx\\nr(x)P (x)dx\\n=\\n‚à´\\nx\\nmin{P (y = 0 |x),P (y = 1 |x)}dx\\n=P (y = 0)\\n‚à´\\nL1\\nP (x |y = 0)dx +P (y = 1)\\n‚à´\\nL0\\nP (x |y = 1)dx,\\nwhere Li is the region over which the decision rule outputs label i.\\nThe risk value we computed assumes that both errors (assigning instances of class 1 to 0 and\\nvice versa) are equally harmful. In general, we can set the weight penalty Li,j(x) for assigning\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='instances of class i to class j. This gives us the concept of a loss function\\nE[L] =L0,1(x)P (y = 0)\\n‚à´\\nL1\\nP (x |y = 0)dx +L1,0(x)P (y = 1)\\n‚à´\\nL0\\nP (x |y = 1)dx. (2.3)\\n2.2 ClassiÔ¨Åcation\\nThere are roughly three types of classiÔ¨Åers:\\n1. Instance based classiÔ¨Åers : use observation directly (no models). Example: K nearest\\nneighbor.\\n2. Generative: build a generative statistical model. Example: Naive Bayes.\\n3. Discriminative: directly estimate a decision rule/boundary. Example: decision tree.\\nThe classiÔ¨Åcation task itself contains several steps:\\n1. Feature transformation: e.g, how do we encode a picture?\\n2. Model / classiÔ¨Åer speciÔ¨Åcation : What type of classiÔ¨Åer to use?\\n3. Model / classiÔ¨Åer estimation (with regularization): How do we learn the parameters\\nof our classiÔ¨Åer? Do we have enough examples to learn a good model?\\n4. Feature selection: Do we really need all the features? Can we use a smaller number and\\nstill achieve the same (or better) results?\\nClassiÔ¨Åcation is one of the key components ofsupervised learning, where we provide the algorithm\\nwith labels to some of the instances and the goal is to generalize so that a model / method can\\nbe used to determine the labels of the unobserved examples.\\n2.3 K-nearest neighbors\\nA simple yet surprisingly eÔ¨Écient algorithm is K nearest neighbors.\\nAlgorithm 4: (K-nearest neighbors)\\nGivenn data points, a distance function d and a new point x to classify, select the class of\\nx based on the majority vote in the K closest points.\\nNote that this requires the deÔ¨Ånition of a distance function or similarity measure between sam-\\nples. We also need to determine K beforehand. Larger K means the resulting classiÔ¨Åer is more\\n‚Äòsmooth‚Äô (but smoothness is primarily dependent on the actual distribution of the data).\\nFrom a probabilistic view, KNN tries to approximate the Bayes decision rule on a subset of data.\\nWe compute P (x |y),P (y) and P (x) for some small region around our sample, and the size of\\nthat region will be dependent on the distribution of the test sample.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 2 - Computing probabilities for KNN\\nLet z be the new point we want to classify. Let V be the volume of the m dimensional ball\\nR around z containing the K nearest neighbors for z (where m is the number of features).\\nAlso assume that the distribution in R is uniform.\\nConsider the probability P that a data point chosen at random is in R. On one hand,\\nbecause there are K points in R out of a total of N points, P = K\\nN . On the other hand, let\\nP (x) =q be the density at a pointx ‚àà R (q is constant because R has uniform distribution).\\nThen P =\\n‚à´\\nx‚ààR\\nP (x)dx =qV . Hence we see that the marginal probability of z is\\nP (z) =q = P\\nV = K\\nNV .\\nSimilarly, the conditional probability of z given a class i is\\nP (z |y =i) = Ki\\nNiV .\\nFinally, we compute the prior of class i:\\nP (y =i) = Ni\\nN.\\nUsing Bayes formula:\\nP (y =i |z) = P (z |y =i)P (y =i)\\nP (z) = Ki\\nK.\\nUsing the Bayes decision rule we will choose the class with the highest probability, which\\ncorresponds to the class with the highest Ki - the number of samples in K.\\n2.4 Local Kernel Regression\\nKernel regression is similar to KNN but used for regression. In particular, it focuses on a speciÔ¨Åc\\nregion of the input, as opposed to the global space (like linear regression does). For example,\\nwe can output the local average:\\nÀÜf(x) =\\n‚àën\\ni=1yi ¬∑ I (‚à•xi ‚àíx‚à• ‚â§ h)‚àën\\ni=1 I (‚à•xi ‚àíx‚à• ‚â§ h) , (2.4)\\nwhich can also be expressed in a form similar to linear regression:\\nÀÜf(x) =\\nn‚àë\\ni=1\\nwiyi, w i = I (‚à•xi ‚àíx‚à• ‚â§ h)\\nI\\n(‚àën\\nj=1 ‚à•xj ‚àíx‚à• ‚â§ h\\n).\\nNote that the wi‚Äôs here represent a hard boundary: if Xi is close to x then wi = 1, else wi = 0.\\nIn the general case, w can be expressed using kernel functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 10, 'page_label': '10', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 5: (Nadaraya-Watson Kernel Regression)\\nGivenn data points {(xi,yi)}n\\ni=1, we can output the value at a new point x as\\nÀÜf(x) =\\nn‚àë\\ni=1\\nwiyi, w i = K\\n(‚èê‚èêx‚àíxi\\nh\\n‚èê‚èê)\\n‚àën\\nj=1K\\n(‚èê‚èêx‚àíxi\\nh\\n‚èê‚èê), (2.5)\\nwhere K is a kernel function. Some typical kernel functions include:\\n‚Ä¢ Boxcar kernel: K(t) = I (t ‚â§ 1).\\n‚Ä¢ Gaussian kernel: K(t) = 1‚àö\\n2œÄ exp\\n(\\n‚àít2\\n2\\n)\\n.\\nThe distance h in this case the called the kernel bandwidth . The choice of h should depend\\non the number of training data (determines variance) and smoothness of function (determines\\nbias).\\n‚Ä¢ Large bandwidth averages more data points so reduces noice (lower variance).\\n‚Ä¢ Small bandwidth Ô¨Åts more accurately (lower bias).\\nIn general this is the bias-variance tradeoÔ¨Ä. Bias represents how accurate the result is (lower\\nbias = more accurate). Variance represents how sensitive the algorithm is to changes in the\\ninput (lower variance = less sensitive). Here a large bandwidth ( h = 200) yields low variance\\nand high bias, while a small bandwidth ( h = 1) yields high variance and low bias. In this case,\\nh = 50 seems like the best middle ground.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 11, 'page_label': '11', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 3\\nLinear Regression\\n3.1 Basic linear regression\\nDeÔ¨Ånition 3: (Linear Regression)\\nGiven an input x we would like to compute an output y as\\ny =wx +œµ,\\nwhere w is a parameter and œµ represents measurement of noise.\\nOur goal is to estimate w from training data of (xi,yi) pairs. One way is to Ô¨Ånd the least square\\nerror (LSE)\\nÀÜwLR = arg min\\nw\\n‚àë\\ni\\n(yi ‚àíwxi)2 (3.1)\\nwhich minimizes squared distance between measurements and predicted lines. LSE has a nice\\nprobabilistic interpretation (as we will see shortly, if œµ ‚àº N (0,œÉ 2) then ÀÜw is MLE of w). and is\\neasy to compute. In particular, the solution to (3.1) is\\nÀÜw =\\n‚àë\\nixiyi‚àë\\nix2\\ni\\n. (3.2)\\nDiving in the Math 3 - Solving linear regression using LSE\\nWe take the derivative w.r.t w and set to 0:\\n0 = ‚àÇ\\n‚àÇw\\n‚àë\\ni\\n(yi ‚àíwxi)2 = ‚àí2\\n‚àë\\ni\\nxi(yi ‚àíwxi),\\nwhich yields\\n‚àë\\ni\\nxiyi =\\n‚àë\\ni\\nwx2\\ni ‚áí w =\\n‚àë\\nixiyi‚àë\\nix2\\ni\\nIf the line does not pass through the origin, simply change the model to\\ny =w0 +w1x +œµ,\\nand following the same process gives\\nw0 =\\n‚àë\\niyi ‚àíw1xi\\nn , w 1 =\\n‚àë\\nixi(yi ‚àíw0)‚àë\\nix2\\ni\\n. (3.3)\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='3.2 Multivariate and general linear regression\\nIf we have several inputs, this becomes a multivariate regression problem:\\ny =w0 +w1x1 +... +wkxk +œµ.\\nHowever, not all functions can be approximated using the input values directly. In some cases we\\nwould like to use polynomial or other terms based on the input data. As long as the coeÔ¨Écients\\nare linear, the equation is still a linear regression problem. For instance,\\ny =w0x1 +w1x2\\n1 +... +wkx2\\nk +œµ.\\nTypical non-linear basis functions include:\\n‚Ä¢ PolynomialœÜj(x) =xj,\\n‚Ä¢ Gaussian œÜj(x) = (x‚àí¬µj)2\\n2œÉ2\\nj\\n,\\n‚Ä¢ Sigmoid œÜj(x) = 1\\n1+exp(‚àísjx).\\nUsing this new notation, we formulate the general linear regression problem:\\ny =\\n‚àë\\nj\\nwjœÜj(x),\\nwhereœÜj(x) can either bexj for multivariate regression or one of the non-linear bases we deÔ¨Åned.\\nNow assume the general case where we where have n data points\\n(x(1),y (1)), (x(2),y (2)),..., (x(n),y (n)), and each data point has k features (recall that feature j of\\nx(i) is denoted x(i)\\nj ). Again using LSE to Ô¨Ånd the optimal solution, by deÔ¨Åning\\nŒ¶ =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nœÜ0(x(1)) œÜ1(x(1)) ... œÜ k(x(1))\\nœÜ0(x(2)) œÜ1(x(2)) ... œÜ k(x(2))\\n... ... ... ...\\nœÜ0(x(n)) œÜ1(x(n)) ... œÜ k(x(n))\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n‚Äî œÜ(x(1))T ‚Äî\\n‚Äî œÜ(x(2))T ‚Äî\\n...\\n‚Äî œÜ(x(n))T ‚Äî\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8, (3.4)\\nwe then get\\nw = (Œ¶T Œ¶)‚àí1Œ¶Ty. (3.5)\\nDiving in the Math 4 - LSE for general linear regression problem\\nOur goal is to minimize the following loss function:\\nJ(w) =\\n‚àë\\ni\\n(y(i) ‚àí\\n‚àë\\nj\\nwjœÜj(x(i)))2 =\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))2,\\nwhere w and œÜ(x(i)) are vectors of dimension k + 1 and y(i) is a scalar.\\nSetting the derivative w.r.t w to 0:\\n0 = ‚àÇ\\n‚àÇw\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))2 = 2\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))œÜ(x(i))T,\\nwhich yields ‚àë\\ni\\ny(i)œÜ(x(i))T =wT\\n‚àë\\ni\\nœÜ(x(i))œÜ(x(i))T.\\nHence, deÔ¨Åning Œ¶ as in (3.4) would give us\\n(Œ¶T Œ¶)w = Œ¶Ty ‚áí w = (Œ¶T Œ¶)‚àí1Œ¶Ty\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='To sum up, we have the following algorithm for the general linear regression problem.\\nAlgorithm 6: (General linear regression algorithm)\\nInput: Given n input data {(x(i),y (i))}n\\ni=1 where x(i) is 1 √óm and y(i) is scalar, as well as\\nm basis functions {œÜj}m\\nj=1, we Ô¨Ånd\\nÀÜw = arg min\\nw\\nn‚àë\\ni=1\\n(y(i) ‚àíwTœÜ(x(i)))2\\nby the following procedure:\\n1. Compute Œ¶ as in (3.4).\\n2. Output ÀÜw = (Œ¶T Œ¶)‚àí1Œ¶Ty.\\n3.3 Regularized least squares\\n3.3.1 DeÔ¨Ånition\\nIn the previous chapter we see that a linear regression problem involves solving (Œ¶ T Œ¶)w = Œ¶Ty\\nfor w. If Œ¶T Œ¶ is invertible, we would get w = (Œ¶T Œ¶)‚àí1Œ¶Ty as in (3.5). Now what if Œ¶ T Œ¶ is not\\ninvertible?\\nRecall that full rank matrices are invertible, and that\\nrank(Œ¶T Œ¶) = the number of non-zero eigenvalues of Œ¶ T Œ¶\\n‚â§ min(n,k ) since Œ¶ is n √ók\\nIn other words, Œ¶T Œ¶ is not invertible ifn<k , i.e., there are more features than data point. More\\nspeciÔ¨Åcally, we haven equations andk >n unknowns - this is an undetermined system of linear\\nequations with many feasible solutions. In that case, the solution needs to be further constrained.\\nOne way, for example, is Ridge Regression - using L2 norm as penalty to bias the solution to\\n‚Äúsmall‚Äù values of w (so that small changes in input don‚Äôt translate to large changes in output):\\nÀÜwRidge = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•2\\n2\\n= arg min\\nw\\n(Œ¶w ‚àíy)T (Œ¶w ‚àíy) +Œª ‚à•w‚à•2\\n2, Œª ‚â• 0\\n= (Œ¶T Œ¶ +ŒªI)‚àí1Œ¶Ty. (3.6)\\nWe could also use Lasso Regression (L1 penalty)\\nÀÜwLasso = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•1, (3.7)\\nwhich biases towards many parameter values being zero - in other words, many inputs become\\nirrelevant to prediction in high-dimensional settings. There is no closed form solution for (3.7),\\nbut it can be optimized by sub-gradient descent.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 14, 'page_label': '14', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In general, we can phrase the problem as Ô¨Ånding\\nÀÜw = arg min\\nw\\n(Œ¶w ‚àíy)T (Œ¶w ‚àíy) +Œªpen(w), (3.8)\\nwhere pen(w) is a penalty function. Here‚Äôs a visualization for diÔ¨Äerent kinds of penalty functions:\\n3.3.2 Connection to MLE and MAP\\nConsider a linear regression problem\\nY = ÀÜf(X) +œµ =X ÀÜw +œµ,\\nwhere the noise œµ ‚àº N (0,œÉ 2I), which implies Y ‚àº N (X ÀÜw,œÉ 2I). If Œ¶ T Œ¶ is invertible, then w\\ncan be determined exactly by MCLE:\\nÀÜwMCLE = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 = ÀÜwLR, (3.9)\\nwhere the last equality follows from (3.1). In other words, Least Square Estimate is the same as\\nMCLE under a Gaussian model .\\nIn case Œ¶ T Œ¶ is not invertible, we can encode the Ridge bias by letting w ‚àº N (0,œÑ 2I) and\\nP (w) ‚àù exp(‚àíwTw/2œÑ 2), which would yield\\nÀÜwMCAP = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•2\\n2 = ÀÜwRidge, (3.10)\\nwhere Œª is constant in terms of œÉ2 and œÑ 2, and the last equality follows from (3.6). In other\\nwords, Prior belief that w is Gaussian with mean 0 biases solution to ‚Äúsmall‚Äù w.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 15, 'page_label': '15', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 5 - Ridge regression and MCAP\\nSince we are given P (w) ‚àù exp(‚àíwTw/2œÑ 2), let P (w) = exp( ‚àíc ‚à•w‚à•2\\n2), where c is some\\nconstant, then ‚àí logP (w) =c ‚à•w‚à•2\\n2, so (3.10) is equivalent to Ô¨Ånding\\ninf\\nw\\n{L(w) +c ‚à•w‚à•2\\n2}\\n= inf\\nw\\n{L(w)} such that ‚à•w‚à•2\\n2 ‚â§L(c),\\nwhere L(c) is a bijective function of c. So adding c ‚à•w‚à•2\\n2 is the same as the ridge regression\\nconstraint ‚à•w‚à•2\\n2 ‚â§L for some constant L.\\nSimilarly, we can encode the Lasso bias by letting wi ‚àº Laplace(0,t ) (iid) and P (wi) ‚àù\\nexp(‚àí|wi|/t), which would yield\\nÀÜwMCAP = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•1 = ÀÜwLasso, (3.11)\\nwhereŒª is constant in terms ofœÉ2 andt, and the last equality follows from (3.7). In other words,\\nPrior belief that w is Laplace with mean 0 biases solution to ‚Äúsparse‚Äù w.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 16, 'page_label': '16', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 4\\nLogistic Regression\\n4.1 DeÔ¨Ånition\\nWe know that regression is for predicting real-valued output Y , while classiÔ¨Åcation is for pre-\\ndicting (Ô¨Ånite) discrete-valuedY . But is there a way to connect regression to classiÔ¨Åcation? Can\\nwe predict the ‚Äúprobability‚Äù of a class label? The answer is generally yes, but we have to keep\\nin mind the constraint that the probability value should lie in [0 , 1].\\nDeÔ¨Ånition 4: (Logistic Regression)\\nAssume the following functional form for P (Y |X):\\nP (Y = 1 |X) = 1\\n1 + exp(‚àí(w0 + ‚àë\\niwiXi)), (4.1)\\nP (Y = 0 |X) = 1\\n1 + exp(w0 + ‚àë\\niwiXi). (4.2)\\nIn essence, logistic regression means applying the logistic function œÉ(z) = 1\\n1+exp(‚àíz) to a linear\\nfunction of the data. However, note that it is still a linear classiÔ¨Åer.\\nDiving in the Math 6 - Logistic Regression as linear classiÔ¨Åer\\nNote that P (Y = 1 |X) can be rewritten as\\nP (Y = 1 |X) = exp(w0 + ‚àë\\niwiXi)\\n1 + exp(w0 + ‚àë\\niwiXi).\\nWe would assign label 1 if P (Y = 1 |X)>P (Y = 0 |X), which is equivalent to\\nexp(w0 +\\n‚àë\\ni\\nwiXi)> 1 ‚áîw0 +\\n‚àë\\ni\\nwiXi > 0.\\nSimilarly, we would assign label 0 if P (Y = 1 |X)<P (Y = 0 |X), which is equivalent to\\nexp(w0 +\\n‚àë\\ni\\nwiXi)< 1 ‚áîw0 +\\n‚àë\\ni\\nwiXi < 0.\\nIn other words, the decision boundary is the line w0 + ‚àë\\niwiXi, which is linear.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 17, 'page_label': '17', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='4.2 Training logistic regression\\nGiven training data {(xi,yi)}n\\ni=1 where the input hasd features, we want to learn the parameters\\nw0,w 1,...,w d. We can do so by MCLE:\\nÀÜwMCLE = arg max\\nw\\nn‚àè\\ni=1\\nP (y(i) |x(i),w ). (4.3)\\nNote the Discriminative philosophy: don‚Äôt waste eÔ¨Äort learningP (X), focus onP (Y |X) - that‚Äôs\\nall that matters for classiÔ¨Åcation! Using (4.1) and (4.2), we can then compute the log-likelihood:\\nl(w) = ln\\n( n‚àè\\ni=1\\nP (y(i) |x(i),w )\\n)\\n=\\nn‚àë\\ni=1\\n[\\ny(i)(w0 +\\nd‚àë\\nj=1\\nwix(i)\\nj ) ‚àí ln(1 + exp(w0 +\\nd‚àë\\nj=1\\nwix(i)\\nj ))\\n]\\n. (4.4)\\nThere is no closed-form solution to maximize l(w), but we note that it is a concave function.\\nDeÔ¨Ånition 5: (Concave function)\\nA function l(w) is called concave if the line joining two points l(w1),l (w2) on the function\\ndoes not lie above the function on the interval [ w1,w 2].\\nEquivalently, a functionl(w) is concave on [w1,w 2] if\\nl(tx1 + (1 ‚àít)x2) ‚â•tl(x1) + (1 ‚àít)l(x2)\\nfor all x1,x 2 ‚àà [w1,w 2] and t ‚àà [0, 1]. If the sign is reversed, l is a convex function.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 7 - Log likelihood of logistic regression is concave\\nFor convenience we denote x(i)\\n0 = 1, so that w0 + ‚àëd\\ni=jwix(i)\\nj =wTx(i).\\nWe Ô¨Årst note the following lemmas:\\n1. If f is convex then ‚àíf is concave and vice versa.\\n2. A linear combination of n convex (concave) functions f1,f 2,...,f n with nonnegative\\ncoeÔ¨Écients is convex (concave).\\n3. Another property of twice diÔ¨Äerentiable convex function is that the second derivative\\nis nonnegative. Using this property, we can see that f(x) = log(1 + expx) is convex.\\n4. If f and g are both convex, twice diÔ¨Äerentiable and g is non-decreasing, then g ‚ó¶f is\\nconvex.\\nNow we rewrite l(w) as follows:\\nl(w) =\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí log(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí\\nn‚àë\\ni=1\\nlog(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)fi(w) ‚àí\\nn‚àë\\ni=1\\ng(fi(w)),\\nwhere fi(w) =wTx(i) and g(z) = log(1 + expz).\\nfi(w) is of the form Ax +b where A = x(i) and b = 0, which means it‚Äôs aÔ¨Éne (i.e., both\\nconcave and convex). We also know that g(z) is convex, and it‚Äôs easy to see g is non-\\ndecreasing. This means g(fi(w)) is convex, or equivalently, ‚àíg(fi(w)) is concave.\\nTo sum up, we can express l(w) as\\nl(w) =\\nn‚àë\\ni=1\\ny(i)fi(w)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nconcave\\n+\\nn‚àë\\ni=1\\n‚àíg(fi(w))\\n\\ued19 \\ued18\\ued17 \\ued1a\\nconcave\\n,\\nhence l(w) is concave.\\nAs such, it can be optimized by the gradient ascent algorthim.\\nAlgorithm 7: (Gradient ascent algorithm)\\nInitialize: Pick w at random.\\nGradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.\\nUpdate:\\n‚àÜw =Œ∑‚àáwE(w)\\nw(t+1)\\nt ‚Üêw(t)\\ni +Œ∑‚àÇE(w)\\n‚àÇwi\\n,\\nwhere Œ∑ >0 is the learning rate.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 19, 'page_label': '19', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In this case our likelihood function is speciÔ¨Åed in (4.4), so we have the following steps for training\\nlogistic regression:\\nAlgorithm 8: (Gradient ascent algorithm for logistic regression)\\nInitialize: Pick w at random and a learning rate Œ∑.\\nUpdate:\\n‚Ä¢ Set an œµ> 0 and denote\\nÀÜP (y(i) = 1 |x(i),w (t)) =\\nexp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n1 + exp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n.\\n‚Ä¢ Iterate until |w(t+1)\\n0 ‚àíw(t)\\n0 |<œµ :\\nw(t+1)\\n0 ‚Üêw(t)\\n0 +Œ∑\\nn‚àë\\ni=1\\n[\\ny(i) ‚àí ÀÜP (y(i) = 1 |x(i),w (t))\\n]\\n.\\n‚Ä¢ Fork = 1,...,d , iterate until |w(t+1)\\nk ‚àíw(t)\\nk |<œµ :\\nw(t+1)\\nk ‚Üêw(t)\\nk +Œ∑\\nn‚àë\\ni=1\\nx(i)\\nj\\n[\\ny(i) ‚àí ÀÜP (y(i) = 1 |x(i),w (t))\\n]\\n.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 5\\nNaive Bayes ClassiÔ¨Åer\\n5.1 Gaussian Bayes\\nRecall that the Bayes decision rule (2.1) is the optimal classiÔ¨Åer, but requires having knowledge\\nofP (Y |X) =P (X |Y )P (Y ), which is diÔ¨Écult to compute. To tackle this problem, we consider\\nappropriate models for the two terms: class probability P (Y ) and class conditional distribution\\nof features P (X |Y ).\\nConsider an example of X being continuous 1-dimensional and Y being binary. The class\\nprobability P (Y ) then follows a Bernoulli distribution with parameter Œ∏, i.e., P (Y = 1) = Œ∏.\\nWe further assume the Gaussian class conditional density\\nP (X =x |Y =i) = 1‚àö2œÄœÉ2\\ny\\nexp\\n(\\n‚àí(x ‚àí¬µi)2\\n2œÉ2\\ni\\n)\\n, (5.1)\\nwhere we note that the distribution would be diÔ¨Äerent for each class (hence the notation ¬µi,œÉi).\\nIn total there are 5 parameters: Œ∏,¬µ 0,¬µ 1,œÉ 0,œÉ 1.\\nIn case X is 2-dimensional, we would have\\nP (X =x |Y =i) = 1‚àö\\n2œÄ|Œ£i|\\nexp\\n(\\n‚àí(x ‚àí¬µi)T Œ£‚àí1\\ni (x ‚àí¬µi)\\n2\\n)\\n, (5.2)\\nwhere Œ£i is a 2√ó2 covariance matrix. In total there are 11 parameters: Œ∏,¬µ0,¬µ 1 (2-dimensional),\\nŒ£0, Œ£1 (2 √ó 2 symmetric matrix).\\nFurther note that the decision boundary in this case is\\nP (Y = 1 |X =x)\\nP (Y = 0 |X =x) = P (X =x |Y = 1)P (Y = 1)\\nP (X =x |Y = 0)P (Y = 0)\\n=\\n‚àö\\n|Œ£0|\\n|Œ£1| exp\\n(\\n‚àí(x ‚àí¬µ1)T Œ£‚àí1\\n1 (x ‚àí¬µ1)\\n2 + (x ‚àí¬µ0)T Œ£‚àí1\\n0 (x ‚àí¬µ0)\\n2\\n)\\n¬∑ Œ∏\\n1 ‚àíŒ∏. (5.3)\\nThis implies a quadratic equation in x, but if Œ£ 1 = Œ£0 the quadratic terms cancel out and the\\nequation is linear.\\nThe number of parameters we would need to learn for Gaussian Bayes in the general case, with\\nk labels and d-dimensional inputs, is:\\n‚Ä¢ P (Y =i) =pi for 1 ‚â§i ‚â§k ‚àí 1: k ‚àí 1 parameters, and\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ P (X =x |Y =i) ‚àº N (¬µi, Œ£i): d parameters for ¬µi and d(d+1)\\n2 parameters for Œ£i,\\nwhich result in kd + kd(d+1)\\n2 =O(kd2) parameters.\\nIf X is discrete, again with k labels and d-dimensional inputs, the number of parameters is:\\n‚Ä¢ P (Y =i) =pi for 1 ‚â§i ‚â§k ‚àí 1: k ‚àí 1 parameters, and\\n‚Ä¢ P (X =x |Y =i) comes from a probability table with 2 d ‚àí 1 entries,\\nwhich result is k(2d ‚àí 1) parameters to learn.\\nHaving too many parameters means we need a lot of training data to learn them. We therefore\\nintroduce an assumption that can signiÔ¨Åcantly reduce the number of parameters.\\n5.2 Naive Bayes ClassiÔ¨Åer\\nDeÔ¨Ånition 6: (Naive Bayes ClassiÔ¨Åer)\\nThe Naive Bayes ClassiÔ¨Åer is the Bayes decision rule with an additional ‚Äúnaive‚Äù assumption\\nthat features are independent given the class label :\\nP (X |Y ) =P (X1,X 2,...,X d |Y ) =\\nd‚àè\\ni=1\\nP (Xi |Y ). (5.4)\\nIn this case, the output class of an input x is\\nÀÜfNB (x) = arg max\\ny\\nP (x1,...,x d |y)P (y) = arg max\\ny\\nd‚àè\\ni=1\\nP (xi |y)P (y).\\nTherefore, if the conditional assumption holds, Naive Bayes is the optimal classiÔ¨Åer. Using this\\nassumption, we can then formulate the Naive Bayes classiÔ¨Åer algorithm, where the class priors\\nand class conditional probabilities are estimated using MLE.\\nAlgorithm 9: (Naive Bayes ClassiÔ¨Åer for discrete features)\\nGiven training data {(x(i),y (i))}n\\ni=1 where x(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), we compute the follow-\\nings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class joint distribution:\\nÀÜP (Xj =xj,Y =y) = 1\\nn\\nn‚àë\\ni=1\\nI\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n.\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nÀÜP (xj,y )\\nÀÜP (y)\\n= arg max\\ny\\n‚àën\\ni=1 I\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n‚àën\\ni=1 I (y(i) =y) . (5.5)\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 10: (Naive Bayes ClassiÔ¨Åer for continuous features)\\nGiven training data {(xi,yi)}n\\ni=1 wherex(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), we compute the followings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class conditional distribution:\\nÀÜ¬µjy = 1‚àën\\ni=1 I (y(i) =y)\\nn‚àë\\ni=1\\nx(i)\\nj I\\n(\\ny(i) =y\\n)\\n,\\nÀÜœÉ2\\njy = 1‚àën\\ni=1 I (y(i) =y) ‚àí 1\\nn‚àë\\ni=1\\n(x(i)\\nj ‚àí ÀÜ¬µjy)2I\\n(\\ny(i) =y\\n)\\n,\\nÀÜP (Xj =xj |Y =y) = 1\\nÀÜœÉ2\\njy\\n‚àö\\n2œÄ exp\\n(\\n‚àí(x ‚àí ÀÜ¬µjy)2\\n2ÀÜœÉ2\\njy\\n)\\n.\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nP (xj |y). (5.6)\\nAs we noted earlier, the number of parameters in the above algorithms is much fewer.\\nDiving in the Math 8 - Number of parameters in Naive Bayes\\nConsider input variable X with discrete features X1,...,X d each taking one of K values\\nand output label Y taking one of M values. Further suppose that the label distribution is\\nBernoulli and the feature distribution conditioned on the label is multinomial.\\nWithout naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for label:\\nP (Y =y) =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\npy if y <M ‚àí 1\\n1 ‚àí\\nM‚àí2‚àë\\ni=0\\npi if y =M ‚àí 1\\n‚Ä¢ For each label y, the corresponding probability table has Kn ‚àí 1 parameters.\\nSo the total number of parameters is M ‚àí 1 +M(Kn ‚àí 1) =M ¬∑Kn ‚àí 1.\\nWith naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for labels, as mentioned above.\\n‚Ä¢ For each Y = y and 1 ‚â§ i ‚â§ n, Xi | Y comes from a categorial distribution so it has\\nK ‚àí 1 parameters. So we need Mn(K ‚àí 1) parameters for\\n{P (Xi =xi |Y =y) |i = 1..n, xi = 1..K, y = 0..M ‚àí 1}.\\nThe total number of parameters is M ‚àí 1 +Mn(K ‚àí 1). Therefore the number of param-\\neters is reduced from exponential to linear with the Naive Bayes assumption.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We note two important issues in Naive Bayes. First, the conditional independence assumption\\ndoes not always hold; nevertheless, this is still the most used classiÔ¨Åer, especially when data\\nis limited. Second, in practice we typically use MAP estimates for the probabilities instead of\\nMLE since insuÔ¨Écient data may cause MLE to be zero. For instance, if we never see a training\\ninstance where X1 =a and Y =b then, based on MLE, ÀÜP (X1 =a |Y =b) = 0. Consequently,\\nno matter what the values X2,...,X d take, we see that\\nÀÜP (X1 =a,X 2,...,X d |Y ) = ÀÜP (X1 =a |Y )\\nd‚àè\\nj=2\\nÀÜP (Xj |Y ) = 0.\\nTo resolve this issue, we can use a technique called smoothing and add m ‚Äúvirtual‚Äù data points.\\nAlgorithm 11: (Naive Bayes ClassiÔ¨Åer for discrete features with smoothing)\\nGiven training data {(xi,yi)}n\\ni=1 where x(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), assume some prior dis-\\ntributions (typically uniform) Q(Y = b) and Q(Xj = a,Y = b). We then compute the\\nfollowings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class conditional distribution:\\nÀÜP (Xj =xj |Y =y) =\\n‚àën\\ni=1 I\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n+mQ(Xj =xj,Y =y)\\n‚àën\\ni=1 I (y(i) =y) +mQ(Y =y) .\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nP (xj |y). (5.7)\\n5.3 Text classiÔ¨Åcation: bag of words model\\nWe present a case study of Naive Bayes ClassiÔ¨Åer: text classiÔ¨Åcation. Given the input text as\\na string of words, we have to predict a category for it (i.e., what is the topic, is this a spam\\nemail). Using the bag of words approach, we construct the input features X as the count of\\nhow many times each word appears in the document. The dimension d ofX is then the number\\nof distinct words in the document, which can be very large, leading to a huge probability table\\nfor P (X | Y ) (with 2d ‚àí 1 entries). However, under the Naive Bayes assumption, P (X | Y ) is\\nsimply a product of probability of each word raised to its count. It follows that\\nÀÜfNB (x) = arg max\\ny\\nP (y)\\nwk‚àè\\nw=w1\\nP (w |Y )count(w), (5.8)\\nwherew1,...,w k are all the distinct words in the document. Note that here we assume the order\\nof words in the document doesn‚Äôt matter , which sounds silly but often works very well.\\n5.4 Generative vs Discriminative Classifer\\nWe Ô¨Årst note the following comparison between Naive Bayes and Logistic Regression:\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 24, 'page_label': '24', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In other words, the two methods have representation equivalence - in particular, a linear decision\\nboundary. However, keep in mind that:\\n‚Ä¢ This is only true in the special case where we assume the feature variances are independent\\nof class label (more speciÔ¨Åcally, Œ£ 1 = Œ£0 in (5.3)).\\n‚Ä¢ Logistic regression is a discriminative model and makes no assumption about P (X |Y ) in\\nlearning. Instead, it assumes a sigmoid form for P (Y |X).\\n‚Ä¢ The two optimize diÔ¨Äerent functions: MLE / MCLE vs MAP / MCAP and yield diÔ¨Äerent\\nsolutions.\\nMore generally, we outline the problem between a generative classiÔ¨Åer (e.g., Naive Bayes) and a\\ndiscriminative classiÔ¨Åer (e.g., logistic regression).\\nGenerative Discriminatve\\nAssume some prorability model forP (Y ) and\\nP (X |Y ).\\nAssume some functional form for P (Y | X)\\nor the decision boundary.\\nEstimate parameters of probability models\\nfrom training data.\\nEstimate parameters of functional form di-\\nrectly from training data.\\nTable 5.1: Generative (model-based) vs Discriminative (model-free) classiÔ¨Åer.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 25, 'page_label': '25', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 6\\nNeural Networks and Deep Learning\\n6.1 DeÔ¨Ånition\\nDeÔ¨Ånition 7: (Neural network)\\nGiven a function f :X ‚ÜíY such that:\\n‚Ä¢ f can be non-linear\\n‚Ä¢ X is vector of continuous / discrete variables\\n‚Ä¢ Y is vector of continuous / discrete variables\\nA neural network is a way to represent f by a network of logistic / sigmoid unit.\\nIn its simplest form, a neural network has one input node and one output node\\nx ow\\nso it is equivalent to logistic regression\\no(x) =œÉ(wx) = 1\\n1 +e‚àíwx. (6.1)\\nOn the other hand, a neural network with one hidden layer\\nx oh ow0 wh\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='would output\\no(x) =œÉ(w0 +\\n‚àë\\nh\\nwhœÉ(wh\\n0 +\\n‚àë\\ni\\nwh\\nixi)\\n\\ued19 \\ued18\\ued17 \\ued1a\\noh\\n). (6.2)\\nMore generally, prediction in neural networks is done by starting from the input layer and, for\\neach subsequent layer, computing the output of the sigmoid units (forward propagation).\\n6.2 Training a neural network\\n6.2.1 Gradient descent\\nLet‚Äôs treat a neural network as a function Y =fw(X) +œµ where fw is determined given w and\\nœµ ‚àº N (0,œÉ 2I), so Y ‚àº N (fw(X),œÉ 2I). One way to learn the weights is by MCLE:\\nÀÜwMCLE = arg max\\nw\\nln\\n(‚àè\\nl\\nP (y(l) |x(l),w )\\n)\\n= arg min\\nw\\n‚àë\\nl\\n(y(l) ‚àí ÀÜfw(x(l)))2.\\nIn other words, the weights are trained to minimize sum of squared errors of predicted network\\noutputs. We may also want to restrict the weights to small values, and this bias can be encoded\\nin MCAP:\\nÀÜwMCAP = arg max\\nw\\nln\\n(\\nP (w)\\n‚àè\\nl\\nP (y(l) |x(l),w )\\n)\\n= arg min\\nw\\nc\\n‚àë\\ni\\nw2\\ni +\\n‚àë\\nl\\n(y(l) ‚àí ÀÜfw(x(l)))2,\\nwhere P (w) ‚àù exp(‚àíwTw/2œÑ 2) (recall the connection between Ridge Regression and MCAP\\n(3.10)). In other words, the weights are trained to minimize sum of squared errors of predicted\\nnetwork outputs plus weight magnitudes.\\nTo perform the above optiminization, we introduce a routine called gradient descent.\\nAlgorithm 12: (Gradient descent algorithm)\\nInitialize: Pick w at random.\\nGradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.\\nUpdate:\\n‚àÜw = ‚àíŒ∑‚àáwE(w)\\nw(t+1)\\nt ‚Üêw(t)\\ni ‚àíŒ∑‚àÇE(w)\\n‚àÇwi\\n,\\nwhere Œ∑ >0 is the learning rate.\\nSuppose we feed the training data {(x(l),y (l))}n\\nl=1 where x(l) is d-dimensional into a one-layer\\nneural network with d input nodes xi and one output node o. The gradient descent w.r.t every\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 27, 'page_label': '27', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='output weight wi is\\n‚àÇE\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l))o(l)(1 ‚àío(l))x(l)\\ni . (6.3)\\nDiving in the Math 9 - Gradient descent for weights at output layer\\nx1\\nx2\\n...\\nxd\\n‚àë œÉ o\\nw1\\nw2\\nwd\\nnet = ‚àëd\\ni=1wixi o =œÉ(net) = 1\\n1+e‚àínet\\nNote that œÉ(x) = 1\\n1+e‚àíx is the sigmoid function and d\\ndxœÉ(x) =œÉ(x)(1 ‚àíœÉ(x)).\\nWe haveE(w) = 1\\n2\\n‚àën\\nl=1(y(l) ‚àío(l))2 so\\n‚àÇE\\n‚àÇwi\\n= 1\\n2\\nn‚àë\\nl=1\\n‚àÇ\\n‚àÇwi\\n(y(l) ‚àío(l))2 = 1\\n2\\nn‚àë\\nl=1\\n‚àÇ(y(l) ‚àío(l))2\\n‚àÇo(l) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l)) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n.\\nNow note that\\n‚àÇo(l)\\n‚àÇwi\\n= ‚àÇo(l)\\n‚àÇnet(l) ¬∑ ‚àÇnet(l)\\n‚àÇwi\\n=o(l)(1 ‚àío(l))x(l)\\ni ,\\nso\\n‚àÇE\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l))o(l)(1 ‚àío(l))x(l)\\ni .\\nFor the hidden layers, consider ‚àÇE\\n‚àÇwhk\\nwherewhk connects a node oh from layerL took from layer\\nL + 1. Further assume that Œì is the set of all nodes at layer L + 2. We then have the expression\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)\\n‚àë\\nŒ≥‚ààŒì\\nŒ¥Œ≥wkŒ≥., (6.4)\\nwhereŒ¥Œ≥ =oŒ≥(1 ‚àíoŒ≥)‚àÇE\\n‚àÇoŒ≥\\n. This means that we can Ô¨Årst compute the gradients w.r.t the weights\\nat the output layer, then propagate back to the weights at one previous layer, then two previous\\nlayers, and so on, until the input layer.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 10 - Gradient descent for weights at hidden layers\\nWe would like to evaluate ‚àÇE\\n‚àÇwhk\\n= ‚àÇE\\n‚àÇok\\n¬∑ ‚àÇok\\n‚àÇwhk\\n.\\nSince ‚àÇok\\n‚àÇwhk\\n=ok(1 ‚àíok)oh, we can rewrite the above expression as\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)‚àÇE\\n‚àÇok\\n=ohŒ¥k.\\nNow we consider\\n‚àÇE\\n‚àÇok\\n=\\n‚àë\\nŒ≥\\n‚àÇE\\n‚àÇoŒ≥\\n¬∑ ‚àÇoŒ≥\\n‚àÇok\\n=\\n‚àë\\nŒ≥\\n‚àÇE\\n‚àÇoŒ≥\\noŒ≥(1 ‚àíoŒ≥)wkŒ≥ =\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\nHence\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\n6.2.2 Backpropagation\\nMore generally, we can derive the backpropagation algorithm as in the next page. We also note\\nthe special properties of this algorithm:\\n‚Ä¢ It computes gradient descent over the entire network weight vector. Training can take\\nthousands of iterations, which is slow, but using network after training is very fast.\\n‚Ä¢ It can easy generalize to arbitrary directed graphs.\\n‚Ä¢ It will Ô¨Ånd a local, not necessarily global error minimum (because error function E is no\\nlonger convex in weights), but often works well in practice.\\n‚Ä¢ It often includes a weight momentum Œ±:\\n‚àÜw(n)\\nij =Œ∑Œ¥jxij +Œ±‚àÜw(n‚àí1)\\nij .\\nThe expressive capabilities of neural network are powerful:\\n‚Ä¢ Every boolean function can be represented by a network with one hidden layer, but may\\nrequire exponential (in number of inputs) hidden units.\\n‚Ä¢ Every bounded continuous function can be approximated with arbitrarily small error, by\\na network with one hidden layer.\\n‚Ä¢ Any function can be approximated to arbitrary accuracy by a network with two hidden\\nlayers.\\nHowever, neural network may still encounter the issue of overÔ¨Åtting, which can be avoided by\\nMCAP, early stopping, or by regulating the number of hidden units, which essentially prevents\\noverly complex models.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 13: (Backpropagation)\\nDenote the followings:\\n‚Ä¢ l is the index of the traning example\\n‚Ä¢ yk is target output (label) of output unit k\\n‚Ä¢ oh,ok are unit output (obtained by forward propagation) of output units h,k . If i is\\ninput variable, oi =xi.\\n‚Ä¢ wij is the weight from node i to node j in the next layer.\\nInitialize all weights to small random numbers. Until satisÔ¨Åed, do\\n‚Ä¢ For each training example, do\\n1. Input the training example to the network and compute the network outputs,\\nusing forward propagation.\\n2. For each output unit k, let\\nŒ¥(l)\\nk ‚Üêo(l)\\nk (1 ‚àío(l)\\nk )(y(l)\\nk ‚àío(l)\\nk ).\\n3. For each hidden unit h, let\\nŒ¥(l)\\nh ‚Üêo(l)\\nh (1 ‚àío(l)\\nh )\\n‚àë\\nk‚ààK\\nwhkŒ¥(l)\\nk ,\\nwhere K is the set of output nodes.\\n4. Update each network weight wij:\\nwij ‚Üêwij + ‚àÜw(l)\\nik\\nwhere ‚àÜw(l)\\nij = ‚àíŒ∑Œ¥(l)\\nj o(l)\\ni .\\nUnlike in logistic regression, the function E(w) in neural network is not convex in w. Thus,\\ngradient descent (and backpropagation) will Ô¨Ånd a local, not necessarily global minimum, but\\nit often works well in practice.\\nFinally, we note the two ways in which backpropagation can be implemented in practice: Batch\\nmode and Incremental mode (also called Stochastic Gradient Descent ). Incremental mode is\\nfaster to compute and can approximate Batch mode arbitrary closely if Œ∑ is small enough.\\nBatch mode Gradient mode\\nLet ED(w) = 1\\n2\\n‚àë\\nl‚ààD(y(l) ‚àío(l))2. Let El(w) = 1\\n2(y(l) ‚àío(l))2.\\nDo until satisÔ¨Åed:\\n1. Compute the gradient ‚àáED(w).\\n2. w ‚Üêw ‚àíŒ∑‚àáED(w).\\nDo until satisÔ¨Åed:\\n‚Ä¢ For each training example l in D:\\n1. Compute the gradient ‚àáEl(w).\\n2. w ‚Üêw ‚àíŒ∑‚àáEl(w).\\nTable 6.1: Batch mode vs Gradient mode\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 30, 'page_label': '30', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='6.3 Convolutional neural networks\\nDeÔ¨Ånition 8: (Deep architectures)\\nDeep architectures are composed of multiple levels of non-linear operations, such as neural\\nnets with many hidden layers.\\nDeep learning methods aim at learning feature hierarchies, where features from the higher levels\\nof the hierarchy are formed by lower level features. One such method is convoluational neural\\nnetwork, which, compared to standard feedforward neural network:\\n‚Ä¢ have much fewer connections and parameters\\n‚Ä¢ is easier to train\\n‚Ä¢ has only slightly worse theoretically best performance.\\nFirst, we deÔ¨Åne the term convolution.\\nDeÔ¨Ånition 9: (Convolution)\\nThe convolution of two functions f and g, denoted f ‚àóg, is deÔ¨Åned as:\\n‚Ä¢ If f and g are continuous:\\n(f ‚àóg)(t) =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(œÑ)g(t ‚àíœÑ)dœÑ =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(t ‚àíœÑ)g(œÑ)dœÑ. (6.5)\\n‚Ä¢ If f and g are discrete:\\n(f ‚àóg)[n] =\\n‚àû‚àë\\nm=‚àí‚àû\\nf[m]g[n ‚àím] =\\n‚àû‚àë\\nm=‚àí‚àû\\nf[n ‚àím]g[m]. (6.6)\\n‚Ä¢ If discrete g has support on {‚àíM,...,M }:\\n(f ‚àóg)[n] =\\nM‚àë\\nm=‚àíM\\nf[n ‚àím]g[m]. (6.7)\\nInformally, the convolution gives a sense of how much two functions ‚Äúoverlap.‚Äù A simple con-\\nvolutional neural network (CNN) is a sequence of layers, each of which transforms one volume\\nof activations to another through a diÔ¨Äerentiable function. We use three main types of layers to\\nbuild the network architectures: Convolutional Layer, Pooling Layer, and Fully Connected Layer\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(exactly as seen in regular Neural Networks). Typically, CNNS are used for image classiÔ¨Åcation\\nin computer vision.\\nWe now discuss the components of a CNN. The following text excerpts are from Chapter 10 of\\nArtiÔ¨Åcial Intelligence for Humans, Vol 3: Neural Networks and Deep Learning and Stanford‚Äôs\\nCS 231n.\\n6.3.1 Convolutional Layer\\nThe primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of\\ncolor, and other visual elements. Each feature is represented by a Ô¨Ålter; the more Ô¨Ålters that we\\ngive to a convolutional layer, the more features it can detect.\\nMore formally, a Ô¨Ålter is a square 2D matrix that scans over the image. The convolutional layer,\\nwhich is essentially a set of Ô¨Ålters, acts as a smaller grid that sweeps left to right over each of\\nrow of the image. Each cell in a Ô¨Ålter is a weight.\\nThe sweeping phase (forward propagation) is done as follows. First, the input image may be\\npadded with some layers of zero cells as need. The stride speciÔ¨Åes the number of positions at\\nwhich the convolutional Ô¨Ålters will stop. The convolutional Ô¨Ålters move to the right, advancing\\nby the number of cells speciÔ¨Åed in the stride. Once the far right is reached, the convolutional\\nÔ¨Ålter moves back to the far left, then it moves down by the stride amount and continues to the\\nright again. Hence, the number of steps in one sweeping phase is\\nNumber of steps = W ‚àíF + 2P\\nS + 1,\\nwhere W is the image size, F is the Ô¨Ålter size, P is the padding and S is the stride.\\nWe can use the same set of weights as the convolutional Ô¨Ålter sweeps over the image. This\\nprocess allows convolutional layers to share weights and greatly reduce the amount of processing\\nneeded. In this way, we can recognize the image in shift positions because the same convolutional\\nÔ¨Ålter sweeps across the entire image.\\nThe input and output of a convolutional layer are both 3D boxes. For the input to a convolu-\\ntional layer, the width and height of the box is equal to the width and height of the input image.\\nThe depth of the box is equal to the color depth of the image. For an RGB image, the depth\\nis 3, equal to the components of red, green, and blue. If the input to the convolutional layer\\nis another layer, then it will also be a 3D box; however, the dimensions of that 3D box will be\\ndictated by the hyper-parameters of that layer. Like any other layer in the neural network, the\\nsize of the 3D box output by a convolutional layer is dictated by the hyper-parameters of the\\nlayer. The width and height of this box are both equal to the Ô¨Ålter size. However, the depth is\\nequal to the number of Ô¨Ålters.\\nA visualization for the sweeping phase is in the Convolution Demo section at Stanford‚Äôs CS\\n231n. In summary, the procedure taking place at the convolutional layer is as follows.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 14: (Convolutional layer‚Äôs forward pass)\\nAccepts a 3D image of size W1 √óH1 √óD1.\\nRequires four hyperpameters: number of Ô¨Ålters K, spatial extent F , stride S, and amount\\nof zero padding P .\\nProduces a 3D image of size W2 √óH2 √óD2 where\\nW2 = W1 ‚àíF + 2P\\nS + 1; H2 = H1 ‚àíF + 2P\\nS + 1; D2 =K. (6.8)\\nWith parameter sharing, it introduces F √óF √óD1 weights per Ô¨Ålter, for a total of F √óF √ó\\nD1 √óK weights and K biases. In the output volume, the d-th depth slice (of size W2 √óH2)\\nis the result of performing a valid convolution of the d-th Ô¨Ålter over the input volume with\\na stride of S, and then oÔ¨Äset by d-th bias.\\n6.3.2 Pooling layer\\nPooling layer downsamples an input 3D image to a new one with smaller widths and heights.\\nTypically a pooling layer follows immediately after a convolutional layer. A typical choice for\\npooling is max-pooling, which, for every f √óf region in the input image, outputs the maximum\\nnumber in that region to the output image.\\nAlgorithm 15: (Pooling layer‚Äôs forward pass)\\nAccepts a volume of size W1 √óH1 √óD1.\\nRequires two hyperparameters: spatial extent F and stride S, Produces a volume of size\\nW2 √óH2 √óD2 where:\\nW2 = W1 ‚àíF\\nS + 1; H2 = H1 ‚àíF\\nS + 1; D2 =D1. (6.9)\\nIntroduces zero parameters since it computes a Ô¨Åxed function of the input. For Pooling\\nlayers, it is not common to pad the input using zero-padding.\\nIt is worth noting that there are only two commonly seen variations of the max pooling layer\\nfound in practice: A pooling layer with F = 3,S = 2 (also called overlapping pooling), and more\\ncommonly F = 2,S = 2.\\n6.3.3 Fully Connected Layer\\nThis is the same layer from a feedforward neural network, with two hyperparameters: neuron\\ncount and activation function. Every neuron in the the previous layer‚Äôs 3D image output is\\nconnected to each neuron in this layer through a weight value. A dot product of the input\\n(Ô¨Çattened to 1D) and the weight vector is then passed to the activation function. Dense layers\\ncan employ many diÔ¨Äerent kinds of activation functions, such as ReLU, sigmoid or tanh.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 33, 'page_label': '33', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 7\\nSupport Vector Machine\\n7.1 Introduction\\nRecall that a regression classiÔ¨Åer with linear decision boundary would typically look like\\nwhere the boundary is determined by taking into account all data points (e.g., linear regression\\n(3.1)). In this case, the decision line is closer to the blue nodes since many of them are far\\noÔ¨Ä to the right. However, note that there could be many possible classiÔ¨Åers that have diÔ¨Äerent\\nboundaries but yield the same outcome:\\nOne way to decide on a single classiÔ¨Åer is to Ô¨Ånd a max margin classiÔ¨Åer: a boundary that leads\\nto the largest margin from both sets of points. This also means that instead of Ô¨Åtting all points,\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 34, 'page_label': '34', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='we may consider only the ‚Äúboundary‚Äù points, and we want to learn a boundary that leads to the\\nlargest margin from points on both sides. These boundary points are called the support vectors.\\nIn particular, we would specify a max margin classiÔ¨Åer based on parameters w and b, and then\\nperform classiÔ¨Åcation as follows\\n7.2 Primal form\\n7.2.1 Linearly separable case\\nOur goal, as previously mentioned, is to Ô¨Ånd the maximum margin. Let‚Äôs deÔ¨Åne the width of\\nthe margin by M, we can then see that\\nM = 2‚àö\\nwTw\\n. (7.1)\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 11 - Computing margin M in terms of weight w and bias b\\nFirst observe that the vector w is orthogonal to the +1 plane. To prove this, let u andv be\\nany two points on the +1 plane then\\nwTu =wTv = 1 ‚àíb ‚áíwT (u ‚àív) = 0.\\nSimilarly,w is orthogonal to the ‚àí1 plane too. Hence, if x+ is a point on the + plane and\\nx‚àí is the point closest to x+ on the ‚àí plane, then the vector from x+ tox‚àí is parallel to w.\\nIn other words, x+ =Œªw +x‚àí for some Œª. Now we have\\nwTx+ +b = 1\\nwT (Œªw +x‚àí) +b = 1\\nwTx‚àí +ŒªwTw = 1\\nŒª = 2\\nwTw.\\nHence\\nM = |x+ ‚àíx‚àí| = |Œªw| =Œª\\n‚àö\\nwTw = 2\\nwTw\\n‚àö\\nwTw = 2‚àö\\nwTw\\n.\\nWe can now search for the optimal parameters by Ô¨Ånding a solution that:\\n1. Correctly classiÔ¨Åes all points\\n2. Maximizes the margin (or equivalently minimizes wTw).\\nSeveral optimization methods can be used: Gradient descent, simulated annealing, EM, etc. In\\nthis case, the problem also belongs to the category of quadratic programming (QP).\\nDeÔ¨Ånition 10: (Quadratic programming)\\nQuadratic programming solves the optimization problem\\nmin\\nu\\nuTRu\\n2 +dTu +c,\\nwhere u is a vector, R is square matrix, d is vector and c is scalar.\\nFurthermore,u is subject to n inequality constraint\\nai1u1 +ai2u2 +... ‚â§bi, 1 ‚â§i ‚â§n,\\nand k equivalency constraint\\naj1u1 +aj2u2 +... =bn+j, 1 ‚â§j ‚â§k.\\nMore speciÔ¨Åcally, we can frame the margin maximization problem as a QP problem:\\nmin\\nw\\nwTw\\n2\\nsubject to n constraints if there are n samples x.\\n‚Ä¢ wTx +b ‚â• 1 for all x in class +1,\\n‚Ä¢ wTx ‚àíb ‚â§ ‚àí1 for all x in class ‚àí1,\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.2.2 Non linearly separable case\\nSo far we have assumed that the data is linearly separable, i.e., there is a line wTx +b that\\nperfectly separates the +1 and ‚àí1 class. But this is usually not the case in practice, as there\\ncan be noise and outliers. One way to address this is to penalize the number of misclassiÔ¨Åed\\npoints m, i.e.,\\nmin\\nw\\nwTw +C ¬∑m,\\nwhere C is a regularization constant. However, this is hard to encode in a QP problem.\\nInstead of minimizing the number of misclassiÔ¨Åed points we can minimize the distance between\\nthese points and their correct plane. In this case, the new optimization problem is\\nmin\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi,\\nsubject to 2n constraints if there are n samples x(i):\\n‚Ä¢ wTx(i) +b ‚â• 1 ‚àíœµi for all x(i) in class +1\\n‚Ä¢ wTx(i) +b ‚â§ ‚àí1 +œµi for all x(i) in class ‚àí1.\\n‚Ä¢ œµi ‚â• 0 for all i.\\nIn summary, we have two optimization problems for two cases:\\nSeparable case Non-separable case\\nFind\\nmin\\nw\\nwTw\\n2\\nsubject to\\n‚Ä¢ wTx +b ‚â• 1 for all x\\nin class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí 1 for all\\nx in class ‚àí1\\nFind\\nmin\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi\\nsubject to\\n‚Ä¢ wTx+b ‚â• 1‚àíœµi for all\\nxi in class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí1 +œµi for\\nall xi in class ‚àí1\\n‚Ä¢ œµi ‚â• 0 for all i.\\nTable 7.1: Optimization constraints for separable and non-separable case in primal SVM\\n7.3 Dual representation\\n7.3.1 Linearly separable case\\nInstead of solving the QPs in Table 7.1 directly, we will solve a dual formulation of the SVM\\noptimization problem. The main reason for switching to this type of representation is that it\\nwould allow us to use a neat trick that will make our lives easier (and the run time faster).\\nStarting from the separable case, note that we can rephrase the constraints as\\n(wTx(i) +b)y(i) ‚â• 1 (7.2)\\nfor alli, whereyi - the class ofxi - is ¬±1. We can then encoding this as part of our minimization\\nproblem using Lagrange multiplier.\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 37, 'page_label': '37', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 16: (Lagrange multiplier method)\\nConsider a problem of Ô¨Ånding\\nmin\\nw\\nf(w)\\nsuch that hi(w) = 0, i = 1,...,l\\nIn the Lagrange multiplier method, we can deÔ¨Åne the Lagrangian to be\\nL(w,Œ≤ ) =f(w) +\\nl‚àë\\ni=1\\nŒ≤ihi(w),\\nwhere the Œ≤i‚Äôs are called the Lagrange multipliers. We would then Ô¨Ånd and set L‚Äôs partial\\nderivatives to zero: ‚àÇL\\n‚àÇwi\\n= 0; ‚àÇL\\n‚àÇŒ≤i\\n= 0,\\nand solve for w and Œ≤.\\nMore generally, consider the following primal optimization problem:\\nmin\\nw\\nf(w)\\nsuch that gi(w) ‚â§ 0, i = 1,...,k\\nhi(w) = 0, i = 1,...,l\\nTo solve it, we start by deÔ¨Åning the generalized Lagrangian:\\nL(w,Œ±,Œ≤ ) =f(w) +\\nk‚àë\\ni=1\\nŒ±igi(w) +\\nl‚àë\\ni=1\\nŒ≤ihi(w)\\nwhere the Œ±i,Œ≤i‚Äôs are the Lagrange multipliers.\\nUsing the Lagrange multiplier, consider the quantity\\nŒ∏p(w) = max\\nŒ±,Œ≤:Œ±i‚â•0\\nL(w,Œ±,Œ≤ )\\nthen our minimization problem becomes\\nmin\\nw\\nŒ∏p(w) = min\\nw\\nmax\\nŒ±,Œ≤:Œ±i‚â•0\\nL(w,Œ±,Œ≤ ).\\nSpeciÔ¨Åcally, our original problem is\\nmin\\nw\\nwTw\\n2\\nsuch that gi(w) = ‚àíy(i)(wTx(i) +b) + 1 ‚â§ 0.\\nwhich can be translated to\\nmin\\nw\\nmax\\nŒ±\\nwTw\\n2 ‚àí\\n‚àë\\ni\\nŒ±i(y(i)(wTx(i) +b) ‚àí 1)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nL(w,Œ±)\\n(7.3)\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='where Œ±i ‚â• 0 for all i. Setting the derivative of L w.r.t w,Œ±,b respectively we get:\\n0 = ‚àÇL\\n‚àÇw =w ‚àí\\nn‚àë\\ni=1\\nŒ±iy(i)x(i) ‚áíw =\\nn‚àë\\ni=1\\nŒ±iy(i)x(i). (7.4)\\n0 = ‚àÇL\\n‚àÇŒ±i\\n= ‚àíy(i)(wTx(i) +b) ‚àí 1 ‚áíb =y(i) ‚àíwTx(i) for i where Œ±i > 0. (7.5)\\n0 = ‚àÇL\\n‚àÇb =\\nn‚àë\\ni=1\\nŒ±iy(i). (7.6)\\nWe mentioned earlier that the only data points of importance are the support vectors, which\\naÔ¨Äect the margin. Originally, we need to Ô¨Ånd the points that touch the boundary of the margin\\n(i.e., wTx +b = ¬±1). In this case, howerver, (7.4) gives us the support vectors directly, which\\nare the points (x(i),y (i)) where Œ±i > 0.\\nSubstituting the above results back into (7.3), we get the equivalent problem of\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j) (7.7)\\nwhere ‚àë\\niŒ±iy(i) = 0 and Œ±i ‚â• 0 for all i. After solving for Œ± in this new problem, to evaluate a\\nnew sample x, we simply compute\\nÀÜy = sign(wTx +b) = sign\\n(‚àë\\ni\\nŒ±iyi(x(i))Tx +b\\n)\\n. (7.8)\\nNote that both the optimization function (7.7) and decision function (7.8) rely on the sum of\\ndot products (x(j))Tx(i), which can be expensive to compute.\\n7.3.2 Transformation of inputs\\nWhen the data is not linearly separable, the original input space ( x) can be mapped to some\\nhigher-dimensional feature space (œÜ(x)) where the training set is separable.\\nFor instance, we can map (x) ‚Üí (x,x 2) (from 1D to 2D) and ( x1,x 2) ‚Üí (x2\\n1,x 2\\n2,\\n‚àö\\n2x1x2) (from\\n2D to 3D):\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 39, 'page_label': '39', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In general, if the data is mapped into suÔ¨Éciently high dimension, then samples will be linearly\\nseparable. n data points can be separable in a space of n ‚àí 1 dimensions or more. However, this\\ntransformation poses two diÔ¨Éculties:\\n‚Ä¢ High computation burden due to high-dimensionality.\\n‚Ä¢ Many more parameters.\\nSVM solves these two issues by:\\n‚Ä¢ Using dual formulation, which only assigns parameters to samples, not features (i.e., each\\nx(i) has an associated Œ±i).\\n‚Ä¢ Using kernel tricks for eÔ¨Écient computation.\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Note that if we have n data points with m dimensions then the number of parameters is m in\\nprimal form (i.e., the features of weight w) and n in dual form (i.e., an Œ±i for eachx(i)). At Ô¨Årst\\nglance, because n ‚â´ m, the primal formation is at an advantage. However, note that in dual\\nform we only care about the support vectors; in other words, the parameters are only those Œ±i\\nthat are positive, and their number is usually a lot less thann. Hence, the dual form is not worse\\nthan primal form in the original space. In the transformed space, as x increases in dimension,\\nso does w, so the primal form requires more parameters, while the dual form generally also sees\\nan increase in the number of support vectors, but not as much.\\n7.3.3 Kernel tricks\\nWhile working in higher dimensions is beneÔ¨Åcial, it also increases our run time because of the\\ndot product computation. However, there is a neat trick we can use.\\nConsider, for example, all quadratic terms for the features x1,x 2,...,x m:\\nœÜ(x) = (1,\\n‚àö\\n2x1,...,\\n‚àö\\n2xm\\ued19 \\ued18\\ued17 \\ued1a\\nm+1 linear terms\\n, x 2\\n1,...,x 2\\nm\\ued19 \\ued18\\ued17 \\ued1a\\nm quadratic terms\\n,\\n‚àö\\n2x1x2,...,\\n‚àö\\n2xm‚àí1xm\\ued19 \\ued18\\ued17 \\ued1a\\nm(m‚àí1)/2 pairwise terms\\n)T. (7.9)\\nThe dot product operation would normally be\\nœÜ(x)TœÜ(z) =\\n‚àë\\ni\\n2xizi +\\n‚àë\\ni\\n(xi)2(zi)2 +\\n‚àë\\ni<j\\n2xixjzizj + 1,\\nwhich has O(m2) operations. However, we can obtain dramatic savings by noting that\\n(x ¬∑z + 1)2 = (x ¬∑z)2 + 2(x ¬∑z) + 1\\n= (\\n‚àë\\ni\\nxizi)2 +\\n‚àë\\ni\\n2xizi + 1\\n=\\n‚àë\\ni\\n2xizi +\\n‚àë\\ni\\n(xi)2(zi)2 +\\n‚àë\\ni<j\\n2xixjzizj + 1\\n=œÜ(x)TœÜ(z).\\nIn other words, to compute œÜ(x)TœÜ(z), we can simply compute x ¬∑z + 1 (which only needs m\\noperations) and then square it. Hence, we don‚Äôt need to work directly with the transformations\\nœÜ(x) to compute their dot products. The function œÜ in this case (7.9) is called a polynomial\\nkernel (of degree 2).\\nThe kernel trick works for higher order polynomials as well. In general, a polynomial of degree\\nd can be computed by ( x ¬∑z + 1)d. Beyond polynomials there are other very high dimensional\\nbasis functions that can be made practical by Ô¨Ånding the right kernel function, such as Radial\\nbasis kernel function K(x,z ) = exp\\n(\\n‚àí (x‚àíz)2\\n2œÉ2\\n)\\n.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.3.4 Non linearly separable case\\nUsing the Lagrange multiplier method on the optimization function for the primal form‚Äôs non\\nlinearly separable case (Table 7.1), we obtain our dual target function\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±jy(i)y(j)(x(i))T (x(j)) (7.10)\\nsubject to ‚àë\\niŒ±iy(i) = 0 andC >Œ±i ‚â• 0 (so now theŒ±i‚Äôs are bounded above by the regularization\\nconstant). To evaluate a new sample x, we similarly perform the computation as in (7.8).\\nIn summary, we have two optimization problems for two cases:\\nSeparable case Non-separable case\\nFind\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j)\\nwhere ‚àë\\niŒ±iy(i) = 0 and Œ±i ‚â• 0 for all i.\\nFind\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j)\\nwhere ‚àë\\niŒ±iy(i) = 0 and C >Œ±i ‚â• 0 for all i.\\nTable 7.2: Optimization constraints for separable and non-separable case in dual SVM.\\n7.4 Other topics\\n7.4.1 Why do SVMs work?\\nIf we are using huge features spaces (with kernels) why are we not overÔ¨Åtting the data?\\n‚Ä¢ Number of parameters remains the same (and most are set to 0).\\n‚Ä¢ While we have a lot of input values, at the end we only care about the support vectors\\nand these are usually a small group of samples.\\n‚Ä¢ The minimization (or the maximizing of the margin) function acts as a sort of regularization\\nterm leading to reduced overÔ¨Åtting.\\n7.4.2 Multi-class classiÔ¨Åcation with SVM\\nIf we have data from more than two classes, most common solution is the one-versus-all approach:\\n‚Ä¢ Create a classifer for each class against all other data.\\n‚Ä¢ For a new point use all classiÔ¨Åers and compare the margin for all selected classes. The\\nclass with the largest margin is selected.\\nNote that this is not necessarily valid since this is not what we trained the SVM for, but often\\nworks well in practice.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 8\\nEnsemble Methods and Boosting\\n8.1 Introduction\\nConsider the simple (weak) learners (e.g., naive Bayes, logistic regression, decision tree) - those\\nthat don‚Äôt learn too well but still better than chance, i.e. error < 50% but not close to 0. They\\nare good (low variance, usually don‚Äôt overÔ¨Åt) but also bad (high bias, can‚Äôt solve hard problems).\\nCan we somehow improve them by combining them together? A simple approach is ‚Äúbucket of\\nmodels‚Äù:\\n‚Ä¢ Input:\\n‚Äì Your topT favorite learners L1,...,L T\\n‚Äì A dataset D\\n‚Ä¢ Learning algorithm:\\n1. Use 10-fold cross validation to estimate the error of L1,...,L T\\n2. Pick the best (lowest 10-CV error) learner L‚àó\\n3. Train L‚àó on D and return its hypothesis h‚àó\\nThis approach is simple and will give results not much worse than the best of the ‚Äúbase learners‚Äù,\\nbut what if there‚Äôs not a single best learner? How do we come up with a method that combines\\nmultiple classiÔ¨Åers? One way is to perform voting (ensemble methods):\\n‚Ä¢ Instead of learning a single (weak) classiÔ¨Åer, learn many weak classiÔ¨Åers that are good at\\ndiÔ¨Äerent parts of the input space.\\n‚Ä¢ Output class: (Weighted) vote of each classiÔ¨Åer\\n‚Äì ClassiÔ¨Åers that are most ‚Äúsure‚Äù will vote with more conviction\\n‚Äì Each classiÔ¨Åer will be most ‚Äúsure‚Äù about a particular part of the space\\n‚Äì On average, do better than single classiÔ¨Åer!\\nThe question, then, is how we can force classiÔ¨Åers to learn about diÔ¨Äerent parts of the input\\nspace and weigh the votes of diÔ¨Äerent classiÔ¨Åers? This leads us to the idea of boosting:\\n‚Ä¢ Idea: given a weak learner, run it multiple times on ( reweighted) training data, then let\\nthe learned classiÔ¨Åers vote.\\n‚Ä¢ On each iteration t:\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 43, 'page_label': '43', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Äì weigh each training example by how incorrectly it was classiÔ¨Åed\\n‚Äì learn a hypothesis ht and its strength Œ±t\\n‚Ä¢ Final classiÔ¨Åer: a linear combination of the votes of the diÔ¨Äerent classiÔ¨Åers weighted by\\ntheir strength\\nNote the notion of a weighted dataset - in particular, if D(i) be the weight of the i-th training\\nexample (x(i),y (i)), then it counts as D(i) examples. From now, in all calculations, whenever\\nused, the i-th training example counts as D(i) ‚Äúexamples‚Äù.\\nWith this in mind, we can now deÔ¨Åne the full boosting algorithm (Shapire, 1998):\\nAlgorithm 17: (AdaBoost)\\nGiven dataset {(x(1),y (1)),..., (x(m),y (m))} where x(i) ‚ààX,y (i) ‚ààY = {¬±1}.\\nInitialize D1(i) = 1\\nm.\\nFort = 1,...,T :\\n‚Ä¢ Train weak learner using distribution Dt.\\n‚Ä¢ Get weak classiÔ¨Åer ht :X ‚Üí R.\\n‚Ä¢ Compute the error œµt = ‚àëm\\ni=1Dt(t)I\\n(\\nht(x(i)) Ã∏=y(i))\\nand strength Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n.\\n‚Ä¢ Update\\nDt+1(i) = Dt(i) exp(‚àíŒ±ty(i)ht(x(i))\\nZt\\nwhere\\nZt =\\nm‚àë\\ni=1\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\nis a normalization factor (chosen so that ‚àëm\\ni=1Dt+1(i) = 1).\\nOutput the Ô¨Ånal classiÔ¨Åer H(x) = sign\\n( T‚àë\\nt=1\\nŒ±tht(x)\\n)\\n.\\nAn example is shown below.\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 44, 'page_label': '44', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 45, 'page_label': '45', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='8.2 Mathematical details\\nWe now examine at each of the above formulas to see how they were derived. First observe that:\\n‚Ä¢ œµt is the fraction of misclassiÔ¨Åed (weighted) samples in iteration t. Recall our earlier\\ndeÔ¨Ånition of a weak learner as one that has error < 50%. Hence we always have œµt < 0.5\\nand therefore Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n> 0.\\n‚Ä¢ If sample i is classiÔ¨Åed correctly then y(i)ht(x(i)) = 1 so\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) = Dt(i)\\nexp(Œ±i) <D t(i),\\ni.e., the weight of this sample decreases. On the other hand, if sample i is classiÔ¨Åed\\nincorrectly, then its weight increases.\\nIn other words, in each iteration the classiÔ¨Åer will focus on a diÔ¨Äerent set of points (more\\nspeciÔ¨Åcally, those that were misclassiÔ¨Åed in the previous iteration), and in the end, we hope that\\nthe combination of these classiÔ¨Åers (over all iterations) can classify all points correctly. This is\\nthe idea behind boosting. Now, to prove that it does work (i.e., the error converges to 0 after\\nsome number of iterations), we answer the following questions.\\n8.2.1 What Œ±t to choose for hypothesis ht?\\nNote that the training error of the Ô¨Ånal classiÔ¨Åer H is bounded by\\n1\\nm\\nm‚àë\\ni=1\\nI\\n(\\nH(x(i)) Ã∏=y(i))\\n‚â§ 1\\nm\\nm‚àë\\ni=1\\nexp(‚àíy(i)f(x(i))) =\\nT‚àè\\nt=1\\nZt (8.1)\\nwhere f(x) = ‚àëT\\nt=1Œ±tht(x) and H(x) = sign (f(x)).\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 12 - Upper bound of Ô¨Ånal classiÔ¨Åer‚Äôs training error\\nTo prove the Ô¨Årst inequality, note that:\\n‚Ä¢ Each correctly classiÔ¨Åed sample i contributes 0 to the LHS and 1\\nme to the RHS.\\n‚Ä¢ Each incorrectly classiÔ¨Åed sample i contributes 1\\nm to the LHS and e\\nm to the RHS.\\nIn other words, I\\n(\\nH(x(i)) Ã∏=y(i))\\n< exp(‚àíy(i)f(x(i))) for all i, so ‚àë\\ni I\\n(\\nH(x(i)) Ã∏=y(i))\\n<‚àë\\ni exp(‚àíy(i)f(x(i))).\\nTo prove the second inequality, note that the deÔ¨Ånition of Dt gives us\\nDT +1(i) =DT (i) ¬∑ exp(‚àíŒ±Ty(i)hT (x(i)))\\nZT\\n=DT‚àí1(i) ¬∑ exp(‚àíŒ±T‚àí1y(i)hT‚àí1(x(i)))\\nZT‚àí1\\n¬∑ exp(‚àíŒ±Ty(i)hT (x(i)))\\nZT\\n=...\\n= exp(‚àí ‚àë\\ntŒ±ty(i)ht(x(i)))\\nm ‚àè\\ntZt\\n= exp(‚àíy(i)f(x(i)))\\nm ‚àè\\ntZt\\n.\\nOn the other hand, because we deÔ¨Åne the Zt‚Äôs as normalization factors,\\n1 =\\nm‚àë\\ni=1\\nDT +1(i) = 1\\nm ‚àè\\ntZt\\nm‚àë\\ni=1\\nexp(‚àíy(i)ht(x(i))),\\nwhich leads to\\n1\\nm\\nm‚àë\\ni=1\\nexp(‚àíy(i)f(x(i))) =\\nT‚àè\\nt=1\\nZt.\\nIn other words, to guarantee low error, we just need to make sure its upper bound ‚àè\\ntZt is small.\\nWe can tighten this bound greedily by choosing Œ±t on each iteration to minimize Zt.\\nTo do so, let‚Äôs deÔ¨Åne the error at iteration t as œµt = ‚àë\\niDt(i)I\\n(\\nht(x(i)) Ã∏=y(i))\\n.\\nIt then follows that\\nZt =\\n‚àë\\nt\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) = (1 ‚àíœµt) exp(‚àíŒ±t) +œµt exp(Œ±t).\\nWe can then choose Œ±t that minimizes Zt by solving ‚àÇZt\\n‚àÇŒ±t\\n= 0, which yields Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n.\\nFurther note that Œ±t and œµt are negatively correlated, so intuitively Œ±t is the ‚Äústrength‚Äù of\\nclassiÔ¨Åer ht. Hence, we have shown how to minimize ‚àè\\ntZt, but how small can this minimum\\nvalue be?\\n8.2.2 Show that training error converges to 0\\nWe can further derive another upper bound for the training error:\\nerr(H) = 1\\nm\\nm‚àë\\ni=1\\nI\\n(\\nH(x(i)) Ã∏=y(i))\\n‚â§\\nT‚àè\\nt=1\\nZt ‚â§ exp\\n(\\n‚àí2\\n‚àë\\nt\\nŒ≥2\\nt\\n)\\n(8.2)\\nwhere Œ≥t = 1\\n2 ‚àíœµt.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 47, 'page_label': '47', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 13 - Convergence of AdaBoost‚Äôs training error\\nNote that\\nŒ±t = 1\\n2 ln\\n(1 ‚àíœµt\\nœµt\\n)\\n‚áí exp(Œ±t) =\\n‚àö\\n1 ‚àíœµt\\nœµt\\n,\\nand also\\nœµt =\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i); 1 ‚àíœµt =\\n‚àë\\ny(i)=ht(x(i))\\nDt(i).\\nWe now have\\nZt =\\n‚àë\\ni\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) +\\n‚àë\\ny(i)=ht(x(i))\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n(‚àö œµt\\n1 ‚àíœµt\\n)\\n=\\n‚àö\\n1 ‚àíœµt\\nœµt\\n¬∑\\n\\uf8eb\\n\\uf8ed ‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n\\uf8f6\\n\\uf8f8 +\\n‚àö œµt\\n1 ‚àíœµt\\n¬∑\\n\\uf8eb\\n\\uf8ed ‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n\\uf8f6\\n\\uf8f8\\n=\\n‚àö\\n1 ‚àíœµt\\nœµt\\n¬∑œµt +\\n‚àö œµt\\n1 ‚àíœµt\\n¬∑ (1 ‚àíœµt)\\n= 2\\n‚àö\\nœµt(1 ‚àíœµt).\\nFurthermore, for any x ‚àà R, 1 ‚àíx ‚â§ exp(‚àíx). Substitute x = 4Œ≥2\\nt we see that\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí4Œ≥2\\nt ) ‚áí\\n‚àö\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí2Œ≥2\\nt ).\\nNow we have\\nZt = 2\\n‚àö\\nœµt(1 ‚àíœµt) =\\n‚àö\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí2Œ≥2\\nt ),\\nhence\\nerr(H) ‚â§\\nT‚àè\\nt=1\\nZt ‚â§\\nT‚àè\\nt=1\\nexp(‚àí2Œ≥2\\nt ) = exp(‚àí2\\n‚àë\\nt\\nŒ≥2\\nt ).\\nIt then follows that, as the number of iterations T increases, exp( ‚àí2 ‚àëT\\nt=1Œ≥2\\nt ) decreases expo-\\nnentially, so the training error also approaches 0 exponentially fast.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 48, 'page_label': '48', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In practice, this also happens quite quickly. In fact, Schapire (1989) showed that in digit recog-\\nnition, the testing error can still decrease even after the training error reaches 0 1. Boosting is\\nalso robust to overÔ¨Åtting.\\nSome weak learners also have their own ensemble methods apart from AdaBoost. For example,\\nan ensemble of decision tree is called a random forest. For each tree we select a subset of\\nattributes (recommended subset size = square root of number of total attributes) and build the\\ntree using only the selected attributes. An input sample is the classiÔ¨Åed using majority voting.\\n1Note that the training error reported in this graph is the global training error where each input is weighed\\nequally as usual. During the iterations of AdaBoost, however, we are concerned with the weighted errors œµt. In\\nthis case, while the global training error is 0, the œµt‚Äôs may still be > 0, so there is room for improvement, and\\ntherefore the test error can still decrease.\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 9\\nPrincipal Component Analysis\\n9.1 Introduction\\nSuppose we are given data points in d-dimensional space and want to project them into a\\nlower dimensional space while preserving as much information as possible (e.g., Ô¨Ånd best planar\\napproximation to 3D data or 10 4D data). In particular, choose an orthogonal projection that\\nminimizes the squared error in reconstructing original data.\\nLike auto-encoding neural networks, PCA learns re-representation of input data that can best\\nreconstruct it. However, PCA has some diÔ¨Äerences:\\n‚Ä¢ The learned encoding is a linear function of inputs\\n‚Ä¢ No local minimum problems when training\\n‚Ä¢ Givend-dimensional data X, learns d-dimensional representation where:\\n‚Äì the dimensions are orthogonal\\n‚Äì The top k dimensions are the k-dimensional linear re-representation that minimizes\\nreconstruction error (sum of squared errors)\\nIn particular, PCA involves orthogonal projection of the data onto a lower-dimensional linear\\nspace that equivalently:\\n1. minimizes the mean squared distance between data points and projections\\n2. maximizes variance of projected data\\nPCA has the following properties:\\n‚Ä¢ PCA vectors originate from the center of mass (usually we center the data as the Ô¨Årst step)\\n‚Ä¢ Principal component #1: points in the direction of the largest variance\\n‚Ä¢ Each subsequent principal component:\\n‚Äì is orthogonal to the previous ones, and\\n‚Äì points in the directions of the largest variance of the residual subspace\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 50, 'page_label': '50', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='9.2 PCA algorithms\\nHere we present 3 diÔ¨Äerent algorithms for performing PCA.\\nAlgorithm 18: (Sequential PCA)\\nGiven centered data {x(1),x (2),...,x (m)}, compute the principal vectors:\\nw1 = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wTx(i))2\\nw2 = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àíw1wT\\n1x(i)))2\\n...\\nwk = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àí\\nk‚àí1‚àë\\nj=1\\nwjwT\\njx(i)))2\\nIn the Sequential algorithm, to Ô¨Ånd w1, we maximize the variance of projection of x. To\\nÔ¨Ånd w2, we maximize the variance of the projection in the residual subspace.\\nThe Sequential algorithm is intuitive and gives a sense of what to look for in a principal vector.\\nHowever, it is slow and not often used in practice unless we only care about the Ô¨Årst principal\\nvector.\\nAlgorithm 19: (Sample covariance matrix PCA)\\nGiven data {x(1),x (2),...,x (m)}, compute covariance matrix\\nŒ£ = 1\\nm\\nm‚àë\\ni=1\\n(x(i) ‚àí ¬Øx)(x(i) ‚àí ¬Øx)T\\nwhere ¬Øx = 1\\nm\\n‚àëm\\ni=1x(i). The principal vectors are the eigenvectors of Œ£, and larger eigenval-\\nues corresponds to more important eigenvectors.\\nWhile straightforward, the computation of Œ£ and its eigenvectors is computationally expensive.\\nIn practice, the most useful method is by singular value decomposition (SVD), which avoids\\nexplicitly computing Œ£.\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 20: (SVD PCA)\\nPerform SVD of the centered data matrix X = (x(1),...,x (m)) into\\nX =USV T\\nwhere U and V are orthonormal and S is a diagonal matrix. The columns of V are the\\neigenvectors and the diagonal values of S - which are square roots of the eigenvectors of V -\\ndenote the importance of each eigenvector in descending order. The top k principal vectors\\nare the columns of VT are the leftmost k columns of V .\\nFormally, the SVD of a matrix A is just the decomposition of A into three other matrices, which\\nwe call U, S, and V . The dimensions of these matrices are given as subscripts in the formula\\nbelow:\\nAn√óm =Un√ónSn√ómVT\\nm√óm.\\nThe columns of U are orthonormal eigenvectors of AAT . The columns of V are orthonormal\\neigenvectors of ATA. The matrix S is diagonal, with the square roots of the eigenvalues from\\nU (or V ; the eigenvalues of ATA are the same as those of AAT ) in descending order. These\\neigenvalues are called the singular values of A.\\n9.3 PCA applications\\nThe main applications of PCA are:\\n‚Ä¢ Data visualization - by reducing the number of dimensions to 2 or 3, we can plot the data\\npoints on a graph\\n‚Ä¢ Noise reduction, e.g. eigenfaces\\n‚Ä¢ Data compression\\n9.3.1 Eigenfaces\\nWe want to identify speciÔ¨Åc person, based on facial image, robust to glasses, lighting, facial\\nexpression, ... (i.e., they are considered noise in this case). Each image is 256 √ó 256 pixels so\\neach input x is 2562 = 65536 dimensional.\\nSince the number of dimensions is too large, we cannot perform classiÔ¨Åcation directly. Instead,\\nwe use PCA on the whole dataset to get ‚Äúprincipal component‚Äù images (the eigenfaces), then\\nclassify based on projection weights onto these principal component images.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Suppose there arem instances each with dimensionN (in this casem = 50,,N = 65536). Given\\na N √óN covariance matrix Œ£, can compute:\\n‚Ä¢ All N eigenvectors / eigenvalues in O(N 3)\\n‚Ä¢ First k eigenvectors / eigenvalues in O(kN 2)\\nBut this is expensive if N = 65536. However, there is a clever workaround, since we note that\\nm ‚â™ 65536. SpeciÔ¨Åcally, we Ô¨Årst compute the eigenvectors v‚Äôs of L = XTX (which is much\\nsmaller, only m √óm), then for each v, Xv would be an eigenvector of XTX.\\nDiving in the Math 14 - Proof of workaround for eigenfaces\\nWe want to prove that ifv is eigenvector of L =XTX thenXv is eigenvector of Œ£ = XXT .\\nBased on the deÔ¨Ånition of eigenvector, there exists Œ≥ such that\\nLv =Œ≥v\\nXTXv =Œ≥v\\nX(XTXv) =X(Œ≥v) =Œ≥Xv\\n(XXT )Xv =Œ≥(Xv)\\nŒ£(Xv) =Œ≥(Xv)\\nAgain, using the deÔ¨Ånition of eigenvector, we see that Xv is an eigenvector of Œ£, also with\\neigenvalueŒ≥.\\nIn other words, we do not have to compute the eigenvalues of Œ£ directly from Œ£, but through\\nL. This would reduce the runtime to O(Nm 2) +O(km2), where k is the speciÔ¨Åed number of\\neigenvectors (i.e., the number of dimensions we want to reduce to).\\nWe can then reconstruct the faces using some of the top principal vector. As more eigenvectors\\nare used, we get back more detailed faces but without noises such as lighting, glasses and facial\\nexpression. The below Ô¨Ågures demonstrate we can reconstruct one particular face, starting from\\nusing only one principal vectors, then adding more and more. The circled face denotes the best\\napproximation without noise.\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 53, 'page_label': '53', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Note: this method is quite old. Nowadays we use deep neural network.\\n9.3.2 Image compression\\nTo compress an image, we can divide it in patches (12 √ó 12 pixels on a grid), so each patch is a\\n144-D vector input. Using PCA on these inputs, reconstructing the patches, then putting them\\nback together again will give us the compressed version. In some cases, using only 13 principal\\nvectors can already reduce the relative error to 5% (i.e., most information is in the top principal\\nvectors).\\n9.4 Shortcomings\\nPCA is unsupervised and doesn‚Äôt care about the labels. It maximizes the variance, independence\\nof class. For example, in the plot below, if we want to reduce the dimension to 1 while preserving\\nclass separations, we would pick the green line. However, PCA would pick the magenta line\\ninstead.\\nFurthermore, PCA can only capture linear relationships.\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 54, 'page_label': '54', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 10\\nHidden Markov Model\\n10.1 Introduction\\nDeÔ¨Ånition 11: (Hidden Markov Model)\\nA Hidden Markov Model consists of the followings:\\n‚Ä¢ A set of states S = {s1,s 2,...,s n}. At each time t we are in exactly one of these\\nstates, denoted by qt.\\n‚Ä¢ A list œÄ1,œÄ 2,...,œÄ n where œÄi is the probability that we start at state i.\\n‚Ä¢ A transition probability matrix Aj,i = P (qt = si | qt‚àí1 = sj), which denotes the\\nprobability of transitioning from sj to si.\\n‚Ä¢ A set of possible outputs Œ£, at each time t we emit a symbol œÉt ‚àà Œ£.\\n‚Ä¢ An emission probability matrix Bt,i = P (ot | si), which denotes the probability of\\nemitting symbol œÉt at state si.\\nFor example, a two-state HMM may look like the followings:\\nFigure 10.1: An example Hidden Markov Model with 2 states.\\nNote the Markov property from the above deÔ¨Ånition: given qt, qt+1 is conditionally inde-\\npendent on qt‚àí1 or any earlier time point . In other words, knowing qt is suÔ¨Écient to infer\\nabout the state of qt+1.\\nWith n states and m output symbols, we can see that there are n starting parameters œÄi, n2\\ntransition probabilities Aji, and mn emission probabilities Bik, for a total of n2 +mn +n =\\nO(n2 +mn) parameters. We will discuss how to learn these parameters from data later on, but\\nlet‚Äôs Ô¨Årst focus on the inference task. Assuming all of these parameters are already known, what\\nkind of information can we infer?\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 55, 'page_label': '55', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='10.2 Inference in HMM\\nThere are three big questions that a learned HMM can answer:\\n1. What is P (qt =si)? In other words, what is the probability that we end up in state si at\\ntime t, without observing any output ?\\n2. What is P (qt = si | o1o2...o t)? In other words, given a sequence of output symbols\\no1o2...o t, what is the probability that we end up in state si at time t?\\n3. What is arg max\\nq1q2...qt\\nP (q1q2...q t | o1o2...o t)? In other words, given a sequence of output\\nsymbolso1o2...o t, what is the sequence of states q1q2...q t that is most likely to generate\\nthis output?\\nBefore moving on, we show an example application of these questions. A popular technique of\\nmodeling student knowledge in Educational Data Mining is called Bayesian Knowledge Tracing.\\nThe high-level goal is: given data about the correctness of a student‚Äôs answers on a set of\\nproblems related to a certain skill, can we say whether the student has mastered this skill?\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='DeÔ¨Ånition 12: (Bayesian Knowledge Tracing)\\nFor each skill, BKT models student knowledge as a binary latent variable in an HMM ,\\nwhich consists of the followings:\\n‚Ä¢ Two states: S = {Mastered, Unmastered}.\\n‚Ä¢ œÄMastered: the probability of the student starting at Mastered, i.e., knowing the skill\\nbeforehand.\\n‚Ä¢ pLearn: the probability of transitioning from Unmastered to Mastered. pForget: the\\nprobability of transitioning from Mastered to Unmastered.\\n‚Ä¢ Two possible outputs C (correct) and I (incorrect): whether student‚Äôs answer to a\\nproblem is correct or incorrect.\\n‚Ä¢ pGuess = p(ot = C | qt = Unmastered): the probability of getting a correct answer\\ndespite not mastering the skill, i.e., guessing.\\n‚Ä¢ pSlip = p(ot = I | qt = Mastered): the probability of getting an incorrect answer\\ndespite mastering the skill, i.e., slipping.\\nMastered Unmastered\\nC I\\npForget\\n1 ‚àípForget\\npLearn\\n1 ‚àípLearn\\n1 ‚àípGuess\\npSlip pGuess\\n1 ‚àípSlip\\nUsing Inference #2, we can ask questions like: if the student submits 5 answers and gets the\\nÔ¨Årst 3 correct but last 2 incorrect, what is the probability that she has mastered the skill? More\\nformally, what isP (Mastered | CCCII)? Is this diÔ¨Äerent from, for example,P (Mastered | IICCC)\\n(getting Ô¨Årst 2 incorrect but last 3 correct)?\\n10.2.1 What is P (qt =si)?\\nSince we don‚Äôt have observed data, the emission probabilities can be ignored. Instead, we simply\\nrely on the priors and transition probabilities. For example, in Figure 10.1 we can compute\\nP (q2 =A) as\\nP (q2 =A) =P (q2 =A |q1 =A) ¬∑p(q1 =A) +p(q2 =A |q1 =B) ¬∑p(q1 =B)\\n= AAA ¬∑œÄA + ABA ¬∑œÄB,\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='and then P (q3 =B) as\\nP (q3 =B) =P (q3 =B |q2 =A) ¬∑p(q2 =A) +P (q3 =B |q2 =B) ¬∑p(q2 =B)\\n= AAB ¬∑ (AAA ¬∑œÄA + ABA ¬∑œÄB) + ABB ¬∑ (AAB ¬∑œÄA + ABB ¬∑œÄB).\\nIn general,\\nP (qt =si) =\\n‚àë\\nq1,q2,...,qt‚àí1‚ààS\\nP (si |q1q2...q t‚àí1)P (q1q2...q t‚àí1). (10.1)\\nHowever, this is too costly to compute, with runtime O(2n‚àí1). Instead, an optimization trick is\\nto use dynamic programming:\\nAlgorithm 21: (Computing Ô¨Ånal state without observations)\\nWe perform two steps:\\n‚Ä¢ Base case: P (q1 =si) =œÄi\\n‚Ä¢ Inductive case:\\nP (qt+1 =si) =\\n‚àë\\nj‚ààS\\nP (qt+1 =si |qt =sj) ¬∑P (qt =sj) =\\n‚àë\\nj‚ààS\\nAji ¬∑P (qt =sj). (10.2)\\n10.2.2 What is P (qt =si |o1o2...o t)?\\nUsing the chain rule, we Ô¨Årst see that\\nP (qt =si |o1o2...o t) = P (qt =si ‚àßo1o2...o t)\\nP (o1o2...o t) = P (qt =si ‚àßo1o2...o t)‚àë\\nj‚ààS\\nP (qt =sj ‚àßo1o2...o t)\\n(10.3)\\nThis motivates us to deÔ¨ÅneŒ±t(i) =P (qt =si ‚àßo1o2...o t). We can then computeŒ±t(i) as follows.\\nAlgorithm 22: (Computing Ô¨Ånal state given observed output sequence)\\nTo computeŒ±t(i), we perform two steps:\\n‚Ä¢ Base case: Œ±1(i) =P (q1 =si ‚àßo1) =P (o1 |q1 =s1)P (q1 =s1) = B1,i ¬∑œÄi.\\n‚Ä¢ Inductive case:\\nŒ±t+1(i) =\\n‚àë\\nj‚ààS\\nŒ±t(j) ¬∑ Aj,i ¬∑ Bt+1,i. (10.4)\\nIt follows that\\nP (qt =si |o1o2...o t) = Œ±t(i)‚àë\\nj‚ààS\\nŒ±t(j)\\n(10.5)\\n10.2.3 What is arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t)?\\nLet\\nŒ¥t(i) = max\\nq1,...,qt‚àí1‚ààS\\nP (q1...q t‚àí1 ‚àßqt =si ‚àßo1...o t). (10.6)\\nIn other words, Œ¥t(i) is the probability of the most likely path from time 1 to t that produces\\noutput o1...o t and ends in si. We can then compute Œ¥t(i) as follows.\\n57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 58, 'page_label': '58', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 23: (Viterbi Algorithm)\\nWe perform two steps:\\n‚Ä¢ Base case: Œ¥1(i) =œÄi ¬∑ B1,i.\\n‚Ä¢ Inductive case:\\nŒ¥t+1(i) = max\\nj\\nŒ¥t(i) ¬∑ Aj,i ¬∑ Bt+1,i. (10.7)\\nIt follows that arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t) is the path deÔ¨Åned by arg max\\nj\\nŒ¥t(j).\\n58'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 59, 'page_label': '59', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 11\\nReinforcement Learning\\n11.1 Markov decision process\\nA Markov decision process is a set of nodes and edges where nodes represent states and edges\\nrepresent transition. In this case, the transitions are only based on the previous state (same as\\nin HMM). Unlike HMM, however:\\n‚Ä¢ We know all the states, each state associated with a reward.\\n‚Ä¢ We can have an inÔ¨Çuence on the transition.\\nAn obvious question for such models: what is the combined expected value for each state? What\\ncan we expect to earn over our lifetime if we become asst. prof / go to industry? Before we\\nanswer this quesiton, we need to deÔ¨Åne a model for future rewards. In particular, the value of a\\ncurrent award is higher than the value of future awards (inÔ¨Çation, conÔ¨Ådence). This discounted\\nreward model is speciÔ¨Åed using a parameter 0 <Œ≥ < 1. Therefore, if we let rt be the reward at\\ntime t, then the total reward is\\nTotal =\\n‚àû‚àë\\nt=0\\nŒ≥trt, (11.1)\\n59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 60, 'page_label': '60', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='which does converge because Œ≥ ‚àà (0, 1).\\nNow, let‚Äôs deÔ¨Åne J‚àó(si) as the expected discounted sum of rewards when starting at state si. It\\nfollows that\\nJ‚àó(si) =ri +Œ≥\\nn‚àë\\nk=1\\npikJ‚àó(sk), (11.2)\\nwherepik is the transition probability from si tosk and the sum represents the expected pay for\\nall possible transitions from si.\\nWe have n equations like (11.2), one for each si, so this is a linear system of equations and\\na closed form solution can be derived, but may be time consuming. It also doesn‚Äôt generalize\\nto non-linear models. Alternatively, this problem can be solved in an iterative manner: deÔ¨Åne\\nJt(si) as the expected discounted reward after t steps, then J1(si) =ri and\\nJt+1(si) =ri +Œ≥\\n‚àë\\nk\\npikJt(sk). (11.3)\\nThis can be computed via dynamic programming, and we can stop when max\\ni\\n|Jt+1(si)‚àíJt(si)|<\\nœµ for some threshold œµ, which we know will happen because Jt(si) converges.\\n11.2 Reinforcement learning - No action\\nIn reinforcement learning, we use the same Markov model with rewards and actions. But there\\nare a few diÔ¨Äerences:\\n1. We do not assume we know the Markov model\\n60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2. We adapt to new observation (online vs oÔ¨Ñine)\\nIn other words, we want to learn the expected reward but do not know the model, and in this\\ncase we do so by learning both the reward and model at the same time (e.g., game playing, robot\\ninteracting with environment). Unlike HMM, if we move to a state, we know which state that is\\n(i.e., the states are observed); other than that, however, we don‚Äôt know the reward at that state\\nor the transition probabilities.\\n11.2.1 Supervised RL\\nMore formally, we deÔ¨Åne the scenario in reinforcement learning as follows: we are wandering the\\nworld, and at each time point we see a state and a reward. Our goal is to compute the sum of\\ndiscounted rewards for each state Jest(si). For example, given the following observations\\ns1, 4 s2, 0 s3, 2 s2, 2 s4, 0\\nIn general, we have the supervised learning algorithm for RL as follows.\\nAlgorithm 24: (Supervised reinforcement learning)\\nObserve set of states and rewards (s(0),r (0)), (s(1),r (1)),..., (s(T ),r (T )).\\nFort = 0,...,T compute discounted sum\\nJ(t) =\\nT‚àë\\ni=t\\nŒ≥i‚àítr(i).\\nCompute Jest(si) (mean of J(t) for t such that s(t) =si):\\nJest(si) =\\n‚àëT\\nt=0J(t)I (s(t) =si)‚àëT\\nt=0 I (s(t) =si)\\n.\\nHere we assume that we observe each state frequently enough and that we have many observa-\\ntions so that the Ô¨Ånal observations do not have a big impact on our prediction. Each update\\ntakesO(n) where n is the number of states, since we are updating vectors containing entries for\\nall states. Space is also O(n). Convergence to J‚àó can be proven, and the algorithm can be more\\neÔ¨Écient by ignoring states for which discounted factor Œ≥i is very low already.\\nHowever, the supervised learning approach has two problems:\\n61'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 62, 'page_label': '62', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Takes a long time to converge, because we don‚Äôt try to learn the underlying MDP model,\\nbut just focus on Jest.\\n‚Ä¢ Does not use all available data, we can learn transition probabilities as well.\\nIn other words, we want to utilize the fact that there is an underlying model and the transitions\\nare not completely random.\\n11.2.2 Certainty-Equivalence learning\\nAlgorithm 25: (Certainty-Equivalence (CE) learning)\\nWe keep track of 3 vectors:\\n‚Ä¢ Count(s): number of times we visited state s\\n‚Ä¢ J(s): sum of rewards from state s\\n‚Ä¢ Trans (i,j ): number of time we transitioned from si to sj\\nWhen we visit state si, receive reward r and move to state sj we do the following:\\n‚Ä¢ Counts(si) =Counts(si) + 1\\n‚Ä¢ J(si) =J(si) +r\\n‚Ä¢ Trans (i,j ) =Trans (i,j ) + 1\\nAt any time, we can estimate:\\n‚Ä¢ Reward estimate rest(s) =J(s)/Counts(s)\\n‚Ä¢ Transition probability estimate\\n‚Ä¢ pest(j |i) =Trans (i,j )/Counts(si)\\nAfter learning the model, we can now have an estimate which we can solve for all states si:\\nJest(si) =rest(si) +Œ≥\\n‚àë\\nj\\npest(sj |si)Jest(si), i = 1,...,n (11.4)\\n62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 63, 'page_label': '63', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='The runtime of CE comes from two steps: update (O(1)) and solving MDP (O(n3) using matrix\\ninversion). The space is O(n2) for transition probabilities.\\nTo reduce runtime, we could use the ‚ÄúOne backup‚Äù version, which updatesJest(si) for the current\\nstate si while learning the model , instead of solving n equations after learning like in (11.4). In\\nthis case, the runtime is only O(n), and we can sill prove convergence to J‚àó (but slower than\\nCE). The space remains at O(n2).\\n11.2.3 Temporal diÔ¨Äerence learning\\nWe now look at another algorithm with the same eÔ¨Éciency as one backup CE but requires much\\nless space. In particular, we can ignore all the rest and pest and only focus on Jest with a new\\napproximation rule.\\nAlgorithm 26: (Temporal diÔ¨Äerence (TD) learning)\\nWe only maintain the Jest array. Assume we have Jest(s1),...,J est(sn). If we observe a\\ntransition from state si to state sj and a reward r, we update using the following rule\\nJest(si) = (1 ‚àíŒ±)Jest(si) +Œ±(r +Œ≥J est(sj)), (11.5)\\nwhereŒ± is a hyper-parameter to determine how much weight we place on the current obser-\\nvation (and can change during the algorithm, unlike Œ≥).\\nAs always, choosing a good Œ± is an issue. Nevertheless, it can be proven that TD learning is\\nguaranteed to converge if:\\n‚Ä¢ All states are visited often.\\n‚Ä¢ ‚àë\\ntŒ±t = ‚àû\\n‚Ä¢ ‚àë\\ntŒ±2\\nt < ‚àû\\n63'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='For example,Œ±t = C\\nt for some constant t would satisfy both requirements.\\nNow the runtime of TD is O(1) because there is only one update (11.5) at each iteration, and\\nthe space is O(n) because of the Jest array.\\nHere is a summary so far of the four reinforcement learning algorithms.\\nMethod Time Space\\nSupervised learning O(n) O(n)\\nCE learning O(n3) O(n2)\\nOne backup CE O(n) O(n2)\\nTD learning O(1) O(n)\\n11.3 Reinforcement learning with action - Policy learning\\nSo far we assumed that we cannot impact the outcome transition. In real world situations we\\noften have a choice of actions we take (as we discussed for MDPs). How can we learn the best\\npolicy for such cases?\\nNote the diÔ¨Äerence in the pest table - while the columns are still the states (because we only\\ntransition from state to state), the rows are now (state, action) pair, because each action leads\\nto a diÔ¨Äerent transition. Our goal is to learn the action that leads to the most reward. In\\nparticular, we can update CE by setting\\nJest(si) =rest(si) + max\\na\\n(\\nŒ≥\\n‚àë\\nj\\npest(sj |si,a )Jest(sj)\\n)\\n. (11.6)\\nAs mentioned above, we can also use TD learning for better eÔ¨Éciency. However, TD is model\\nfree, so in this context, we can adjust TD to learn policies by deÔ¨Åning Q‚àó(si,a ) = expected sum\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 65, 'page_label': '65', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='of future (discounted) rewards if we start at state si and take action a. Then, when we take a\\nspeciÔ¨Åc action a in state si and then transition to state sj we can update\\nQest(si,a ) = (1 ‚àíŒ±)Qest(si,a ) +Œ±(ri +Œ≥ max\\na‚Ä≤\\nQest(sj,a‚Ä≤)). (11.7)\\nInstead of the Jest vector we maintain the Qest matrix, which is a rather sparse n by m matrix\\n(n states and m actions).\\nIn practice, when choosing the next action, we may not necessarily pick the one that results in\\nthe highest expected sum of future rewards, because we are only sampling from the distribution\\nof possible outcomes. We do not want to avoid potentially beneÔ¨Åcial actions. Instead, we can\\ntake a more probabilistic approach\\np(a) = 1\\nZ exp\\n(\\n‚àíQest(si,a )\\nf(t)\\n)\\n, (11.8)\\nwhere Z is a normalizing constant and f(t) decreases as time t goes by, to represent that we\\nare more conÔ¨Ådent in the learned model. We can initialize Q values to be high to increase the\\nlikelihood that we will explore more options. Finally, it can be shown that Q learning converges\\nto optimal policy.\\n65'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 12\\nGeneralization and Model Selection\\n12.1 True risk vs Empirical risk\\nDeÔ¨Ånition 13: (True risk)\\nTrue risk is the target performance measure. It is deÔ¨Åned as is the probability of misclassiÔ¨Å-\\ncationP (f(X) Ã∏=Y ) in classiÔ¨Åcation and mean squared error E[(f(X) ‚àíY )2] in regression.\\nMore generally, it is the expected performance on a random test point ( X,Y ).\\nWhile we want to minimize true risk, we do not know that the underlying distribution of X and\\nY is. What we do know are the samples ( Xi,Yi), which give us the empirical risk.\\nDeÔ¨Ånition 14: (Empirical risk)\\nEmpirical risk is the performance on training data. It is deÔ¨Åned as proportion of misclassiÔ¨Åed\\nexamples 1\\nn\\n‚àën\\ni=1 I (f(Xi) Ã∏=Yi) in classiÔ¨Åcation and average squared error 1\\nn\\n‚àën\\ni=1(f(Xi) ‚àí\\nYi)2 in regression.\\nSo we want to minimize the empirical risk and evaluate the true risk, but this may lead to\\noverÔ¨Åtting (i.e., small training error but large generalization error). For instance, the following\\ngraph shows two classiÔ¨Åers for a binary classiÔ¨Åcation problem (football player or not). While\\nthe classiÔ¨Åer on the right has zero training error, we are much less inclined to believe that it\\ncaptures the true distribution. Here it is more likely that football players simply have higher\\nheight and weight, which match better with the classiÔ¨Åer on the left.\\nThe question is: when should we not minimize the empirical risk completely? The following\\ngraph shows what the empirical risk and true risk may look like as we increase the model\\ncomplexity. Initially both types of risk would decrease, but after some point (the Best Model\\n66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 67, 'page_label': '67', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='point), we started Ô¨Åtting the noise instead of the true data. In that case, the empirical risk can\\nkeep decreasing while the true risk increases.\\nAgain, we do not know how true risk in practice, which makes this a diÔ¨Écult problem. Can we\\nestimate the true risk in a way better than just using the empirical risk? One way is to use\\nstructural risk minimization.\\nDeÔ¨Ånition 15: (Structural risk minimization)\\nPenalize models using bound on deviation of true and empirical risk\\nÀÜfn = arg min\\nf‚ààF\\n{ ÀÜRn(f) +ŒªC(f)}, (12.1)\\nwhereŒª is a tuning parameter chosen by model selection, andC(f) is the bound on deviation\\nfrom true risk a. In essence, instead of minimizing the unknown true risk directly, we try to\\nminimize an upper bound (with high probability) on the true risk.\\naWe will discuss how to derive these later.\\nIn other words, we penalize models based on prior information (bias) or information criteria\\n(MDL, AIC, BIC). In ML there is a ‚Äúno free lunch‚Äù theorem: given only the data, we cannot\\nlearn anything. We need some kind of prior information (inductive bias); for example, in using\\nlinear regression, our inductive bias is that the data can be Ô¨Åt by a line. The inductive bias\\nin this case, also called Occam‚Äôs Razor, is to seek the simplest explanation (e.g., if a 10-degree\\n67'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 68, 'page_label': '68', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='polynomial and 100-degree polynomial say roughly the same things, pick the former).\\nInductive bias can also come from domain knowledge. For example, the function of oil spill\\ncontamination should be smooth (if one point is contaminated, the points around it should be\\nas well), while the function of photon arrival is not. Therefore, even if we get the same data,\\nthe Ô¨Åt functions may look very diÔ¨Äerent.\\nAn example of penalizing complex models using prior knowledge is regularized linear regres-\\nsion, which uses some norm of regression coeÔ¨Écients as the cost C(f). An example of penal-\\nizing models based on information content is AIC ( C(f) = # parameters) or BIC ( C(f) =\\n# parameters √ó logn). AIC allows # parameters to be inÔ¨Ånite as # of training data n becomes\\nlarge, while BIC penalizes complex models more heavily.\\n12.2 Model Selection\\n68')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dadea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=500,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    #show example of chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk :\")\n",
    "        print(f\"Content : {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata :  {split_docs[0].metadata}\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b9d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 108 documents into 337 chunks\n",
      "\n",
      "Example chunk :\n",
      "Content : NATURAL LANGUAGE PROCESSING 101Sarah Rodenbeck, Lead Research Data ScientistRosen Center for Advanced Computing\n",
      "1...\n",
      "Metadata :  {'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 0, 'page_label': '1', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 0, 'page_label': '1', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NATURAL LANGUAGE PROCESSING 101Sarah Rodenbeck, Lead Research Data ScientistRosen Center for Advanced Computing\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 1, 'page_label': '2', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Agenda‚Ä¢NLPPrimer‚Ä¢Mechanics & Evolution of NLP‚Ä¢Getting started with NLP\\n2'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 2, 'page_label': '3', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Primer\\n3'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 3, 'page_label': '4', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Text data is everywhere!\\n4'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 4, 'page_label': '5', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Key Idea of NLP\\n5\\nWith a sufficiently large corpus of text data, models can learn the patterns of language'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 5, 'page_label': '6', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='When have you used NLP?\\n6'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can you do with NLP?\\n7\\nCore DomainDescriptionExampleText ClassificationGrouping documents into categoriesSpam FilterInformation ExtractionIdentifying information from text Automatic calendar event creation when times mentionedInformation RetrievalFinding relevant informationSearch EnginesQuestion Answering SystemsAnswering questions based on a natural language questionSiri/AlexaMachine Translation/SummarizationConverting a sequence of text to another with the same meaningGoogle Translate'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Natural Language GenerationGenerate new text based on a promptChat GPT'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 7, 'page_label': '8', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can‚Äôt you do with NLP?\\n8\\nLogical Reasoning'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 8, 'page_label': '9', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='What can‚Äôt you do with NLP?\\n9\\nNLP doesn‚Äôt truly understand language!'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 9, 'page_label': '10', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Challenges of NLP -Ambiguity\\n10\\nThe animal didn‚Äôt cross the street because itwas too tired.------versus ------The animal didn‚Äôt cross the street because itwas too wide.\\nI ranto the store because we ranout of bread.Can I runsomething past you?That house is really rundown.\\nI lovetaking tests \\nüôÑ'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 10, 'page_label': '11', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Challenges of NLP -Language\\n11'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 11, 'page_label': '12', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Ethics\\n12'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 12, 'page_label': '13', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content=\"NLP Ethics\\n13\\n¬°Have our algorithms been tested on diverse data?¬°Are our algorithms equally performant on all groups?\\nAccountability¬°How are we holding ourselves accountable if AI makes a mistake?¬°What recourse is available and how do we ensure the issue doesn't happen again?\\nFairness\"),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 13, 'page_label': '14', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content=\"NLP Ethics\\n14\\nTransparency\\nEthics\\n¬°Are we transparent about how we're using AI?¬°Do we allow outside researchers or watchdogs to examine our use of AI?¬°Are the applications we're using AI for ethical?\"),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 14, 'page_label': '15', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Mechanics & Evolution\\n15'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 15, 'page_label': '16', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Quick Review of Key ML Terms\\n16\\n‚Ä¢Unsupervised Learning: neural network used patterns in unlabeled data, e.g., clustering‚Ä¢SupervisedLearning:Labelleddatausedtohelpthemodel‚Äúlearn‚Äùhowtodoaparticulartask, e.g., classification‚Ä¢Transferlearning:Reusinggeneralinformationlearned from a previous task for a new task; speeds up training and reduces data requirements‚Ä¢Pre-training:Generallearning‚Ä¢Fine-tuning: Tweaking the pre-trained model for a downstream task'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 16, 'page_label': '17', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='EvolutionofNLP\\n17\\n1950s-1990sSymbolic NLPStatistical NLPNeural NLP1990s-20102010s-Present\\n‚Ä¢Expert rule-based systems hand-coded by linguists‚Ä¢Keyachievements:‚Ä¢Georgetown Experiment‚Ä¢ELIZA\\n‚Ä¢Use similarities between words to compete tasks‚Ä¢Key achievements:‚Ä¢Statistical Machine Translation‚Ä¢Latent Semantic Indexing/TFIDF‚Ä¢FirstuseofNNforlanguagemodelling\\n‚Ä¢Rapid advancement in NLP thanks to more data and hardware‚Ä¢Keyachievements:‚Ä¢Word embeddings‚Ä¢AttentionandTransformer‚Ä¢Large language models'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 17, 'page_label': '18', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n18\\nEmbedding ideally captures:‚Ä¢Meaning of words‚Ä¢Similarities/differences between words‚Ä¢Contextual meaning of words'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 18, 'page_label': '19', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n19\\n‚ÄúYou shall know a word by the company it keeps‚Äù ‚Ä¢A word‚Äôs meaning can be understood based on the words it frequently appears close to‚Ä¢Use the many contexts of a word to build up its representation'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 19, 'page_label': '20', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n20\\nHow are embeddings actually created?‚Ä¢Unsupervised training on large corpus of text‚Ä¢Randomly initialized vectors for each word in corpus‚Ä¢Train to maximize similarity (dot product) of target and context word vectors (Word2Vec)‚Ä¢Addglobal statistics about corpus (co-occurrence probabilities) to improve embeddings (GloVe)'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 20, 'page_label': '21', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n21\\nWord2Vec/GloVeEmbeddings Capture:√ºMeaning of words√ºSimilarities/differences between wordsqContextual meaning of words'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 21, 'page_label': '22', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Representational Learning: Text as Numbers\\n22\\n‚ÄúAfter stealing money from the bank vault, the bank robberwas seen fishing on the Mississippi river bank.‚Äù\\nHow are conditionalembeddings actually created?‚Ä¢Unsupervised pre-training on large corpus of text‚Ä¢Runpre-processedtextthroughthepre-trained model to dynamically generate embeddings for each word √†‚Äúfine-tuning‚Äù the embeddings‚Ä¢ELMo/BERT/other conditional embeddings satisfy all ofour requirements!\\nEach use of ‚Äúbank‚Äù has a different embedding'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 22, 'page_label': '23', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n23\\nAttributes of text data:‚Ä¢Word orderencodesmeaning‚Ä¢The most relevant information for understanding a word may be near or far away‚Ä¢Words havedifferentialimportance\\nThe animal didn‚Äôt cross the street because itwas too tired.------versus ------The animal didn‚Äôt cross the street because itwas too wide.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 23, 'page_label': '24', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n24\\nRecurrent Neural Networks'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 24, 'page_label': '25', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n25\\n1stattempt: RNNs and LSTMs Key Features:√ºWord order encodes meaning¬ßThe most relevant information for understanding a word may be near or far awayqWords have differential importance\\nLong Short-Term Memory'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 25, 'page_label': '26', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n26\\nAttention\\nTo predict a word, use only the most relevant parts of the input text'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 26, 'page_label': '27', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n27\\nAttentionSelf-attention: relating different positions of a single sequence to itself to compute attentionlProcesses each word in the input one at a time (query) by looking at all other words in the input sequence (keys) for clues that can help the model learn a better encoding for the query (values)'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 27, 'page_label': '28', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n28\\n2ndAttempt:Transformers‚Ä¢‚ÄùAttentionisallyouNeed‚Äù:SeminalNLPpaperthat presented SOTA results by only using attention mechanisms withoutrecurrence‚Ä¢Basis ofBERTandGPTSOTAmodels‚Ä¢Manytimesfasterandparallelizable‚Ä¢Addressestheissueofdifferentialimportance'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 28, 'page_label': '29', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n29\\nBERT Model‚Ä¢Bidirectional Encoder Representations from Transformers‚Ä¢Uses both left and right context for training (bi-directional)‚Ä¢Language representation model (pre-trained) that can be fine-tuned for a variety of NLP tasks‚Ä¢Basedontransformer architecture'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 29, 'page_label': '30', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n30\\nGPT‚Ä¢Generative Pre-trained Transformer‚Ä¢Uni-directional‚Ä¢Draws from corpus of information to generate best results for query‚Ä¢Based ontransformer architecture'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 30, 'page_label': '31', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='NLP Models\\n31\\nBERT                          vs                          GPTPros‚Ä¢SuitableforawiderangeofNLPtasks‚Ä¢Canbeadaptedtoaspecificdomain/taskandcanlearnnewinformationthroughfine-tuning‚Ä¢Open-source modelCons‚Ä¢Requires more effort to develop a model\\nPros‚Ä¢Suitable for a wide range of tasks‚Ä¢Lower barriertoentrybecause nofine-tuning required‚Ä¢TrainedonmassivecorpusofinformationCons‚Ä¢Cannot be fine-tuned or learn anything new'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 31, 'page_label': '32', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Review: Key Terms\\n32\\n‚Ä¢Embedding: way to numerically represent the meaning of a word, sentence, paragraph, etc. ‚Ä¢Language Model: probabilistic model of words and phrases in a language‚Ä¢Transformers: Architecture based on attention mechanisms‚Ä¢Representation Learning: Based on pattern discovery‚Ä¢Generative AI: Utilizes knowledge to generate data/information'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 32, 'page_label': '33', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n33'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 33, 'page_label': '34', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='GettingStarted\\n34\\n‚Ä¢User-friendly resource to help you get started with NLP‚Ä¢Transformers python package‚Ä¢Models/datasets for variety of different tasks'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 34, 'page_label': '35', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n35\\nGLUE Benchmark includes many tasks to assess general language understanding‚Ä¢LinguisticAcceptability‚Ä¢Paraphrasing‚Ä¢Semantic Similarity‚Ä¢Question-Answering‚Ä¢Sentiment‚Ä¢And more!'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 35, 'page_label': '36', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Getting Started\\n36\\nNLP in 3 easy steps\\nHuggingFaceTutorial: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb\\nLoad pre-trained model and dataTokenize and pre-process dataFine-tune model and save checkpoint'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Practical Tips¬ßNewer NLP approaches generally don‚Äôt require much manual preprocessing (e.g.older methods like stop word removal and  stemming/lemmatization are not usually needed). ¬ßThere are a lot of specialized pre-trained models out there (e.g., BERT pre-trained models available for twitter data, medical data, etc.) that are an easy win in boosting performance ‚Äìresearch and experiment whenever possibleGeneral ML:¬ßAs withanyML‚Äìgarbage in, garbage out! Take the time to ensure sufficient data'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='medical data, etc.) that are an easy win in boosting performance ‚Äìresearch and experiment whenever possibleGeneral ML:¬ßAs withanyML‚Äìgarbage in, garbage out! Take the time to ensure sufficient data quality¬ßThe ‚Äúbest‚Äù new model may not be the best for you ‚Äìkeep in mind the benefits of using a more established model with more support and try these first ¬ßGPUs not strictly necessary if you are only fine-tuning or doing inference, but will definitely speedup tasks'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 36, 'page_label': '37', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='37'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 37, 'page_label': '38', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='Resources‚Ä¢Stanford CS224N NLP with Deep Learning Course: https://youtu.be/rmVRLeJRkl4‚Ä¢Variety of excellent explainers on key concepts/architectures: https://jalammar.github.io/\\n38'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20230331193131Z00'00'\", 'moddate': \"D:20230331193131Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\NLP_101.pdf', 'total_pages': 39, 'page': 38, 'page_label': '39', 'source_file': 'NLP_101.pdf', 'file_type': 'pdf'}, page_content='THANK YOUContact: rcac-help@purdue.edu\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 0, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Introduction to Machine Learning Class Notes\\nHuy Nguyen\\nPhD Student, Human-Computer Interaction Institute\\nCarnegie Mellon University'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Contents\\nPreface 3\\n1 MLE and MAP 4\\n1.1 MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Bayesian learning and MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Nonparametric models: KNN and kernel regression 7\\n2.1 Bayes decision rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2.1 Bayes decision rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2 ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.3 K-nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.4 Local Kernel Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3 Linear Regression 11\\n3.1 Basic linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='3 Linear Regression 11\\n3.1 Basic linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.2 Multivariate and general linear regression . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3 Regularized least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3.2 Connection to MLE and MAP . . . . . . . . . . . . . . . . . . . . . . . . 14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='3.3.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.3.2 Connection to MLE and MAP . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Logistic Regression 16\\n4.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.2 Training logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n5 Naive Bayes ClassiÔ¨Åer 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='4.2 Training logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n5 Naive Bayes ClassiÔ¨Åer 20\\n5.1 Gaussian Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n5.2 Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.3 Text classiÔ¨Åcation: bag of words model . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.4 Generative vs Discriminative Classifer . . . . . . . . . . . . . . . . . . . . . . . . 23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='5.3 Text classiÔ¨Åcation: bag of words model . . . . . . . . . . . . . . . . . . . . . . . 23\\n5.4 Generative vs Discriminative Classifer . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6 Neural Networks and Deep Learning 25\\n6.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n6.2 Training a neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='6.1 DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n6.2 Training a neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6.2.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6.2.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n6.3 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 1, 'page_label': '1', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='6.2.2 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n6.3 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n6.3.1 Convolutional Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n6.3.2 Pooling layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n6.3.3 Fully Connected Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7 Support Vector Machine 33\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n7.2 Primal form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2.1 Linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n7.2.2 Non linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3 Dual representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.2.2 Non linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3 Dual representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3.1 Linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n7.3.2 Transformation of inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n7.3.3 Kernel tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.3.2 Transformation of inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n7.3.3 Kernel tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n7.3.4 Non linearly separable case . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4 Other topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4.1 Why do SVMs work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.4 Other topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4.1 Why do SVMs work? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.4.2 Multi-class classiÔ¨Åcation with SVM . . . . . . . . . . . . . . . . . . . . . 41\\n8 Ensemble Methods and Boosting 42\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n8.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n8.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.2.1 What Œ±t to choose for hypothesis ht? . . . . . . . . . . . . . . . . . . . . 45\\n8.2.2 Show that training error converges to 0 . . . . . . . . . . . . . . . . . . . 46\\n9 Principal Component Analysis 49\\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='9 Principal Component Analysis 49\\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n9.2 PCA algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n9.3 PCA applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n9.3.1 Eigenfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n9.3.2 Image compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='9.3.1 Eigenfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n9.3.2 Image compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n9.4 Shortcomings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n10 Hidden Markov Model 54\\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n10.2 Inference in HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n10.2 Inference in HMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\n10.2.1 What is P (qt =si)? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n10.2.2 What is P (qt =si |o1o2...o t)? . . . . . . . . . . . . . . . . . . . . . . . 57\\n10.2.3 What is arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t)? . . . . . . . . . . . . . . . . . 57\\n11 Reinforcement Learning 59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='10.2.3 What is arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t)? . . . . . . . . . . . . . . . . . 57\\n11 Reinforcement Learning 59\\n11.1 Markov decision process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n11.2 Reinforcement learning - No action . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n11.2.1 Supervised RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n11.2.2 Certainty-Equivalence learning . . . . . . . . . . . . . . . . . . . . . . . . 62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='11.2.1 Supervised RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\n11.2.2 Certainty-Equivalence learning . . . . . . . . . . . . . . . . . . . . . . . . 62\\n11.2.3 Temporal diÔ¨Äerence learning . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n11.3 Reinforcement learning with action - Policy learning . . . . . . . . . . . . . . . . 64\\n12 Generalization and Model Selection 66\\n12.1 True risk vs Empirical risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 2, 'page_label': '2', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='12 Generalization and Model Selection 66\\n12.1 True risk vs Empirical risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n12.2 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 3, 'page_label': '3', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Preface\\nThis is the class notes I took for CMU‚Äôs 10701: Introduction to Machine Learning in Fall 2018.\\nThe goal of this document is to serve as a quick review of key points from each topic covered in\\nthe course. A more comprehensive note collection for beginners is available at UPenn‚Äôs CIS520:\\nMachine Learning.\\nIn this document, each chapter typically covers one machine learning methodology and contains\\nthe followings:\\n‚Ä¢ DeÔ¨Ånition - deÔ¨Ånition of important concepts.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 3, 'page_label': '3', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Machine Learning.\\nIn this document, each chapter typically covers one machine learning methodology and contains\\nthe followings:\\n‚Ä¢ DeÔ¨Ånition - deÔ¨Ånition of important concepts.\\n‚Ä¢ Diving in the Math - mathematical proof for a statement / formula.\\n‚Ä¢ Algorithm - the steps to perform a common routine / subroutine.\\nIntertwined with these components are transitional text (as I Ô¨Ånd them easier to review than\\nbullet points), so the document as a whole ends up looking like a mini textbook. While there'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 3, 'page_label': '3', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Intertwined with these components are transitional text (as I Ô¨Ånd them easier to review than\\nbullet points), so the document as a whole ends up looking like a mini textbook. While there\\nare already plenty of ML textbooks out there, I am still interested in writing up something\\nthat stays closest to the content taught by Professor Ziv Bar-Joseph and Pradeep Ravikumar.\\nI would also like to take this opportunity to thank the two professors for their guidance.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 1\\nMLE and MAP\\n1.1 MLE\\nDeÔ¨Ånition 1: (Likelihood function and MLE)\\nGivenn data points x1,x 2,...,x n we can deÔ¨Åne the likelihood of the data given the model\\nŒ∏ (usually a collection of parameters) as follows.\\nÀÜP (dataset |Œ∏) =\\nn‚àè\\nk=1\\nÀÜP (xk |Œ∏). (1.1)\\nThe maximum likelihood estimate (MLE) of Œ∏ is\\nÀÜŒ∏MLE = arg max\\nŒ∏\\nÀÜP (dataset |Œ∏). (1.2)\\nTo determine the values for the parameters in Œ∏, we maximize the probability of generating the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='ÀÜP (xk |Œ∏). (1.1)\\nThe maximum likelihood estimate (MLE) of Œ∏ is\\nÀÜŒ∏MLE = arg max\\nŒ∏\\nÀÜP (dataset |Œ∏). (1.2)\\nTo determine the values for the parameters in Œ∏, we maximize the probability of generating the\\nobserved samples. For example, let Œ∏ be the model of a coin Ô¨Çip (so Œ∏ = {P (Head) =q}), then\\nthe best assignment (MLE) for Œ∏ in this case is ÀÜŒ∏ =\\n{\\nÀÜq = # heads\\n# samples\\n}\\n.\\nDiving in the Math 1 - MLE for binary variable\\nFor a binary random variable A with P (A = 1) =q, we show that ÀÜq = # 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='{\\nÀÜq = # heads\\n# samples\\n}\\n.\\nDiving in the Math 1 - MLE for binary variable\\nFor a binary random variable A with P (A = 1) =q, we show that ÀÜq = # 1\\n# samples .\\nAssume we observe n samples x1,x 2,...,x n with n1 heads and n2 tails.\\nThen, the likelihood function is\\nP (D |Œ∏) =\\nn‚àè\\ni=1\\nP (xi |Œ∏) =qn1(1 ‚àíq)n2.\\nWe now Ô¨Ånd ÀÜq that maximizes this likelihood function, i.e., ÀÜq = arg max\\nq\\nqn1(1 ‚àíq)n2.\\nTo do so, we set the derivative to 0:\\n0 = ‚àÇ\\n‚àÇqqn1(1 ‚àíq)n2 =n1qn1‚àí1(1 ‚àíq)n2 ‚àíqn1n2(1 ‚àíq)n2‚àí1,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 4, 'page_label': '4', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We now Ô¨Ånd ÀÜq that maximizes this likelihood function, i.e., ÀÜq = arg max\\nq\\nqn1(1 ‚àíq)n2.\\nTo do so, we set the derivative to 0:\\n0 = ‚àÇ\\n‚àÇqqn1(1 ‚àíq)n2 =n1qn1‚àí1(1 ‚àíq)n2 ‚àíqn1n2(1 ‚àíq)n2‚àí1,\\nwhich is equivalent to\\nqn1‚àí1(1 ‚àíq)n2‚àí1(n1(1 ‚àíq) ‚àíqn2) = 0,\\nwhich yields\\nn1(1 ‚àíq) ‚àíqn2 = 0 ‚áî q = n1\\nn1 +n2\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 5, 'page_label': '5', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='When working with products, probabilities of entire datasets often get too small. A possible\\nsolution is to use the log of probabilities, often termed log likelihood 1:\\nlog ÀÜP (dataset |M) = log\\nn‚àè\\nk=1\\nÀÜP (xk |M) =\\nn‚àë\\nk=1\\nlog ÀÜP (xk |M). (1.3)\\nIn this case, the algorithm for MLE is as follows.\\nAlgorithm 1: (Finding the MLE)\\nGivenn data points x1,x 2,...,x n and a model Œ∏ represented by an expression for P (X |Œ∏),\\nperform the following steps:\\n1. Compute the log-likelihood\\nL = log\\nn‚àè\\ni=1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 5, 'page_label': '5', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1: (Finding the MLE)\\nGivenn data points x1,x 2,...,x n and a model Œ∏ represented by an expression for P (X |Œ∏),\\nperform the following steps:\\n1. Compute the log-likelihood\\nL = log\\nn‚àè\\ni=1\\nP (xi |Œ∏) =\\nn‚àë\\ni=1\\nlogP (xi |Œ∏).\\n2. For each parameter Œ≥ in Œ∏, Ô¨Ånd the solution(s) to the equation ‚àÇL\\n‚àÇŒ≥ = 0.\\n3. The solution ÀÜŒ≥ that satisÔ¨Åes ‚àÇ2L\\n‚àÇÀÜŒ≥2 ‚â§ 0 is the MLE of Œ≥.\\n1.2 Bayesian learning and MAP\\nWe Ô¨Årst note the Bayes formula\\nP (A |B) = P (B |A)P (A)\\nP (B) = P (B |A)P (A)‚àë\\nA\\nP (B |A)P (A)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 5, 'page_label': '5', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àÇŒ≥ = 0.\\n3. The solution ÀÜŒ≥ that satisÔ¨Åes ‚àÇ2L\\n‚àÇÀÜŒ≥2 ‚â§ 0 is the MLE of Œ≥.\\n1.2 Bayesian learning and MAP\\nWe Ô¨Årst note the Bayes formula\\nP (A |B) = P (B |A)P (A)\\nP (B) = P (B |A)P (A)‚àë\\nA\\nP (B |A)P (A)\\n. (1.4)\\nIn Bayesian learning, prior information is encoded as a distribution over possible values of\\nparameters P (M). Using (1.4), we get an updated posterior distribution over parameters. To\\nderive the estimate of true parameter, we choose the value that maximizes posterior probability.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 5, 'page_label': '5', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='parameters P (M). Using (1.4), we get an updated posterior distribution over parameters. To\\nderive the estimate of true parameter, we choose the value that maximizes posterior probability.\\nDeÔ¨Ånition 2: (MAP)\\nGiven a dataset and a modelM with priorP (M), the maximum a posteriori (MAP) estimate\\nof M is\\nÀÜŒ∏MAP = arg max\\nŒ∏\\nP (Œ∏ | dataset) = arg max\\nŒ∏\\nP (dataset |Œ∏)P (Œ∏). (1.5)\\n1Note that because log t is monotonous on R, maximizing log t is the same as maximizing t.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 6, 'page_label': '6', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='If we only have very few samples, MLE may not yield accurate results, so it is useful to take into\\naccount prior knowledge. When the number of samples gets large, the eÔ¨Äect of prior knowledge\\nwill diminish.\\nSimilar to MLE, we have the following algorithm for MAP.\\nAlgorithm 2: (Finding the MAP)\\nGivenn data pointsx1,x 2,...,x n, a model Œ∏ represented by an expression forP (X |Œ∏), and\\nthe prior knowledge P (Œ∏), perform the following steps:\\n1. Compute the log-likelihood\\nL = logP (Œ∏) ¬∑\\nn‚àè\\ni=1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 6, 'page_label': '6', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Givenn data pointsx1,x 2,...,x n, a model Œ∏ represented by an expression forP (X |Œ∏), and\\nthe prior knowledge P (Œ∏), perform the following steps:\\n1. Compute the log-likelihood\\nL = logP (Œ∏) ¬∑\\nn‚àè\\ni=1\\nP (xi |Œ∏) = logP (Œ∏) +\\nn‚àë\\ni=1\\nlogP (xi |Œ∏).\\n2. For each parameter Œ≥ in Œ∏, Ô¨Ånd the solution(s) to the equation ‚àÇL\\n‚àÇŒ≥ = 0.\\n3. The solution ÀÜŒ≥ that satisÔ¨Åes ‚àÇ2L\\n‚àÇÀÜŒ≥2 ‚â§ 0 is the MAP of Œ≥.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 2\\nNonparametric models: KNN and\\nkernel regression\\n2.1 Bayes decision rule\\nClassiÔ¨Åcation is the task of predicting a (discrete) output label given the input data. The\\nperformance of any classiÔ¨Åcation algorithm depends on two factors: (1) the parameters are\\ncorrect, and (2) the underlying assumption holds. The most optimal algorithm is called the\\nBayes decision rule.\\nAlgorithm 3: (Bayes decision rule)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='correct, and (2) the underlying assumption holds. The most optimal algorithm is called the\\nBayes decision rule.\\nAlgorithm 3: (Bayes decision rule)\\nIf we know the conditional probability P (x |y) and class prior P (y), use (1.4) to compute\\nP (y =i |x) = P (x |y =i)P (y =i)\\nP (x) ‚àùP (x |y =i)P (y =i) =qi(x) (2.1)\\nand qi(x) to select the appropriate class. Choose class 0 if q0(x) > q1(x) and 1 otherwise.\\nIn general choose the class ÀÜc = arg max\\nc\\n{qc(x)}.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='P (x) ‚àùP (x |y =i)P (y =i) =qi(x) (2.1)\\nand qi(x) to select the appropriate class. Choose class 0 if q0(x) > q1(x) and 1 otherwise.\\nIn general choose the class ÀÜc = arg max\\nc\\n{qc(x)}.\\nBecause our decision is probabilistic, there is still chance for error. The Bayes error rate (risk)\\nof the data distribution is the probability an instance is misclassiÔ¨Åed by the Bayes decision rule.\\nFor binary classiÔ¨Åcation, the risk for sample x is\\nR(x) = min{P (y = 0 |x),P (y = 1 |x)}. (2.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='of the data distribution is the probability an instance is misclassiÔ¨Åed by the Bayes decision rule.\\nFor binary classiÔ¨Åcation, the risk for sample x is\\nR(x) = min{P (y = 0 |x),P (y = 1 |x)}. (2.2)\\nIn other words, if P (y = 0 |x)> P(y = 1 |x), then we would pick the label 0, and the risk is\\nthe probability that the actual label is 1, which is P (y = 1 |x).\\nWe can also compute the expected risk - the risk for the entire range of values of x:\\nE[r(x)] =\\n‚à´\\nx\\nr(x)P (x)dx\\n=\\n‚à´\\nx'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='the probability that the actual label is 1, which is P (y = 1 |x).\\nWe can also compute the expected risk - the risk for the entire range of values of x:\\nE[r(x)] =\\n‚à´\\nx\\nr(x)P (x)dx\\n=\\n‚à´\\nx\\nmin{P (y = 0 |x),P (y = 1 |x)}dx\\n=P (y = 0)\\n‚à´\\nL1\\nP (x |y = 0)dx +P (y = 1)\\n‚à´\\nL0\\nP (x |y = 1)dx,\\nwhere Li is the region over which the decision rule outputs label i.\\nThe risk value we computed assumes that both errors (assigning instances of class 1 to 0 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 7, 'page_label': '7', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚à´\\nL0\\nP (x |y = 1)dx,\\nwhere Li is the region over which the decision rule outputs label i.\\nThe risk value we computed assumes that both errors (assigning instances of class 1 to 0 and\\nvice versa) are equally harmful. In general, we can set the weight penalty Li,j(x) for assigning\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='instances of class i to class j. This gives us the concept of a loss function\\nE[L] =L0,1(x)P (y = 0)\\n‚à´\\nL1\\nP (x |y = 0)dx +L1,0(x)P (y = 1)\\n‚à´\\nL0\\nP (x |y = 1)dx. (2.3)\\n2.2 ClassiÔ¨Åcation\\nThere are roughly three types of classiÔ¨Åers:\\n1. Instance based classiÔ¨Åers : use observation directly (no models). Example: K nearest\\nneighbor.\\n2. Generative: build a generative statistical model. Example: Naive Bayes.\\n3. Discriminative: directly estimate a decision rule/boundary. Example: decision tree.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='neighbor.\\n2. Generative: build a generative statistical model. Example: Naive Bayes.\\n3. Discriminative: directly estimate a decision rule/boundary. Example: decision tree.\\nThe classiÔ¨Åcation task itself contains several steps:\\n1. Feature transformation: e.g, how do we encode a picture?\\n2. Model / classiÔ¨Åer speciÔ¨Åcation : What type of classiÔ¨Åer to use?\\n3. Model / classiÔ¨Åer estimation (with regularization): How do we learn the parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2. Model / classiÔ¨Åer speciÔ¨Åcation : What type of classiÔ¨Åer to use?\\n3. Model / classiÔ¨Åer estimation (with regularization): How do we learn the parameters\\nof our classiÔ¨Åer? Do we have enough examples to learn a good model?\\n4. Feature selection: Do we really need all the features? Can we use a smaller number and\\nstill achieve the same (or better) results?\\nClassiÔ¨Åcation is one of the key components ofsupervised learning, where we provide the algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='still achieve the same (or better) results?\\nClassiÔ¨Åcation is one of the key components ofsupervised learning, where we provide the algorithm\\nwith labels to some of the instances and the goal is to generalize so that a model / method can\\nbe used to determine the labels of the unobserved examples.\\n2.3 K-nearest neighbors\\nA simple yet surprisingly eÔ¨Écient algorithm is K nearest neighbors.\\nAlgorithm 4: (K-nearest neighbors)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='be used to determine the labels of the unobserved examples.\\n2.3 K-nearest neighbors\\nA simple yet surprisingly eÔ¨Écient algorithm is K nearest neighbors.\\nAlgorithm 4: (K-nearest neighbors)\\nGivenn data points, a distance function d and a new point x to classify, select the class of\\nx based on the majority vote in the K closest points.\\nNote that this requires the deÔ¨Ånition of a distance function or similarity measure between sam-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='x based on the majority vote in the K closest points.\\nNote that this requires the deÔ¨Ånition of a distance function or similarity measure between sam-\\nples. We also need to determine K beforehand. Larger K means the resulting classiÔ¨Åer is more\\n‚Äòsmooth‚Äô (but smoothness is primarily dependent on the actual distribution of the data).\\nFrom a probabilistic view, KNN tries to approximate the Bayes decision rule on a subset of data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 8, 'page_label': '8', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Äòsmooth‚Äô (but smoothness is primarily dependent on the actual distribution of the data).\\nFrom a probabilistic view, KNN tries to approximate the Bayes decision rule on a subset of data.\\nWe compute P (x |y),P (y) and P (x) for some small region around our sample, and the size of\\nthat region will be dependent on the distribution of the test sample.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 2 - Computing probabilities for KNN\\nLet z be the new point we want to classify. Let V be the volume of the m dimensional ball\\nR around z containing the K nearest neighbors for z (where m is the number of features).\\nAlso assume that the distribution in R is uniform.\\nConsider the probability P that a data point chosen at random is in R. On one hand,\\nbecause there are K points in R out of a total of N points, P = K\\nN . On the other hand, let'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Consider the probability P that a data point chosen at random is in R. On one hand,\\nbecause there are K points in R out of a total of N points, P = K\\nN . On the other hand, let\\nP (x) =q be the density at a pointx ‚àà R (q is constant because R has uniform distribution).\\nThen P =\\n‚à´\\nx‚ààR\\nP (x)dx =qV . Hence we see that the marginal probability of z is\\nP (z) =q = P\\nV = K\\nNV .\\nSimilarly, the conditional probability of z given a class i is\\nP (z |y =i) = Ki\\nNiV .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Then P =\\n‚à´\\nx‚ààR\\nP (x)dx =qV . Hence we see that the marginal probability of z is\\nP (z) =q = P\\nV = K\\nNV .\\nSimilarly, the conditional probability of z given a class i is\\nP (z |y =i) = Ki\\nNiV .\\nFinally, we compute the prior of class i:\\nP (y =i) = Ni\\nN.\\nUsing Bayes formula:\\nP (y =i |z) = P (z |y =i)P (y =i)\\nP (z) = Ki\\nK.\\nUsing the Bayes decision rule we will choose the class with the highest probability, which\\ncorresponds to the class with the highest Ki - the number of samples in K.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='P (z) = Ki\\nK.\\nUsing the Bayes decision rule we will choose the class with the highest probability, which\\ncorresponds to the class with the highest Ki - the number of samples in K.\\n2.4 Local Kernel Regression\\nKernel regression is similar to KNN but used for regression. In particular, it focuses on a speciÔ¨Åc\\nregion of the input, as opposed to the global space (like linear regression does). For example,\\nwe can output the local average:\\nÀÜf(x) =\\n‚àën\\ni=1yi ¬∑ I (‚à•xi ‚àíx‚à• ‚â§ h)‚àën'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 9, 'page_label': '9', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='region of the input, as opposed to the global space (like linear regression does). For example,\\nwe can output the local average:\\nÀÜf(x) =\\n‚àën\\ni=1yi ¬∑ I (‚à•xi ‚àíx‚à• ‚â§ h)‚àën\\ni=1 I (‚à•xi ‚àíx‚à• ‚â§ h) , (2.4)\\nwhich can also be expressed in a form similar to linear regression:\\nÀÜf(x) =\\nn‚àë\\ni=1\\nwiyi, w i = I (‚à•xi ‚àíx‚à• ‚â§ h)\\nI\\n(‚àën\\nj=1 ‚à•xj ‚àíx‚à• ‚â§ h\\n).\\nNote that the wi‚Äôs here represent a hard boundary: if Xi is close to x then wi = 1, else wi = 0.\\nIn the general case, w can be expressed using kernel functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 10, 'page_label': '10', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 5: (Nadaraya-Watson Kernel Regression)\\nGivenn data points {(xi,yi)}n\\ni=1, we can output the value at a new point x as\\nÀÜf(x) =\\nn‚àë\\ni=1\\nwiyi, w i = K\\n(‚èê‚èêx‚àíxi\\nh\\n‚èê‚èê)\\n‚àën\\nj=1K\\n(‚èê‚èêx‚àíxi\\nh\\n‚èê‚èê), (2.5)\\nwhere K is a kernel function. Some typical kernel functions include:\\n‚Ä¢ Boxcar kernel: K(t) = I (t ‚â§ 1).\\n‚Ä¢ Gaussian kernel: K(t) = 1‚àö\\n2œÄ exp\\n(\\n‚àít2\\n2\\n)\\n.\\nThe distance h in this case the called the kernel bandwidth . The choice of h should depend'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 10, 'page_label': '10', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Boxcar kernel: K(t) = I (t ‚â§ 1).\\n‚Ä¢ Gaussian kernel: K(t) = 1‚àö\\n2œÄ exp\\n(\\n‚àít2\\n2\\n)\\n.\\nThe distance h in this case the called the kernel bandwidth . The choice of h should depend\\non the number of training data (determines variance) and smoothness of function (determines\\nbias).\\n‚Ä¢ Large bandwidth averages more data points so reduces noice (lower variance).\\n‚Ä¢ Small bandwidth Ô¨Åts more accurately (lower bias).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 10, 'page_label': '10', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='bias).\\n‚Ä¢ Large bandwidth averages more data points so reduces noice (lower variance).\\n‚Ä¢ Small bandwidth Ô¨Åts more accurately (lower bias).\\nIn general this is the bias-variance tradeoÔ¨Ä. Bias represents how accurate the result is (lower\\nbias = more accurate). Variance represents how sensitive the algorithm is to changes in the\\ninput (lower variance = less sensitive). Here a large bandwidth ( h = 200) yields low variance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 10, 'page_label': '10', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='bias = more accurate). Variance represents how sensitive the algorithm is to changes in the\\ninput (lower variance = less sensitive). Here a large bandwidth ( h = 200) yields low variance\\nand high bias, while a small bandwidth ( h = 1) yields high variance and low bias. In this case,\\nh = 50 seems like the best middle ground.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 11, 'page_label': '11', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 3\\nLinear Regression\\n3.1 Basic linear regression\\nDeÔ¨Ånition 3: (Linear Regression)\\nGiven an input x we would like to compute an output y as\\ny =wx +œµ,\\nwhere w is a parameter and œµ represents measurement of noise.\\nOur goal is to estimate w from training data of (xi,yi) pairs. One way is to Ô¨Ånd the least square\\nerror (LSE)\\nÀÜwLR = arg min\\nw\\n‚àë\\ni\\n(yi ‚àíwxi)2 (3.1)\\nwhich minimizes squared distance between measurements and predicted lines. LSE has a nice'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 11, 'page_label': '11', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='error (LSE)\\nÀÜwLR = arg min\\nw\\n‚àë\\ni\\n(yi ‚àíwxi)2 (3.1)\\nwhich minimizes squared distance between measurements and predicted lines. LSE has a nice\\nprobabilistic interpretation (as we will see shortly, if œµ ‚àº N (0,œÉ 2) then ÀÜw is MLE of w). and is\\neasy to compute. In particular, the solution to (3.1) is\\nÀÜw =\\n‚àë\\nixiyi‚àë\\nix2\\ni\\n. (3.2)\\nDiving in the Math 3 - Solving linear regression using LSE\\nWe take the derivative w.r.t w and set to 0:\\n0 = ‚àÇ\\n‚àÇw\\n‚àë\\ni\\n(yi ‚àíwxi)2 = ‚àí2\\n‚àë\\ni\\nxi(yi ‚àíwxi),\\nwhich yields\\n‚àë\\ni\\nxiyi ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 11, 'page_label': '11', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àë\\nixiyi‚àë\\nix2\\ni\\n. (3.2)\\nDiving in the Math 3 - Solving linear regression using LSE\\nWe take the derivative w.r.t w and set to 0:\\n0 = ‚àÇ\\n‚àÇw\\n‚àë\\ni\\n(yi ‚àíwxi)2 = ‚àí2\\n‚àë\\ni\\nxi(yi ‚àíwxi),\\nwhich yields\\n‚àë\\ni\\nxiyi =\\n‚àë\\ni\\nwx2\\ni ‚áí w =\\n‚àë\\nixiyi‚àë\\nix2\\ni\\nIf the line does not pass through the origin, simply change the model to\\ny =w0 +w1x +œµ,\\nand following the same process gives\\nw0 =\\n‚àë\\niyi ‚àíw1xi\\nn , w 1 =\\n‚àë\\nixi(yi ‚àíw0)‚àë\\nix2\\ni\\n. (3.3)\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='3.2 Multivariate and general linear regression\\nIf we have several inputs, this becomes a multivariate regression problem:\\ny =w0 +w1x1 +... +wkxk +œµ.\\nHowever, not all functions can be approximated using the input values directly. In some cases we\\nwould like to use polynomial or other terms based on the input data. As long as the coeÔ¨Écients\\nare linear, the equation is still a linear regression problem. For instance,\\ny =w0x1 +w1x2\\n1 +... +wkx2\\nk +œµ.\\nTypical non-linear basis functions include:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='are linear, the equation is still a linear regression problem. For instance,\\ny =w0x1 +w1x2\\n1 +... +wkx2\\nk +œµ.\\nTypical non-linear basis functions include:\\n‚Ä¢ PolynomialœÜj(x) =xj,\\n‚Ä¢ Gaussian œÜj(x) = (x‚àí¬µj)2\\n2œÉ2\\nj\\n,\\n‚Ä¢ Sigmoid œÜj(x) = 1\\n1+exp(‚àísjx).\\nUsing this new notation, we formulate the general linear regression problem:\\ny =\\n‚àë\\nj\\nwjœÜj(x),\\nwhereœÜj(x) can either bexj for multivariate regression or one of the non-linear bases we deÔ¨Åned.\\nNow assume the general case where we where have n data points'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='y =\\n‚àë\\nj\\nwjœÜj(x),\\nwhereœÜj(x) can either bexj for multivariate regression or one of the non-linear bases we deÔ¨Åned.\\nNow assume the general case where we where have n data points\\n(x(1),y (1)), (x(2),y (2)),..., (x(n),y (n)), and each data point has k features (recall that feature j of\\nx(i) is denoted x(i)\\nj ). Again using LSE to Ô¨Ånd the optimal solution, by deÔ¨Åning\\nŒ¶ =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nœÜ0(x(1)) œÜ1(x(1)) ... œÜ k(x(1))\\nœÜ0(x(2)) œÜ1(x(2)) ... œÜ k(x(2))\\n... ... ... ...\\nœÜ0(x(n)) œÜ1(x(n)) ... œÜ k(x(n))\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='j ). Again using LSE to Ô¨Ånd the optimal solution, by deÔ¨Åning\\nŒ¶ =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\nœÜ0(x(1)) œÜ1(x(1)) ... œÜ k(x(1))\\nœÜ0(x(2)) œÜ1(x(2)) ... œÜ k(x(2))\\n... ... ... ...\\nœÜ0(x(n)) œÜ1(x(n)) ... œÜ k(x(n))\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8 =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n‚Äî œÜ(x(1))T ‚Äî\\n‚Äî œÜ(x(2))T ‚Äî\\n...\\n‚Äî œÜ(x(n))T ‚Äî\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8, (3.4)\\nwe then get\\nw = (Œ¶T Œ¶)‚àí1Œ¶Ty. (3.5)\\nDiving in the Math 4 - LSE for general linear regression problem\\nOur goal is to minimize the following loss function:\\nJ(w) =\\n‚àë\\ni\\n(y(i) ‚àí\\n‚àë\\nj\\nwjœÜj(x(i)))2 =\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))2,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 12, 'page_label': '12', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 4 - LSE for general linear regression problem\\nOur goal is to minimize the following loss function:\\nJ(w) =\\n‚àë\\ni\\n(y(i) ‚àí\\n‚àë\\nj\\nwjœÜj(x(i)))2 =\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))2,\\nwhere w and œÜ(x(i)) are vectors of dimension k + 1 and y(i) is a scalar.\\nSetting the derivative w.r.t w to 0:\\n0 = ‚àÇ\\n‚àÇw\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))2 = 2\\n‚àë\\ni\\n(y(i) ‚àíwTœÜ(x(i)))œÜ(x(i))T,\\nwhich yields ‚àë\\ni\\ny(i)œÜ(x(i))T =wT\\n‚àë\\ni\\nœÜ(x(i))œÜ(x(i))T.\\nHence, deÔ¨Åning Œ¶ as in (3.4) would give us\\n(Œ¶T Œ¶)w = Œ¶Ty ‚áí w = (Œ¶T Œ¶)‚àí1Œ¶Ty\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='To sum up, we have the following algorithm for the general linear regression problem.\\nAlgorithm 6: (General linear regression algorithm)\\nInput: Given n input data {(x(i),y (i))}n\\ni=1 where x(i) is 1 √óm and y(i) is scalar, as well as\\nm basis functions {œÜj}m\\nj=1, we Ô¨Ånd\\nÀÜw = arg min\\nw\\nn‚àë\\ni=1\\n(y(i) ‚àíwTœÜ(x(i)))2\\nby the following procedure:\\n1. Compute Œ¶ as in (3.4).\\n2. Output ÀÜw = (Œ¶T Œ¶)‚àí1Œ¶Ty.\\n3.3 Regularized least squares\\n3.3.1 DeÔ¨Ånition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='j=1, we Ô¨Ånd\\nÀÜw = arg min\\nw\\nn‚àë\\ni=1\\n(y(i) ‚àíwTœÜ(x(i)))2\\nby the following procedure:\\n1. Compute Œ¶ as in (3.4).\\n2. Output ÀÜw = (Œ¶T Œ¶)‚àí1Œ¶Ty.\\n3.3 Regularized least squares\\n3.3.1 DeÔ¨Ånition\\nIn the previous chapter we see that a linear regression problem involves solving (Œ¶ T Œ¶)w = Œ¶Ty\\nfor w. If Œ¶T Œ¶ is invertible, we would get w = (Œ¶T Œ¶)‚àí1Œ¶Ty as in (3.5). Now what if Œ¶ T Œ¶ is not\\ninvertible?\\nRecall that full rank matrices are invertible, and that\\nrank(Œ¶T Œ¶) = the number of non-zero eigenvalues of Œ¶ T Œ¶'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='invertible?\\nRecall that full rank matrices are invertible, and that\\nrank(Œ¶T Œ¶) = the number of non-zero eigenvalues of Œ¶ T Œ¶\\n‚â§ min(n,k ) since Œ¶ is n √ók\\nIn other words, Œ¶T Œ¶ is not invertible ifn<k , i.e., there are more features than data point. More\\nspeciÔ¨Åcally, we haven equations andk >n unknowns - this is an undetermined system of linear\\nequations with many feasible solutions. In that case, the solution needs to be further constrained.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='speciÔ¨Åcally, we haven equations andk >n unknowns - this is an undetermined system of linear\\nequations with many feasible solutions. In that case, the solution needs to be further constrained.\\nOne way, for example, is Ridge Regression - using L2 norm as penalty to bias the solution to\\n‚Äúsmall‚Äù values of w (so that small changes in input don‚Äôt translate to large changes in output):\\nÀÜwRidge = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•2\\n2\\n= arg min\\nw\\n(Œ¶w ‚àíy)T (Œ¶w ‚àíy) +Œª ‚à•w‚à•2\\n2, Œª ‚â• 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Äúsmall‚Äù values of w (so that small changes in input don‚Äôt translate to large changes in output):\\nÀÜwRidge = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•2\\n2\\n= arg min\\nw\\n(Œ¶w ‚àíy)T (Œ¶w ‚àíy) +Œª ‚à•w‚à•2\\n2, Œª ‚â• 0\\n= (Œ¶T Œ¶ +ŒªI)‚àí1Œ¶Ty. (3.6)\\nWe could also use Lasso Regression (L1 penalty)\\nÀÜwLasso = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•1, (3.7)\\nwhich biases towards many parameter values being zero - in other words, many inputs become'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 13, 'page_label': '13', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We could also use Lasso Regression (L1 penalty)\\nÀÜwLasso = arg min\\nw\\nn‚àë\\ni=1\\n(yi ‚àíxiw)2 +Œª ‚à•w‚à•1, (3.7)\\nwhich biases towards many parameter values being zero - in other words, many inputs become\\nirrelevant to prediction in high-dimensional settings. There is no closed form solution for (3.7),\\nbut it can be optimized by sub-gradient descent.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 14, 'page_label': '14', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In general, we can phrase the problem as Ô¨Ånding\\nÀÜw = arg min\\nw\\n(Œ¶w ‚àíy)T (Œ¶w ‚àíy) +Œªpen(w), (3.8)\\nwhere pen(w) is a penalty function. Here‚Äôs a visualization for diÔ¨Äerent kinds of penalty functions:\\n3.3.2 Connection to MLE and MAP\\nConsider a linear regression problem\\nY = ÀÜf(X) +œµ =X ÀÜw +œµ,\\nwhere the noise œµ ‚àº N (0,œÉ 2I), which implies Y ‚àº N (X ÀÜw,œÉ 2I). If Œ¶ T Œ¶ is invertible, then w\\ncan be determined exactly by MCLE:\\nÀÜwMCLE = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 14, 'page_label': '14', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='where the noise œµ ‚àº N (0,œÉ 2I), which implies Y ‚àº N (X ÀÜw,œÉ 2I). If Œ¶ T Œ¶ is invertible, then w\\ncan be determined exactly by MCLE:\\nÀÜwMCLE = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 = ÀÜwLR, (3.9)\\nwhere the last equality follows from (3.1). In other words, Least Square Estimate is the same as\\nMCLE under a Gaussian model .\\nIn case Œ¶ T Œ¶ is not invertible, we can encode the Ridge bias by letting w ‚àº N (0,œÑ 2I) and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 14, 'page_label': '14', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='MCLE under a Gaussian model .\\nIn case Œ¶ T Œ¶ is not invertible, we can encode the Ridge bias by letting w ‚àº N (0,œÑ 2I) and\\nP (w) ‚àù exp(‚àíwTw/2œÑ 2), which would yield\\nÀÜwMCAP = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•2\\n2 = ÀÜwRidge, (3.10)\\nwhere Œª is constant in terms of œÉ2 and œÑ 2, and the last equality follows from (3.6). In other'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 14, 'page_label': '14', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•2\\n2 = ÀÜwRidge, (3.10)\\nwhere Œª is constant in terms of œÉ2 and œÑ 2, and the last equality follows from (3.6). In other\\nwords, Prior belief that w is Gaussian with mean 0 biases solution to ‚Äúsmall‚Äù w.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 15, 'page_label': '15', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 5 - Ridge regression and MCAP\\nSince we are given P (w) ‚àù exp(‚àíwTw/2œÑ 2), let P (w) = exp( ‚àíc ‚à•w‚à•2\\n2), where c is some\\nconstant, then ‚àí logP (w) =c ‚à•w‚à•2\\n2, so (3.10) is equivalent to Ô¨Ånding\\ninf\\nw\\n{L(w) +c ‚à•w‚à•2\\n2}\\n= inf\\nw\\n{L(w)} such that ‚à•w‚à•2\\n2 ‚â§L(c),\\nwhere L(c) is a bijective function of c. So adding c ‚à•w‚à•2\\n2 is the same as the ridge regression\\nconstraint ‚à•w‚à•2\\n2 ‚â§L for some constant L.\\nSimilarly, we can encode the Lasso bias by letting wi ‚àº Laplace(0,t ) (iid) and P (wi) ‚àù'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 15, 'page_label': '15', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2 is the same as the ridge regression\\nconstraint ‚à•w‚à•2\\n2 ‚â§L for some constant L.\\nSimilarly, we can encode the Lasso bias by letting wi ‚àº Laplace(0,t ) (iid) and P (wi) ‚àù\\nexp(‚àí|wi|/t), which would yield\\nÀÜwMCAP = arg max\\nw\\nP ({yi}n\\ni=1 |w,œÉ 2, {xi}n\\ni=1)\\ued19 \\ued18\\ued17 \\ued1a\\nConditional log likelihood\\n+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•1 = ÀÜwLasso, (3.11)\\nwhereŒª is constant in terms ofœÉ2 andt, and the last equality follows from (3.7). In other words,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 15, 'page_label': '15', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='+ logP (w)\\ued19 \\ued18\\ued17 \\ued1a\\nlog prior\\n= arg min\\nw\\nn‚àë\\ni=1\\n(xiw ‚àíyi)2 +Œª ‚à•w‚à•1 = ÀÜwLasso, (3.11)\\nwhereŒª is constant in terms ofœÉ2 andt, and the last equality follows from (3.7). In other words,\\nPrior belief that w is Laplace with mean 0 biases solution to ‚Äúsparse‚Äù w.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 16, 'page_label': '16', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 4\\nLogistic Regression\\n4.1 DeÔ¨Ånition\\nWe know that regression is for predicting real-valued output Y , while classiÔ¨Åcation is for pre-\\ndicting (Ô¨Ånite) discrete-valuedY . But is there a way to connect regression to classiÔ¨Åcation? Can\\nwe predict the ‚Äúprobability‚Äù of a class label? The answer is generally yes, but we have to keep\\nin mind the constraint that the probability value should lie in [0 , 1].\\nDeÔ¨Ånition 4: (Logistic Regression)\\nAssume the following functional form for P (Y |X):'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 16, 'page_label': '16', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='in mind the constraint that the probability value should lie in [0 , 1].\\nDeÔ¨Ånition 4: (Logistic Regression)\\nAssume the following functional form for P (Y |X):\\nP (Y = 1 |X) = 1\\n1 + exp(‚àí(w0 + ‚àë\\niwiXi)), (4.1)\\nP (Y = 0 |X) = 1\\n1 + exp(w0 + ‚àë\\niwiXi). (4.2)\\nIn essence, logistic regression means applying the logistic function œÉ(z) = 1\\n1+exp(‚àíz) to a linear\\nfunction of the data. However, note that it is still a linear classiÔ¨Åer.\\nDiving in the Math 6 - Logistic Regression as linear classiÔ¨Åer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 16, 'page_label': '16', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='1+exp(‚àíz) to a linear\\nfunction of the data. However, note that it is still a linear classiÔ¨Åer.\\nDiving in the Math 6 - Logistic Regression as linear classiÔ¨Åer\\nNote that P (Y = 1 |X) can be rewritten as\\nP (Y = 1 |X) = exp(w0 + ‚àë\\niwiXi)\\n1 + exp(w0 + ‚àë\\niwiXi).\\nWe would assign label 1 if P (Y = 1 |X)>P (Y = 0 |X), which is equivalent to\\nexp(w0 +\\n‚àë\\ni\\nwiXi)> 1 ‚áîw0 +\\n‚àë\\ni\\nwiXi > 0.\\nSimilarly, we would assign label 0 if P (Y = 1 |X)<P (Y = 0 |X), which is equivalent to\\nexp(w0 +\\n‚àë\\ni\\nwiXi)< 1 ‚áîw0 +\\n‚àë\\ni'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 16, 'page_label': '16', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='exp(w0 +\\n‚àë\\ni\\nwiXi)> 1 ‚áîw0 +\\n‚àë\\ni\\nwiXi > 0.\\nSimilarly, we would assign label 0 if P (Y = 1 |X)<P (Y = 0 |X), which is equivalent to\\nexp(w0 +\\n‚àë\\ni\\nwiXi)< 1 ‚áîw0 +\\n‚àë\\ni\\nwiXi < 0.\\nIn other words, the decision boundary is the line w0 + ‚àë\\niwiXi, which is linear.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 17, 'page_label': '17', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='4.2 Training logistic regression\\nGiven training data {(xi,yi)}n\\ni=1 where the input hasd features, we want to learn the parameters\\nw0,w 1,...,w d. We can do so by MCLE:\\nÀÜwMCLE = arg max\\nw\\nn‚àè\\ni=1\\nP (y(i) |x(i),w ). (4.3)\\nNote the Discriminative philosophy: don‚Äôt waste eÔ¨Äort learningP (X), focus onP (Y |X) - that‚Äôs\\nall that matters for classiÔ¨Åcation! Using (4.1) and (4.2), we can then compute the log-likelihood:\\nl(w) = ln\\n( n‚àè\\ni=1\\nP (y(i) |x(i),w )\\n)\\n=\\nn‚àë\\ni=1\\n[\\ny(i)(w0 +\\nd‚àë\\nj=1\\nwix(i)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 17, 'page_label': '17', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='all that matters for classiÔ¨Åcation! Using (4.1) and (4.2), we can then compute the log-likelihood:\\nl(w) = ln\\n( n‚àè\\ni=1\\nP (y(i) |x(i),w )\\n)\\n=\\nn‚àë\\ni=1\\n[\\ny(i)(w0 +\\nd‚àë\\nj=1\\nwix(i)\\nj ) ‚àí ln(1 + exp(w0 +\\nd‚àë\\nj=1\\nwix(i)\\nj ))\\n]\\n. (4.4)\\nThere is no closed-form solution to maximize l(w), but we note that it is a concave function.\\nDeÔ¨Ånition 5: (Concave function)\\nA function l(w) is called concave if the line joining two points l(w1),l (w2) on the function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 17, 'page_label': '17', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='DeÔ¨Ånition 5: (Concave function)\\nA function l(w) is called concave if the line joining two points l(w1),l (w2) on the function\\ndoes not lie above the function on the interval [ w1,w 2].\\nEquivalently, a functionl(w) is concave on [w1,w 2] if\\nl(tx1 + (1 ‚àít)x2) ‚â•tl(x1) + (1 ‚àít)l(x2)\\nfor all x1,x 2 ‚àà [w1,w 2] and t ‚àà [0, 1]. If the sign is reversed, l is a convex function.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 7 - Log likelihood of logistic regression is concave\\nFor convenience we denote x(i)\\n0 = 1, so that w0 + ‚àëd\\ni=jwix(i)\\nj =wTx(i).\\nWe Ô¨Årst note the following lemmas:\\n1. If f is convex then ‚àíf is concave and vice versa.\\n2. A linear combination of n convex (concave) functions f1,f 2,...,f n with nonnegative\\ncoeÔ¨Écients is convex (concave).\\n3. Another property of twice diÔ¨Äerentiable convex function is that the second derivative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='coeÔ¨Écients is convex (concave).\\n3. Another property of twice diÔ¨Äerentiable convex function is that the second derivative\\nis nonnegative. Using this property, we can see that f(x) = log(1 + expx) is convex.\\n4. If f and g are both convex, twice diÔ¨Äerentiable and g is non-decreasing, then g ‚ó¶f is\\nconvex.\\nNow we rewrite l(w) as follows:\\nl(w) =\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí log(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí\\nn‚àë\\ni=1\\nlog(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)fi(w) ‚àí\\nn‚àë\\ni=1\\ng(fi(w)),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='convex.\\nNow we rewrite l(w) as follows:\\nl(w) =\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí log(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)wTx(i) ‚àí\\nn‚àë\\ni=1\\nlog(1 + exp(wTx(i)))\\n=\\nn‚àë\\ni=1\\ny(i)fi(w) ‚àí\\nn‚àë\\ni=1\\ng(fi(w)),\\nwhere fi(w) =wTx(i) and g(z) = log(1 + expz).\\nfi(w) is of the form Ax +b where A = x(i) and b = 0, which means it‚Äôs aÔ¨Éne (i.e., both\\nconcave and convex). We also know that g(z) is convex, and it‚Äôs easy to see g is non-\\ndecreasing. This means g(fi(w)) is convex, or equivalently, ‚àíg(fi(w)) is concave.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='concave and convex). We also know that g(z) is convex, and it‚Äôs easy to see g is non-\\ndecreasing. This means g(fi(w)) is convex, or equivalently, ‚àíg(fi(w)) is concave.\\nTo sum up, we can express l(w) as\\nl(w) =\\nn‚àë\\ni=1\\ny(i)fi(w)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nconcave\\n+\\nn‚àë\\ni=1\\n‚àíg(fi(w))\\n\\ued19 \\ued18\\ued17 \\ued1a\\nconcave\\n,\\nhence l(w) is concave.\\nAs such, it can be optimized by the gradient ascent algorthim.\\nAlgorithm 7: (Gradient ascent algorithm)\\nInitialize: Pick w at random.\\nGradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 18, 'page_label': '18', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='As such, it can be optimized by the gradient ascent algorthim.\\nAlgorithm 7: (Gradient ascent algorithm)\\nInitialize: Pick w at random.\\nGradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.\\nUpdate:\\n‚àÜw =Œ∑‚àáwE(w)\\nw(t+1)\\nt ‚Üêw(t)\\ni +Œ∑‚àÇE(w)\\n‚àÇwi\\n,\\nwhere Œ∑ >0 is the learning rate.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 19, 'page_label': '19', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In this case our likelihood function is speciÔ¨Åed in (4.4), so we have the following steps for training\\nlogistic regression:\\nAlgorithm 8: (Gradient ascent algorithm for logistic regression)\\nInitialize: Pick w at random and a learning rate Œ∑.\\nUpdate:\\n‚Ä¢ Set an œµ> 0 and denote\\nÀÜP (y(i) = 1 |x(i),w (t)) =\\nexp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n1 + exp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n.\\n‚Ä¢ Iterate until |w(t+1)\\n0 ‚àíw(t)\\n0 |<œµ :\\nw(t+1)\\n0 ‚Üêw(t)\\n0 +Œ∑\\nn‚àë\\ni=1\\n[\\ny(i) ‚àí ÀÜP (y(i) = 1 |x(i),w (t))\\n]\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 19, 'page_label': '19', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='exp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n1 + exp(w(t)\\n0 + ‚àëd\\nj=1w(t)\\nj x(i)\\nj )\\n.\\n‚Ä¢ Iterate until |w(t+1)\\n0 ‚àíw(t)\\n0 |<œµ :\\nw(t+1)\\n0 ‚Üêw(t)\\n0 +Œ∑\\nn‚àë\\ni=1\\n[\\ny(i) ‚àí ÀÜP (y(i) = 1 |x(i),w (t))\\n]\\n.\\n‚Ä¢ Fork = 1,...,d , iterate until |w(t+1)\\nk ‚àíw(t)\\nk |<œµ :\\nw(t+1)\\nk ‚Üêw(t)\\nk +Œ∑\\nn‚àë\\ni=1\\nx(i)\\nj\\n[\\ny(i) ‚àí ÀÜP (y(i) = 1 |x(i),w (t))\\n]\\n.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 5\\nNaive Bayes ClassiÔ¨Åer\\n5.1 Gaussian Bayes\\nRecall that the Bayes decision rule (2.1) is the optimal classiÔ¨Åer, but requires having knowledge\\nofP (Y |X) =P (X |Y )P (Y ), which is diÔ¨Écult to compute. To tackle this problem, we consider\\nappropriate models for the two terms: class probability P (Y ) and class conditional distribution\\nof features P (X |Y ).\\nConsider an example of X being continuous 1-dimensional and Y being binary. The class'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='of features P (X |Y ).\\nConsider an example of X being continuous 1-dimensional and Y being binary. The class\\nprobability P (Y ) then follows a Bernoulli distribution with parameter Œ∏, i.e., P (Y = 1) = Œ∏.\\nWe further assume the Gaussian class conditional density\\nP (X =x |Y =i) = 1‚àö2œÄœÉ2\\ny\\nexp\\n(\\n‚àí(x ‚àí¬µi)2\\n2œÉ2\\ni\\n)\\n, (5.1)\\nwhere we note that the distribution would be diÔ¨Äerent for each class (hence the notation ¬µi,œÉi).\\nIn total there are 5 parameters: Œ∏,¬µ 0,¬µ 1,œÉ 0,œÉ 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='y\\nexp\\n(\\n‚àí(x ‚àí¬µi)2\\n2œÉ2\\ni\\n)\\n, (5.1)\\nwhere we note that the distribution would be diÔ¨Äerent for each class (hence the notation ¬µi,œÉi).\\nIn total there are 5 parameters: Œ∏,¬µ 0,¬µ 1,œÉ 0,œÉ 1.\\nIn case X is 2-dimensional, we would have\\nP (X =x |Y =i) = 1‚àö\\n2œÄ|Œ£i|\\nexp\\n(\\n‚àí(x ‚àí¬µi)T Œ£‚àí1\\ni (x ‚àí¬µi)\\n2\\n)\\n, (5.2)\\nwhere Œ£i is a 2√ó2 covariance matrix. In total there are 11 parameters: Œ∏,¬µ0,¬µ 1 (2-dimensional),\\nŒ£0, Œ£1 (2 √ó 2 symmetric matrix).\\nFurther note that the decision boundary in this case is\\nP (Y = 1 |X =x)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Œ£0, Œ£1 (2 √ó 2 symmetric matrix).\\nFurther note that the decision boundary in this case is\\nP (Y = 1 |X =x)\\nP (Y = 0 |X =x) = P (X =x |Y = 1)P (Y = 1)\\nP (X =x |Y = 0)P (Y = 0)\\n=\\n‚àö\\n|Œ£0|\\n|Œ£1| exp\\n(\\n‚àí(x ‚àí¬µ1)T Œ£‚àí1\\n1 (x ‚àí¬µ1)\\n2 + (x ‚àí¬µ0)T Œ£‚àí1\\n0 (x ‚àí¬µ0)\\n2\\n)\\n¬∑ Œ∏\\n1 ‚àíŒ∏. (5.3)\\nThis implies a quadratic equation in x, but if Œ£ 1 = Œ£0 the quadratic terms cancel out and the\\nequation is linear.\\nThe number of parameters we would need to learn for Gaussian Bayes in the general case, with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 20, 'page_label': '20', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='equation is linear.\\nThe number of parameters we would need to learn for Gaussian Bayes in the general case, with\\nk labels and d-dimensional inputs, is:\\n‚Ä¢ P (Y =i) =pi for 1 ‚â§i ‚â§k ‚àí 1: k ‚àí 1 parameters, and\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ P (X =x |Y =i) ‚àº N (¬µi, Œ£i): d parameters for ¬µi and d(d+1)\\n2 parameters for Œ£i,\\nwhich result in kd + kd(d+1)\\n2 =O(kd2) parameters.\\nIf X is discrete, again with k labels and d-dimensional inputs, the number of parameters is:\\n‚Ä¢ P (Y =i) =pi for 1 ‚â§i ‚â§k ‚àí 1: k ‚àí 1 parameters, and\\n‚Ä¢ P (X =x |Y =i) comes from a probability table with 2 d ‚àí 1 entries,\\nwhich result is k(2d ‚àí 1) parameters to learn.\\nHaving too many parameters means we need a lot of training data to learn them. We therefore'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='which result is k(2d ‚àí 1) parameters to learn.\\nHaving too many parameters means we need a lot of training data to learn them. We therefore\\nintroduce an assumption that can signiÔ¨Åcantly reduce the number of parameters.\\n5.2 Naive Bayes ClassiÔ¨Åer\\nDeÔ¨Ånition 6: (Naive Bayes ClassiÔ¨Åer)\\nThe Naive Bayes ClassiÔ¨Åer is the Bayes decision rule with an additional ‚Äúnaive‚Äù assumption\\nthat features are independent given the class label :\\nP (X |Y ) =P (X1,X 2,...,X d |Y ) =\\nd‚àè\\ni=1\\nP (Xi |Y ). (5.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='that features are independent given the class label :\\nP (X |Y ) =P (X1,X 2,...,X d |Y ) =\\nd‚àè\\ni=1\\nP (Xi |Y ). (5.4)\\nIn this case, the output class of an input x is\\nÀÜfNB (x) = arg max\\ny\\nP (x1,...,x d |y)P (y) = arg max\\ny\\nd‚àè\\ni=1\\nP (xi |y)P (y).\\nTherefore, if the conditional assumption holds, Naive Bayes is the optimal classiÔ¨Åer. Using this\\nassumption, we can then formulate the Naive Bayes classiÔ¨Åer algorithm, where the class priors\\nand class conditional probabilities are estimated using MLE.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='assumption, we can then formulate the Naive Bayes classiÔ¨Åer algorithm, where the class priors\\nand class conditional probabilities are estimated using MLE.\\nAlgorithm 9: (Naive Bayes ClassiÔ¨Åer for discrete features)\\nGiven training data {(x(i),y (i))}n\\ni=1 where x(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), we compute the follow-\\nings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class joint distribution:\\nÀÜP (Xj =xj,Y =y) = 1\\nn\\nn‚àë\\ni=1\\nI\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 21, 'page_label': '21', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='1 ,x (i)\\n2 ,...,x (i)\\nd ), we compute the follow-\\nings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class joint distribution:\\nÀÜP (Xj =xj,Y =y) = 1\\nn\\nn‚àë\\ni=1\\nI\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n.\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nÀÜP (xj,y )\\nÀÜP (y)\\n= arg max\\ny\\n‚àën\\ni=1 I\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n‚àën\\ni=1 I (y(i) =y) . (5.5)\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 10: (Naive Bayes ClassiÔ¨Åer for continuous features)\\nGiven training data {(xi,yi)}n\\ni=1 wherex(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), we compute the followings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class conditional distribution:\\nÀÜ¬µjy = 1‚àën\\ni=1 I (y(i) =y)\\nn‚àë\\ni=1\\nx(i)\\nj I\\n(\\ny(i) =y\\n)\\n,\\nÀÜœÉ2\\njy = 1‚àën\\ni=1 I (y(i) =y) ‚àí 1\\nn‚àë\\ni=1\\n(x(i)\\nj ‚àí ÀÜ¬µjy)2I\\n(\\ny(i) =y\\n)\\n,\\nÀÜP (Xj =xj |Y =y) = 1\\nÀÜœÉ2\\njy\\n‚àö\\n2œÄ exp\\n(\\n‚àí(x ‚àí ÀÜ¬µjy)2\\n2ÀÜœÉ2\\njy\\n)\\n.\\n‚Ä¢ Prediction for test data given input x:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='j I\\n(\\ny(i) =y\\n)\\n,\\nÀÜœÉ2\\njy = 1‚àën\\ni=1 I (y(i) =y) ‚àí 1\\nn‚àë\\ni=1\\n(x(i)\\nj ‚àí ÀÜ¬µjy)2I\\n(\\ny(i) =y\\n)\\n,\\nÀÜP (Xj =xj |Y =y) = 1\\nÀÜœÉ2\\njy\\n‚àö\\n2œÄ exp\\n(\\n‚àí(x ‚àí ÀÜ¬µjy)2\\n2ÀÜœÉ2\\njy\\n)\\n.\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nP (xj |y). (5.6)\\nAs we noted earlier, the number of parameters in the above algorithms is much fewer.\\nDiving in the Math 8 - Number of parameters in Naive Bayes\\nConsider input variable X with discrete features X1,...,X d each taking one of K values'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 8 - Number of parameters in Naive Bayes\\nConsider input variable X with discrete features X1,...,X d each taking one of K values\\nand output label Y taking one of M values. Further suppose that the label distribution is\\nBernoulli and the feature distribution conditioned on the label is multinomial.\\nWithout naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for label:\\nP (Y =y) =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\npy if y <M ‚àí 1\\n1 ‚àí\\nM‚àí2‚àë\\ni=0\\npi if y =M ‚àí 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Without naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for label:\\nP (Y =y) =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\npy if y <M ‚àí 1\\n1 ‚àí\\nM‚àí2‚àë\\ni=0\\npi if y =M ‚àí 1\\n‚Ä¢ For each label y, the corresponding probability table has Kn ‚àí 1 parameters.\\nSo the total number of parameters is M ‚àí 1 +M(Kn ‚àí 1) =M ¬∑Kn ‚àí 1.\\nWith naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for labels, as mentioned above.\\n‚Ä¢ For each Y = y and 1 ‚â§ i ‚â§ n, Xi | Y comes from a categorial distribution so it has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 22, 'page_label': '22', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='With naive Bayes assumption:\\n‚Ä¢ M ‚àí 1 parameters p0,p 1,...,p M‚àí2 for labels, as mentioned above.\\n‚Ä¢ For each Y = y and 1 ‚â§ i ‚â§ n, Xi | Y comes from a categorial distribution so it has\\nK ‚àí 1 parameters. So we need Mn(K ‚àí 1) parameters for\\n{P (Xi =xi |Y =y) |i = 1..n, xi = 1..K, y = 0..M ‚àí 1}.\\nThe total number of parameters is M ‚àí 1 +Mn(K ‚àí 1). Therefore the number of param-\\neters is reduced from exponential to linear with the Naive Bayes assumption.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We note two important issues in Naive Bayes. First, the conditional independence assumption\\ndoes not always hold; nevertheless, this is still the most used classiÔ¨Åer, especially when data\\nis limited. Second, in practice we typically use MAP estimates for the probabilities instead of\\nMLE since insuÔ¨Écient data may cause MLE to be zero. For instance, if we never see a training\\ninstance where X1 =a and Y =b then, based on MLE, ÀÜP (X1 =a |Y =b) = 0. Consequently,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='MLE since insuÔ¨Écient data may cause MLE to be zero. For instance, if we never see a training\\ninstance where X1 =a and Y =b then, based on MLE, ÀÜP (X1 =a |Y =b) = 0. Consequently,\\nno matter what the values X2,...,X d take, we see that\\nÀÜP (X1 =a,X 2,...,X d |Y ) = ÀÜP (X1 =a |Y )\\nd‚àè\\nj=2\\nÀÜP (Xj |Y ) = 0.\\nTo resolve this issue, we can use a technique called smoothing and add m ‚Äúvirtual‚Äù data points.\\nAlgorithm 11: (Naive Bayes ClassiÔ¨Åer for discrete features with smoothing)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='d‚àè\\nj=2\\nÀÜP (Xj |Y ) = 0.\\nTo resolve this issue, we can use a technique called smoothing and add m ‚Äúvirtual‚Äù data points.\\nAlgorithm 11: (Naive Bayes ClassiÔ¨Åer for discrete features with smoothing)\\nGiven training data {(xi,yi)}n\\ni=1 where x(i) = (x(i)\\n1 ,x (i)\\n2 ,...,x (i)\\nd ), assume some prior dis-\\ntributions (typically uniform) Q(Y = b) and Q(Xj = a,Y = b). We then compute the\\nfollowings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class conditional distribution:\\nÀÜP (Xj =xj |Y =y) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='followings:\\n‚Ä¢ Class prior: ÀÜP (Y =y) = 1\\nn\\n‚àën\\ni=1 I\\n(\\ny(i) =y\\n)\\n.\\n‚Ä¢ Class conditional distribution:\\nÀÜP (Xj =xj |Y =y) =\\n‚àën\\ni=1 I\\n(\\nx(i)\\nj =xj,y (i) =y\\n)\\n+mQ(Xj =xj,Y =y)\\n‚àën\\ni=1 I (y(i) =y) +mQ(Y =y) .\\n‚Ä¢ Prediction for test data given input x:\\nÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nP (xj |y). (5.7)\\n5.3 Text classiÔ¨Åcation: bag of words model\\nWe present a case study of Naive Bayes ClassiÔ¨Åer: text classiÔ¨Åcation. Given the input text as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='ÀÜfNB (x) = arg max\\ny\\nÀÜP (y)\\nd‚àè\\nj=1\\nP (xj |y). (5.7)\\n5.3 Text classiÔ¨Åcation: bag of words model\\nWe present a case study of Naive Bayes ClassiÔ¨Åer: text classiÔ¨Åcation. Given the input text as\\na string of words, we have to predict a category for it (i.e., what is the topic, is this a spam\\nemail). Using the bag of words approach, we construct the input features X as the count of\\nhow many times each word appears in the document. The dimension d ofX is then the number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='email). Using the bag of words approach, we construct the input features X as the count of\\nhow many times each word appears in the document. The dimension d ofX is then the number\\nof distinct words in the document, which can be very large, leading to a huge probability table\\nfor P (X | Y ) (with 2d ‚àí 1 entries). However, under the Naive Bayes assumption, P (X | Y ) is\\nsimply a product of probability of each word raised to its count. It follows that\\nÀÜfNB (x) = arg max\\ny\\nP (y)\\nwk‚àè\\nw=w1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 23, 'page_label': '23', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='simply a product of probability of each word raised to its count. It follows that\\nÀÜfNB (x) = arg max\\ny\\nP (y)\\nwk‚àè\\nw=w1\\nP (w |Y )count(w), (5.8)\\nwherew1,...,w k are all the distinct words in the document. Note that here we assume the order\\nof words in the document doesn‚Äôt matter , which sounds silly but often works very well.\\n5.4 Generative vs Discriminative Classifer\\nWe Ô¨Årst note the following comparison between Naive Bayes and Logistic Regression:\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 24, 'page_label': '24', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In other words, the two methods have representation equivalence - in particular, a linear decision\\nboundary. However, keep in mind that:\\n‚Ä¢ This is only true in the special case where we assume the feature variances are independent\\nof class label (more speciÔ¨Åcally, Œ£ 1 = Œ£0 in (5.3)).\\n‚Ä¢ Logistic regression is a discriminative model and makes no assumption about P (X |Y ) in\\nlearning. Instead, it assumes a sigmoid form for P (Y |X).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 24, 'page_label': '24', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Logistic regression is a discriminative model and makes no assumption about P (X |Y ) in\\nlearning. Instead, it assumes a sigmoid form for P (Y |X).\\n‚Ä¢ The two optimize diÔ¨Äerent functions: MLE / MCLE vs MAP / MCAP and yield diÔ¨Äerent\\nsolutions.\\nMore generally, we outline the problem between a generative classiÔ¨Åer (e.g., Naive Bayes) and a\\ndiscriminative classiÔ¨Åer (e.g., logistic regression).\\nGenerative Discriminatve\\nAssume some prorability model forP (Y ) and\\nP (X |Y ).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 24, 'page_label': '24', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='discriminative classiÔ¨Åer (e.g., logistic regression).\\nGenerative Discriminatve\\nAssume some prorability model forP (Y ) and\\nP (X |Y ).\\nAssume some functional form for P (Y | X)\\nor the decision boundary.\\nEstimate parameters of probability models\\nfrom training data.\\nEstimate parameters of functional form di-\\nrectly from training data.\\nTable 5.1: Generative (model-based) vs Discriminative (model-free) classiÔ¨Åer.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 25, 'page_label': '25', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 6\\nNeural Networks and Deep Learning\\n6.1 DeÔ¨Ånition\\nDeÔ¨Ånition 7: (Neural network)\\nGiven a function f :X ‚ÜíY such that:\\n‚Ä¢ f can be non-linear\\n‚Ä¢ X is vector of continuous / discrete variables\\n‚Ä¢ Y is vector of continuous / discrete variables\\nA neural network is a way to represent f by a network of logistic / sigmoid unit.\\nIn its simplest form, a neural network has one input node and one output node\\nx ow\\nso it is equivalent to logistic regression\\no(x) =œÉ(wx) = 1\\n1 +e‚àíwx. (6.1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 25, 'page_label': '25', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In its simplest form, a neural network has one input node and one output node\\nx ow\\nso it is equivalent to logistic regression\\no(x) =œÉ(wx) = 1\\n1 +e‚àíwx. (6.1)\\nOn the other hand, a neural network with one hidden layer\\nx oh ow0 wh\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='would output\\no(x) =œÉ(w0 +\\n‚àë\\nh\\nwhœÉ(wh\\n0 +\\n‚àë\\ni\\nwh\\nixi)\\n\\ued19 \\ued18\\ued17 \\ued1a\\noh\\n). (6.2)\\nMore generally, prediction in neural networks is done by starting from the input layer and, for\\neach subsequent layer, computing the output of the sigmoid units (forward propagation).\\n6.2 Training a neural network\\n6.2.1 Gradient descent\\nLet‚Äôs treat a neural network as a function Y =fw(X) +œµ where fw is determined given w and\\nœµ ‚àº N (0,œÉ 2I), so Y ‚àº N (fw(X),œÉ 2I). One way to learn the weights is by MCLE:\\nÀÜwMCLE = arg max\\nw\\nln'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Let‚Äôs treat a neural network as a function Y =fw(X) +œµ where fw is determined given w and\\nœµ ‚àº N (0,œÉ 2I), so Y ‚àº N (fw(X),œÉ 2I). One way to learn the weights is by MCLE:\\nÀÜwMCLE = arg max\\nw\\nln\\n(‚àè\\nl\\nP (y(l) |x(l),w )\\n)\\n= arg min\\nw\\n‚àë\\nl\\n(y(l) ‚àí ÀÜfw(x(l)))2.\\nIn other words, the weights are trained to minimize sum of squared errors of predicted network\\noutputs. We may also want to restrict the weights to small values, and this bias can be encoded\\nin MCAP:\\nÀÜwMCAP = arg max\\nw\\nln\\n(\\nP (w)\\n‚àè\\nl'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='outputs. We may also want to restrict the weights to small values, and this bias can be encoded\\nin MCAP:\\nÀÜwMCAP = arg max\\nw\\nln\\n(\\nP (w)\\n‚àè\\nl\\nP (y(l) |x(l),w )\\n)\\n= arg min\\nw\\nc\\n‚àë\\ni\\nw2\\ni +\\n‚àë\\nl\\n(y(l) ‚àí ÀÜfw(x(l)))2,\\nwhere P (w) ‚àù exp(‚àíwTw/2œÑ 2) (recall the connection between Ridge Regression and MCAP\\n(3.10)). In other words, the weights are trained to minimize sum of squared errors of predicted\\nnetwork outputs plus weight magnitudes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(3.10)). In other words, the weights are trained to minimize sum of squared errors of predicted\\nnetwork outputs plus weight magnitudes.\\nTo perform the above optiminization, we introduce a routine called gradient descent.\\nAlgorithm 12: (Gradient descent algorithm)\\nInitialize: Pick w at random.\\nGradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.\\nUpdate:\\n‚àÜw = ‚àíŒ∑‚àáwE(w)\\nw(t+1)\\nt ‚Üêw(t)\\ni ‚àíŒ∑‚àÇE(w)\\n‚àÇwi\\n,\\nwhere Œ∑ >0 is the learning rate.\\nSuppose we feed the training data {(x(l),y (l))}n'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 26, 'page_label': '26', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Gradient:\\n‚àáwE(w) =\\n(‚àÇE(w)\\n‚àÇw0\\n,‚àÇE(w)\\n‚àÇw1\\n,..., ‚àÇE(w)\\n‚àÇwd\\n)\\n.\\nUpdate:\\n‚àÜw = ‚àíŒ∑‚àáwE(w)\\nw(t+1)\\nt ‚Üêw(t)\\ni ‚àíŒ∑‚àÇE(w)\\n‚àÇwi\\n,\\nwhere Œ∑ >0 is the learning rate.\\nSuppose we feed the training data {(x(l),y (l))}n\\nl=1 where x(l) is d-dimensional into a one-layer\\nneural network with d input nodes xi and one output node o. The gradient descent w.r.t every\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 27, 'page_label': '27', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='output weight wi is\\n‚àÇE\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l))o(l)(1 ‚àío(l))x(l)\\ni . (6.3)\\nDiving in the Math 9 - Gradient descent for weights at output layer\\nx1\\nx2\\n...\\nxd\\n‚àë œÉ o\\nw1\\nw2\\nwd\\nnet = ‚àëd\\ni=1wixi o =œÉ(net) = 1\\n1+e‚àínet\\nNote that œÉ(x) = 1\\n1+e‚àíx is the sigmoid function and d\\ndxœÉ(x) =œÉ(x)(1 ‚àíœÉ(x)).\\nWe haveE(w) = 1\\n2\\n‚àën\\nl=1(y(l) ‚àío(l))2 so\\n‚àÇE\\n‚àÇwi\\n= 1\\n2\\nn‚àë\\nl=1\\n‚àÇ\\n‚àÇwi\\n(y(l) ‚àío(l))2 = 1\\n2\\nn‚àë\\nl=1\\n‚àÇ(y(l) ‚àío(l))2\\n‚àÇo(l) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l)) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n.\\nNow note that\\n‚àÇo(l)\\n‚àÇwi\\n= ‚àÇo(l)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 27, 'page_label': '27', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We haveE(w) = 1\\n2\\n‚àën\\nl=1(y(l) ‚àío(l))2 so\\n‚àÇE\\n‚àÇwi\\n= 1\\n2\\nn‚àë\\nl=1\\n‚àÇ\\n‚àÇwi\\n(y(l) ‚àío(l))2 = 1\\n2\\nn‚àë\\nl=1\\n‚àÇ(y(l) ‚àío(l))2\\n‚àÇo(l) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l)) ¬∑ ‚àÇo(l)\\n‚àÇwi\\n.\\nNow note that\\n‚àÇo(l)\\n‚àÇwi\\n= ‚àÇo(l)\\n‚àÇnet(l) ¬∑ ‚àÇnet(l)\\n‚àÇwi\\n=o(l)(1 ‚àío(l))x(l)\\ni ,\\nso\\n‚àÇE\\n‚àÇwi\\n=\\nn‚àë\\nl=1\\n(o(l) ‚àíy(l))o(l)(1 ‚àío(l))x(l)\\ni .\\nFor the hidden layers, consider ‚àÇE\\n‚àÇwhk\\nwherewhk connects a node oh from layerL took from layer\\nL + 1. Further assume that Œì is the set of all nodes at layer L + 2. We then have the expression\\n‚àÇE\\n‚àÇwhk'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 27, 'page_label': '27', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àÇwhk\\nwherewhk connects a node oh from layerL took from layer\\nL + 1. Further assume that Œì is the set of all nodes at layer L + 2. We then have the expression\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)\\n‚àë\\nŒ≥‚ààŒì\\nŒ¥Œ≥wkŒ≥., (6.4)\\nwhereŒ¥Œ≥ =oŒ≥(1 ‚àíoŒ≥)‚àÇE\\n‚àÇoŒ≥\\n. This means that we can Ô¨Årst compute the gradients w.r.t the weights\\nat the output layer, then propagate back to the weights at one previous layer, then two previous\\nlayers, and so on, until the input layer.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 10 - Gradient descent for weights at hidden layers\\nWe would like to evaluate ‚àÇE\\n‚àÇwhk\\n= ‚àÇE\\n‚àÇok\\n¬∑ ‚àÇok\\n‚àÇwhk\\n.\\nSince ‚àÇok\\n‚àÇwhk\\n=ok(1 ‚àíok)oh, we can rewrite the above expression as\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)‚àÇE\\n‚àÇok\\n=ohŒ¥k.\\nNow we consider\\n‚àÇE\\n‚àÇok\\n=\\n‚àë\\nŒ≥\\n‚àÇE\\n‚àÇoŒ≥\\n¬∑ ‚àÇoŒ≥\\n‚àÇok\\n=\\n‚àë\\nŒ≥\\n‚àÇE\\n‚àÇoŒ≥\\noŒ≥(1 ‚àíoŒ≥)wkŒ≥ =\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\nHence\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\n6.2.2 Backpropagation\\nMore generally, we can derive the backpropagation algorithm as in the next page. We also note'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àÇok\\n=\\n‚àë\\nŒ≥\\n‚àÇE\\n‚àÇoŒ≥\\noŒ≥(1 ‚àíoŒ≥)wkŒ≥ =\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\nHence\\n‚àÇE\\n‚àÇwhk\\n=ohok(1 ‚àíok)\\n‚àë\\nŒ≥\\nŒ¥Œ≥wkŒ≥.\\n6.2.2 Backpropagation\\nMore generally, we can derive the backpropagation algorithm as in the next page. We also note\\nthe special properties of this algorithm:\\n‚Ä¢ It computes gradient descent over the entire network weight vector. Training can take\\nthousands of iterations, which is slow, but using network after training is very fast.\\n‚Ä¢ It can easy generalize to arbitrary directed graphs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='thousands of iterations, which is slow, but using network after training is very fast.\\n‚Ä¢ It can easy generalize to arbitrary directed graphs.\\n‚Ä¢ It will Ô¨Ånd a local, not necessarily global error minimum (because error function E is no\\nlonger convex in weights), but often works well in practice.\\n‚Ä¢ It often includes a weight momentum Œ±:\\n‚àÜw(n)\\nij =Œ∑Œ¥jxij +Œ±‚àÜw(n‚àí1)\\nij .\\nThe expressive capabilities of neural network are powerful:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='longer convex in weights), but often works well in practice.\\n‚Ä¢ It often includes a weight momentum Œ±:\\n‚àÜw(n)\\nij =Œ∑Œ¥jxij +Œ±‚àÜw(n‚àí1)\\nij .\\nThe expressive capabilities of neural network are powerful:\\n‚Ä¢ Every boolean function can be represented by a network with one hidden layer, but may\\nrequire exponential (in number of inputs) hidden units.\\n‚Ä¢ Every bounded continuous function can be approximated with arbitrarily small error, by\\na network with one hidden layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 28, 'page_label': '28', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='require exponential (in number of inputs) hidden units.\\n‚Ä¢ Every bounded continuous function can be approximated with arbitrarily small error, by\\na network with one hidden layer.\\n‚Ä¢ Any function can be approximated to arbitrary accuracy by a network with two hidden\\nlayers.\\nHowever, neural network may still encounter the issue of overÔ¨Åtting, which can be avoided by\\nMCAP, early stopping, or by regulating the number of hidden units, which essentially prevents\\noverly complex models.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 13: (Backpropagation)\\nDenote the followings:\\n‚Ä¢ l is the index of the traning example\\n‚Ä¢ yk is target output (label) of output unit k\\n‚Ä¢ oh,ok are unit output (obtained by forward propagation) of output units h,k . If i is\\ninput variable, oi =xi.\\n‚Ä¢ wij is the weight from node i to node j in the next layer.\\nInitialize all weights to small random numbers. Until satisÔ¨Åed, do\\n‚Ä¢ For each training example, do\\n1. Input the training example to the network and compute the network outputs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Initialize all weights to small random numbers. Until satisÔ¨Åed, do\\n‚Ä¢ For each training example, do\\n1. Input the training example to the network and compute the network outputs,\\nusing forward propagation.\\n2. For each output unit k, let\\nŒ¥(l)\\nk ‚Üêo(l)\\nk (1 ‚àío(l)\\nk )(y(l)\\nk ‚àío(l)\\nk ).\\n3. For each hidden unit h, let\\nŒ¥(l)\\nh ‚Üêo(l)\\nh (1 ‚àío(l)\\nh )\\n‚àë\\nk‚ààK\\nwhkŒ¥(l)\\nk ,\\nwhere K is the set of output nodes.\\n4. Update each network weight wij:\\nwij ‚Üêwij + ‚àÜw(l)\\nik\\nwhere ‚àÜw(l)\\nij = ‚àíŒ∑Œ¥(l)\\nj o(l)\\ni .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Œ¥(l)\\nh ‚Üêo(l)\\nh (1 ‚àío(l)\\nh )\\n‚àë\\nk‚ààK\\nwhkŒ¥(l)\\nk ,\\nwhere K is the set of output nodes.\\n4. Update each network weight wij:\\nwij ‚Üêwij + ‚àÜw(l)\\nik\\nwhere ‚àÜw(l)\\nij = ‚àíŒ∑Œ¥(l)\\nj o(l)\\ni .\\nUnlike in logistic regression, the function E(w) in neural network is not convex in w. Thus,\\ngradient descent (and backpropagation) will Ô¨Ånd a local, not necessarily global minimum, but\\nit often works well in practice.\\nFinally, we note the two ways in which backpropagation can be implemented in practice: Batch'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='it often works well in practice.\\nFinally, we note the two ways in which backpropagation can be implemented in practice: Batch\\nmode and Incremental mode (also called Stochastic Gradient Descent ). Incremental mode is\\nfaster to compute and can approximate Batch mode arbitrary closely if Œ∑ is small enough.\\nBatch mode Gradient mode\\nLet ED(w) = 1\\n2\\n‚àë\\nl‚ààD(y(l) ‚àío(l))2. Let El(w) = 1\\n2(y(l) ‚àío(l))2.\\nDo until satisÔ¨Åed:\\n1. Compute the gradient ‚àáED(w).\\n2. w ‚Üêw ‚àíŒ∑‚àáED(w).\\nDo until satisÔ¨Åed:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 29, 'page_label': '29', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Batch mode Gradient mode\\nLet ED(w) = 1\\n2\\n‚àë\\nl‚ààD(y(l) ‚àío(l))2. Let El(w) = 1\\n2(y(l) ‚àío(l))2.\\nDo until satisÔ¨Åed:\\n1. Compute the gradient ‚àáED(w).\\n2. w ‚Üêw ‚àíŒ∑‚àáED(w).\\nDo until satisÔ¨Åed:\\n‚Ä¢ For each training example l in D:\\n1. Compute the gradient ‚àáEl(w).\\n2. w ‚Üêw ‚àíŒ∑‚àáEl(w).\\nTable 6.1: Batch mode vs Gradient mode\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 30, 'page_label': '30', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='6.3 Convolutional neural networks\\nDeÔ¨Ånition 8: (Deep architectures)\\nDeep architectures are composed of multiple levels of non-linear operations, such as neural\\nnets with many hidden layers.\\nDeep learning methods aim at learning feature hierarchies, where features from the higher levels\\nof the hierarchy are formed by lower level features. One such method is convoluational neural\\nnetwork, which, compared to standard feedforward neural network:\\n‚Ä¢ have much fewer connections and parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 30, 'page_label': '30', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='network, which, compared to standard feedforward neural network:\\n‚Ä¢ have much fewer connections and parameters\\n‚Ä¢ is easier to train\\n‚Ä¢ has only slightly worse theoretically best performance.\\nFirst, we deÔ¨Åne the term convolution.\\nDeÔ¨Ånition 9: (Convolution)\\nThe convolution of two functions f and g, denoted f ‚àóg, is deÔ¨Åned as:\\n‚Ä¢ If f and g are continuous:\\n(f ‚àóg)(t) =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(œÑ)g(t ‚àíœÑ)dœÑ =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(t ‚àíœÑ)g(œÑ)dœÑ. (6.5)\\n‚Ä¢ If f and g are discrete:\\n(f ‚àóg)[n] =\\n‚àû‚àë\\nm=‚àí‚àû\\nf[m]g[n ‚àím] =\\n‚àû‚àë\\nm=‚àí‚àû'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 30, 'page_label': '30', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ If f and g are continuous:\\n(f ‚àóg)(t) =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(œÑ)g(t ‚àíœÑ)dœÑ =\\n‚à´ ‚àû\\n‚àí‚àû\\nf(t ‚àíœÑ)g(œÑ)dœÑ. (6.5)\\n‚Ä¢ If f and g are discrete:\\n(f ‚àóg)[n] =\\n‚àû‚àë\\nm=‚àí‚àû\\nf[m]g[n ‚àím] =\\n‚àû‚àë\\nm=‚àí‚àû\\nf[n ‚àím]g[m]. (6.6)\\n‚Ä¢ If discrete g has support on {‚àíM,...,M }:\\n(f ‚àóg)[n] =\\nM‚àë\\nm=‚àíM\\nf[n ‚àím]g[m]. (6.7)\\nInformally, the convolution gives a sense of how much two functions ‚Äúoverlap.‚Äù A simple con-\\nvolutional neural network (CNN) is a sequence of layers, each of which transforms one volume'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 30, 'page_label': '30', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Informally, the convolution gives a sense of how much two functions ‚Äúoverlap.‚Äù A simple con-\\nvolutional neural network (CNN) is a sequence of layers, each of which transforms one volume\\nof activations to another through a diÔ¨Äerentiable function. We use three main types of layers to\\nbuild the network architectures: Convolutional Layer, Pooling Layer, and Fully Connected Layer\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(exactly as seen in regular Neural Networks). Typically, CNNS are used for image classiÔ¨Åcation\\nin computer vision.\\nWe now discuss the components of a CNN. The following text excerpts are from Chapter 10 of\\nArtiÔ¨Åcial Intelligence for Humans, Vol 3: Neural Networks and Deep Learning and Stanford‚Äôs\\nCS 231n.\\n6.3.1 Convolutional Layer\\nThe primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='CS 231n.\\n6.3.1 Convolutional Layer\\nThe primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of\\ncolor, and other visual elements. Each feature is represented by a Ô¨Ålter; the more Ô¨Ålters that we\\ngive to a convolutional layer, the more features it can detect.\\nMore formally, a Ô¨Ålter is a square 2D matrix that scans over the image. The convolutional layer,\\nwhich is essentially a set of Ô¨Ålters, acts as a smaller grid that sweeps left to right over each of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='More formally, a Ô¨Ålter is a square 2D matrix that scans over the image. The convolutional layer,\\nwhich is essentially a set of Ô¨Ålters, acts as a smaller grid that sweeps left to right over each of\\nrow of the image. Each cell in a Ô¨Ålter is a weight.\\nThe sweeping phase (forward propagation) is done as follows. First, the input image may be\\npadded with some layers of zero cells as need. The stride speciÔ¨Åes the number of positions at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='The sweeping phase (forward propagation) is done as follows. First, the input image may be\\npadded with some layers of zero cells as need. The stride speciÔ¨Åes the number of positions at\\nwhich the convolutional Ô¨Ålters will stop. The convolutional Ô¨Ålters move to the right, advancing\\nby the number of cells speciÔ¨Åed in the stride. Once the far right is reached, the convolutional\\nÔ¨Ålter moves back to the far left, then it moves down by the stride amount and continues to the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='by the number of cells speciÔ¨Åed in the stride. Once the far right is reached, the convolutional\\nÔ¨Ålter moves back to the far left, then it moves down by the stride amount and continues to the\\nright again. Hence, the number of steps in one sweeping phase is\\nNumber of steps = W ‚àíF + 2P\\nS + 1,\\nwhere W is the image size, F is the Ô¨Ålter size, P is the padding and S is the stride.\\nWe can use the same set of weights as the convolutional Ô¨Ålter sweeps over the image. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='S + 1,\\nwhere W is the image size, F is the Ô¨Ålter size, P is the padding and S is the stride.\\nWe can use the same set of weights as the convolutional Ô¨Ålter sweeps over the image. This\\nprocess allows convolutional layers to share weights and greatly reduce the amount of processing\\nneeded. In this way, we can recognize the image in shift positions because the same convolutional\\nÔ¨Ålter sweeps across the entire image.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='needed. In this way, we can recognize the image in shift positions because the same convolutional\\nÔ¨Ålter sweeps across the entire image.\\nThe input and output of a convolutional layer are both 3D boxes. For the input to a convolu-\\ntional layer, the width and height of the box is equal to the width and height of the input image.\\nThe depth of the box is equal to the color depth of the image. For an RGB image, the depth'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='tional layer, the width and height of the box is equal to the width and height of the input image.\\nThe depth of the box is equal to the color depth of the image. For an RGB image, the depth\\nis 3, equal to the components of red, green, and blue. If the input to the convolutional layer\\nis another layer, then it will also be a 3D box; however, the dimensions of that 3D box will be\\ndictated by the hyper-parameters of that layer. Like any other layer in the neural network, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='is another layer, then it will also be a 3D box; however, the dimensions of that 3D box will be\\ndictated by the hyper-parameters of that layer. Like any other layer in the neural network, the\\nsize of the 3D box output by a convolutional layer is dictated by the hyper-parameters of the\\nlayer. The width and height of this box are both equal to the Ô¨Ålter size. However, the depth is\\nequal to the number of Ô¨Ålters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 31, 'page_label': '31', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='layer. The width and height of this box are both equal to the Ô¨Ålter size. However, the depth is\\nequal to the number of Ô¨Ålters.\\nA visualization for the sweeping phase is in the Convolution Demo section at Stanford‚Äôs CS\\n231n. In summary, the procedure taking place at the convolutional layer is as follows.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 14: (Convolutional layer‚Äôs forward pass)\\nAccepts a 3D image of size W1 √óH1 √óD1.\\nRequires four hyperpameters: number of Ô¨Ålters K, spatial extent F , stride S, and amount\\nof zero padding P .\\nProduces a 3D image of size W2 √óH2 √óD2 where\\nW2 = W1 ‚àíF + 2P\\nS + 1; H2 = H1 ‚àíF + 2P\\nS + 1; D2 =K. (6.8)\\nWith parameter sharing, it introduces F √óF √óD1 weights per Ô¨Ålter, for a total of F √óF √ó\\nD1 √óK weights and K biases. In the output volume, the d-th depth slice (of size W2 √óH2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='S + 1; D2 =K. (6.8)\\nWith parameter sharing, it introduces F √óF √óD1 weights per Ô¨Ålter, for a total of F √óF √ó\\nD1 √óK weights and K biases. In the output volume, the d-th depth slice (of size W2 √óH2)\\nis the result of performing a valid convolution of the d-th Ô¨Ålter over the input volume with\\na stride of S, and then oÔ¨Äset by d-th bias.\\n6.3.2 Pooling layer\\nPooling layer downsamples an input 3D image to a new one with smaller widths and heights.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='a stride of S, and then oÔ¨Äset by d-th bias.\\n6.3.2 Pooling layer\\nPooling layer downsamples an input 3D image to a new one with smaller widths and heights.\\nTypically a pooling layer follows immediately after a convolutional layer. A typical choice for\\npooling is max-pooling, which, for every f √óf region in the input image, outputs the maximum\\nnumber in that region to the output image.\\nAlgorithm 15: (Pooling layer‚Äôs forward pass)\\nAccepts a volume of size W1 √óH1 √óD1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='number in that region to the output image.\\nAlgorithm 15: (Pooling layer‚Äôs forward pass)\\nAccepts a volume of size W1 √óH1 √óD1.\\nRequires two hyperparameters: spatial extent F and stride S, Produces a volume of size\\nW2 √óH2 √óD2 where:\\nW2 = W1 ‚àíF\\nS + 1; H2 = H1 ‚àíF\\nS + 1; D2 =D1. (6.9)\\nIntroduces zero parameters since it computes a Ô¨Åxed function of the input. For Pooling\\nlayers, it is not common to pad the input using zero-padding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='W2 = W1 ‚àíF\\nS + 1; H2 = H1 ‚àíF\\nS + 1; D2 =D1. (6.9)\\nIntroduces zero parameters since it computes a Ô¨Åxed function of the input. For Pooling\\nlayers, it is not common to pad the input using zero-padding.\\nIt is worth noting that there are only two commonly seen variations of the max pooling layer\\nfound in practice: A pooling layer with F = 3,S = 2 (also called overlapping pooling), and more\\ncommonly F = 2,S = 2.\\n6.3.3 Fully Connected Layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='found in practice: A pooling layer with F = 3,S = 2 (also called overlapping pooling), and more\\ncommonly F = 2,S = 2.\\n6.3.3 Fully Connected Layer\\nThis is the same layer from a feedforward neural network, with two hyperparameters: neuron\\ncount and activation function. Every neuron in the the previous layer‚Äôs 3D image output is\\nconnected to each neuron in this layer through a weight value. A dot product of the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 32, 'page_label': '32', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='count and activation function. Every neuron in the the previous layer‚Äôs 3D image output is\\nconnected to each neuron in this layer through a weight value. A dot product of the input\\n(Ô¨Çattened to 1D) and the weight vector is then passed to the activation function. Dense layers\\ncan employ many diÔ¨Äerent kinds of activation functions, such as ReLU, sigmoid or tanh.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 33, 'page_label': '33', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 7\\nSupport Vector Machine\\n7.1 Introduction\\nRecall that a regression classiÔ¨Åer with linear decision boundary would typically look like\\nwhere the boundary is determined by taking into account all data points (e.g., linear regression\\n(3.1)). In this case, the decision line is closer to the blue nodes since many of them are far\\noÔ¨Ä to the right. However, note that there could be many possible classiÔ¨Åers that have diÔ¨Äerent\\nboundaries but yield the same outcome:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 33, 'page_label': '33', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='oÔ¨Ä to the right. However, note that there could be many possible classiÔ¨Åers that have diÔ¨Äerent\\nboundaries but yield the same outcome:\\nOne way to decide on a single classiÔ¨Åer is to Ô¨Ånd a max margin classiÔ¨Åer: a boundary that leads\\nto the largest margin from both sets of points. This also means that instead of Ô¨Åtting all points,\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 34, 'page_label': '34', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='we may consider only the ‚Äúboundary‚Äù points, and we want to learn a boundary that leads to the\\nlargest margin from points on both sides. These boundary points are called the support vectors.\\nIn particular, we would specify a max margin classiÔ¨Åer based on parameters w and b, and then\\nperform classiÔ¨Åcation as follows\\n7.2 Primal form\\n7.2.1 Linearly separable case\\nOur goal, as previously mentioned, is to Ô¨Ånd the maximum margin. Let‚Äôs deÔ¨Åne the width of\\nthe margin by M, we can then see that\\nM = 2‚àö\\nwTw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 34, 'page_label': '34', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.2 Primal form\\n7.2.1 Linearly separable case\\nOur goal, as previously mentioned, is to Ô¨Ånd the maximum margin. Let‚Äôs deÔ¨Åne the width of\\nthe margin by M, we can then see that\\nM = 2‚àö\\nwTw\\n. (7.1)\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 11 - Computing margin M in terms of weight w and bias b\\nFirst observe that the vector w is orthogonal to the +1 plane. To prove this, let u andv be\\nany two points on the +1 plane then\\nwTu =wTv = 1 ‚àíb ‚áíwT (u ‚àív) = 0.\\nSimilarly,w is orthogonal to the ‚àí1 plane too. Hence, if x+ is a point on the + plane and\\nx‚àí is the point closest to x+ on the ‚àí plane, then the vector from x+ tox‚àí is parallel to w.\\nIn other words, x+ =Œªw +x‚àí for some Œª. Now we have\\nwTx+ +b = 1\\nwT (Œªw +x‚àí) +b = 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='x‚àí is the point closest to x+ on the ‚àí plane, then the vector from x+ tox‚àí is parallel to w.\\nIn other words, x+ =Œªw +x‚àí for some Œª. Now we have\\nwTx+ +b = 1\\nwT (Œªw +x‚àí) +b = 1\\nwTx‚àí +ŒªwTw = 1\\nŒª = 2\\nwTw.\\nHence\\nM = |x+ ‚àíx‚àí| = |Œªw| =Œª\\n‚àö\\nwTw = 2\\nwTw\\n‚àö\\nwTw = 2‚àö\\nwTw\\n.\\nWe can now search for the optimal parameters by Ô¨Ånding a solution that:\\n1. Correctly classiÔ¨Åes all points\\n2. Maximizes the margin (or equivalently minimizes wTw).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àö\\nwTw = 2\\nwTw\\n‚àö\\nwTw = 2‚àö\\nwTw\\n.\\nWe can now search for the optimal parameters by Ô¨Ånding a solution that:\\n1. Correctly classiÔ¨Åes all points\\n2. Maximizes the margin (or equivalently minimizes wTw).\\nSeveral optimization methods can be used: Gradient descent, simulated annealing, EM, etc. In\\nthis case, the problem also belongs to the category of quadratic programming (QP).\\nDeÔ¨Ånition 10: (Quadratic programming)\\nQuadratic programming solves the optimization problem\\nmin\\nu\\nuTRu\\n2 +dTu +c,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='this case, the problem also belongs to the category of quadratic programming (QP).\\nDeÔ¨Ånition 10: (Quadratic programming)\\nQuadratic programming solves the optimization problem\\nmin\\nu\\nuTRu\\n2 +dTu +c,\\nwhere u is a vector, R is square matrix, d is vector and c is scalar.\\nFurthermore,u is subject to n inequality constraint\\nai1u1 +ai2u2 +... ‚â§bi, 1 ‚â§i ‚â§n,\\nand k equivalency constraint\\naj1u1 +aj2u2 +... =bn+j, 1 ‚â§j ‚â§k.\\nMore speciÔ¨Åcally, we can frame the margin maximization problem as a QP problem:\\nmin\\nw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 35, 'page_label': '35', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='ai1u1 +ai2u2 +... ‚â§bi, 1 ‚â§i ‚â§n,\\nand k equivalency constraint\\naj1u1 +aj2u2 +... =bn+j, 1 ‚â§j ‚â§k.\\nMore speciÔ¨Åcally, we can frame the margin maximization problem as a QP problem:\\nmin\\nw\\nwTw\\n2\\nsubject to n constraints if there are n samples x.\\n‚Ä¢ wTx +b ‚â• 1 for all x in class +1,\\n‚Ä¢ wTx ‚àíb ‚â§ ‚àí1 for all x in class ‚àí1,\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.2.2 Non linearly separable case\\nSo far we have assumed that the data is linearly separable, i.e., there is a line wTx +b that\\nperfectly separates the +1 and ‚àí1 class. But this is usually not the case in practice, as there\\ncan be noise and outliers. One way to address this is to penalize the number of misclassiÔ¨Åed\\npoints m, i.e.,\\nmin\\nw\\nwTw +C ¬∑m,\\nwhere C is a regularization constant. However, this is hard to encode in a QP problem.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='points m, i.e.,\\nmin\\nw\\nwTw +C ¬∑m,\\nwhere C is a regularization constant. However, this is hard to encode in a QP problem.\\nInstead of minimizing the number of misclassiÔ¨Åed points we can minimize the distance between\\nthese points and their correct plane. In this case, the new optimization problem is\\nmin\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi,\\nsubject to 2n constraints if there are n samples x(i):\\n‚Ä¢ wTx(i) +b ‚â• 1 ‚àíœµi for all x(i) in class +1\\n‚Ä¢ wTx(i) +b ‚â§ ‚àí1 +œµi for all x(i) in class ‚àí1.\\n‚Ä¢ œµi ‚â• 0 for all i.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='min\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi,\\nsubject to 2n constraints if there are n samples x(i):\\n‚Ä¢ wTx(i) +b ‚â• 1 ‚àíœµi for all x(i) in class +1\\n‚Ä¢ wTx(i) +b ‚â§ ‚àí1 +œµi for all x(i) in class ‚àí1.\\n‚Ä¢ œµi ‚â• 0 for all i.\\nIn summary, we have two optimization problems for two cases:\\nSeparable case Non-separable case\\nFind\\nmin\\nw\\nwTw\\n2\\nsubject to\\n‚Ä¢ wTx +b ‚â• 1 for all x\\nin class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí 1 for all\\nx in class ‚àí1\\nFind\\nmin\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi\\nsubject to\\n‚Ä¢ wTx+b ‚â• 1‚àíœµi for all\\nxi in class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí1 +œµi for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='min\\nw\\nwTw\\n2\\nsubject to\\n‚Ä¢ wTx +b ‚â• 1 for all x\\nin class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí 1 for all\\nx in class ‚àí1\\nFind\\nmin\\nw\\nwTw\\n2 +C\\nn‚àë\\ni=1\\nœµi\\nsubject to\\n‚Ä¢ wTx+b ‚â• 1‚àíœµi for all\\nxi in class +1\\n‚Ä¢ wTx +b ‚â§ ‚àí1 +œµi for\\nall xi in class ‚àí1\\n‚Ä¢ œµi ‚â• 0 for all i.\\nTable 7.1: Optimization constraints for separable and non-separable case in primal SVM\\n7.3 Dual representation\\n7.3.1 Linearly separable case\\nInstead of solving the QPs in Table 7.1 directly, we will solve a dual formulation of the SVM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.3 Dual representation\\n7.3.1 Linearly separable case\\nInstead of solving the QPs in Table 7.1 directly, we will solve a dual formulation of the SVM\\noptimization problem. The main reason for switching to this type of representation is that it\\nwould allow us to use a neat trick that will make our lives easier (and the run time faster).\\nStarting from the separable case, note that we can rephrase the constraints as\\n(wTx(i) +b)y(i) ‚â• 1 (7.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 36, 'page_label': '36', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='would allow us to use a neat trick that will make our lives easier (and the run time faster).\\nStarting from the separable case, note that we can rephrase the constraints as\\n(wTx(i) +b)y(i) ‚â• 1 (7.2)\\nfor alli, whereyi - the class ofxi - is ¬±1. We can then encoding this as part of our minimization\\nproblem using Lagrange multiplier.\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 37, 'page_label': '37', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 16: (Lagrange multiplier method)\\nConsider a problem of Ô¨Ånding\\nmin\\nw\\nf(w)\\nsuch that hi(w) = 0, i = 1,...,l\\nIn the Lagrange multiplier method, we can deÔ¨Åne the Lagrangian to be\\nL(w,Œ≤ ) =f(w) +\\nl‚àë\\ni=1\\nŒ≤ihi(w),\\nwhere the Œ≤i‚Äôs are called the Lagrange multipliers. We would then Ô¨Ånd and set L‚Äôs partial\\nderivatives to zero: ‚àÇL\\n‚àÇwi\\n= 0; ‚àÇL\\n‚àÇŒ≤i\\n= 0,\\nand solve for w and Œ≤.\\nMore generally, consider the following primal optimization problem:\\nmin\\nw\\nf(w)\\nsuch that gi(w) ‚â§ 0, i = 1,...,k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 37, 'page_label': '37', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='derivatives to zero: ‚àÇL\\n‚àÇwi\\n= 0; ‚àÇL\\n‚àÇŒ≤i\\n= 0,\\nand solve for w and Œ≤.\\nMore generally, consider the following primal optimization problem:\\nmin\\nw\\nf(w)\\nsuch that gi(w) ‚â§ 0, i = 1,...,k\\nhi(w) = 0, i = 1,...,l\\nTo solve it, we start by deÔ¨Åning the generalized Lagrangian:\\nL(w,Œ±,Œ≤ ) =f(w) +\\nk‚àë\\ni=1\\nŒ±igi(w) +\\nl‚àë\\ni=1\\nŒ≤ihi(w)\\nwhere the Œ±i,Œ≤i‚Äôs are the Lagrange multipliers.\\nUsing the Lagrange multiplier, consider the quantity\\nŒ∏p(w) = max\\nŒ±,Œ≤:Œ±i‚â•0\\nL(w,Œ±,Œ≤ )\\nthen our minimization problem becomes\\nmin\\nw'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 37, 'page_label': '37', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='l‚àë\\ni=1\\nŒ≤ihi(w)\\nwhere the Œ±i,Œ≤i‚Äôs are the Lagrange multipliers.\\nUsing the Lagrange multiplier, consider the quantity\\nŒ∏p(w) = max\\nŒ±,Œ≤:Œ±i‚â•0\\nL(w,Œ±,Œ≤ )\\nthen our minimization problem becomes\\nmin\\nw\\nŒ∏p(w) = min\\nw\\nmax\\nŒ±,Œ≤:Œ±i‚â•0\\nL(w,Œ±,Œ≤ ).\\nSpeciÔ¨Åcally, our original problem is\\nmin\\nw\\nwTw\\n2\\nsuch that gi(w) = ‚àíy(i)(wTx(i) +b) + 1 ‚â§ 0.\\nwhich can be translated to\\nmin\\nw\\nmax\\nŒ±\\nwTw\\n2 ‚àí\\n‚àë\\ni\\nŒ±i(y(i)(wTx(i) +b) ‚àí 1)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nL(w,Œ±)\\n(7.3)\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='where Œ±i ‚â• 0 for all i. Setting the derivative of L w.r.t w,Œ±,b respectively we get:\\n0 = ‚àÇL\\n‚àÇw =w ‚àí\\nn‚àë\\ni=1\\nŒ±iy(i)x(i) ‚áíw =\\nn‚àë\\ni=1\\nŒ±iy(i)x(i). (7.4)\\n0 = ‚àÇL\\n‚àÇŒ±i\\n= ‚àíy(i)(wTx(i) +b) ‚àí 1 ‚áíb =y(i) ‚àíwTx(i) for i where Œ±i > 0. (7.5)\\n0 = ‚àÇL\\n‚àÇb =\\nn‚àë\\ni=1\\nŒ±iy(i). (7.6)\\nWe mentioned earlier that the only data points of importance are the support vectors, which\\naÔ¨Äect the margin. Originally, we need to Ô¨Ånd the points that touch the boundary of the margin'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Œ±iy(i). (7.6)\\nWe mentioned earlier that the only data points of importance are the support vectors, which\\naÔ¨Äect the margin. Originally, we need to Ô¨Ånd the points that touch the boundary of the margin\\n(i.e., wTx +b = ¬±1). In this case, howerver, (7.4) gives us the support vectors directly, which\\nare the points (x(i),y (i)) where Œ±i > 0.\\nSubstituting the above results back into (7.3), we get the equivalent problem of\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j) (7.7)\\nwhere ‚àë'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='are the points (x(i),y (i)) where Œ±i > 0.\\nSubstituting the above results back into (7.3), we get the equivalent problem of\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j) (7.7)\\nwhere ‚àë\\niŒ±iy(i) = 0 and Œ±i ‚â• 0 for all i. After solving for Œ± in this new problem, to evaluate a\\nnew sample x, we simply compute\\nÀÜy = sign(wTx +b) = sign\\n(‚àë\\ni\\nŒ±iyi(x(i))Tx +b\\n)\\n. (7.8)\\nNote that both the optimization function (7.7) and decision function (7.8) rely on the sum of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='new sample x, we simply compute\\nÀÜy = sign(wTx +b) = sign\\n(‚àë\\ni\\nŒ±iyi(x(i))Tx +b\\n)\\n. (7.8)\\nNote that both the optimization function (7.7) and decision function (7.8) rely on the sum of\\ndot products (x(j))Tx(i), which can be expensive to compute.\\n7.3.2 Transformation of inputs\\nWhen the data is not linearly separable, the original input space ( x) can be mapped to some\\nhigher-dimensional feature space (œÜ(x)) where the training set is separable.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 38, 'page_label': '38', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='When the data is not linearly separable, the original input space ( x) can be mapped to some\\nhigher-dimensional feature space (œÜ(x)) where the training set is separable.\\nFor instance, we can map (x) ‚Üí (x,x 2) (from 1D to 2D) and ( x1,x 2) ‚Üí (x2\\n1,x 2\\n2,\\n‚àö\\n2x1x2) (from\\n2D to 3D):\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 39, 'page_label': '39', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In general, if the data is mapped into suÔ¨Éciently high dimension, then samples will be linearly\\nseparable. n data points can be separable in a space of n ‚àí 1 dimensions or more. However, this\\ntransformation poses two diÔ¨Éculties:\\n‚Ä¢ High computation burden due to high-dimensionality.\\n‚Ä¢ Many more parameters.\\nSVM solves these two issues by:\\n‚Ä¢ Using dual formulation, which only assigns parameters to samples, not features (i.e., each\\nx(i) has an associated Œ±i).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 39, 'page_label': '39', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Many more parameters.\\nSVM solves these two issues by:\\n‚Ä¢ Using dual formulation, which only assigns parameters to samples, not features (i.e., each\\nx(i) has an associated Œ±i).\\n‚Ä¢ Using kernel tricks for eÔ¨Écient computation.\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Note that if we have n data points with m dimensions then the number of parameters is m in\\nprimal form (i.e., the features of weight w) and n in dual form (i.e., an Œ±i for eachx(i)). At Ô¨Årst\\nglance, because n ‚â´ m, the primal formation is at an advantage. However, note that in dual\\nform we only care about the support vectors; in other words, the parameters are only those Œ±i\\nthat are positive, and their number is usually a lot less thann. Hence, the dual form is not worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='form we only care about the support vectors; in other words, the parameters are only those Œ±i\\nthat are positive, and their number is usually a lot less thann. Hence, the dual form is not worse\\nthan primal form in the original space. In the transformed space, as x increases in dimension,\\nso does w, so the primal form requires more parameters, while the dual form generally also sees\\nan increase in the number of support vectors, but not as much.\\n7.3.3 Kernel tricks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='so does w, so the primal form requires more parameters, while the dual form generally also sees\\nan increase in the number of support vectors, but not as much.\\n7.3.3 Kernel tricks\\nWhile working in higher dimensions is beneÔ¨Åcial, it also increases our run time because of the\\ndot product computation. However, there is a neat trick we can use.\\nConsider, for example, all quadratic terms for the features x1,x 2,...,x m:\\nœÜ(x) = (1,\\n‚àö\\n2x1,...,\\n‚àö\\n2xm\\ued19 \\ued18\\ued17 \\ued1a\\nm+1 linear terms\\n, x 2\\n1,...,x 2\\nm\\ued19 \\ued18\\ued17 \\ued1a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Consider, for example, all quadratic terms for the features x1,x 2,...,x m:\\nœÜ(x) = (1,\\n‚àö\\n2x1,...,\\n‚àö\\n2xm\\ued19 \\ued18\\ued17 \\ued1a\\nm+1 linear terms\\n, x 2\\n1,...,x 2\\nm\\ued19 \\ued18\\ued17 \\ued1a\\nm quadratic terms\\n,\\n‚àö\\n2x1x2,...,\\n‚àö\\n2xm‚àí1xm\\ued19 \\ued18\\ued17 \\ued1a\\nm(m‚àí1)/2 pairwise terms\\n)T. (7.9)\\nThe dot product operation would normally be\\nœÜ(x)TœÜ(z) =\\n‚àë\\ni\\n2xizi +\\n‚àë\\ni\\n(xi)2(zi)2 +\\n‚àë\\ni<j\\n2xixjzizj + 1,\\nwhich has O(m2) operations. However, we can obtain dramatic savings by noting that\\n(x ¬∑z + 1)2 = (x ¬∑z)2 + 2(x ¬∑z) + 1\\n= (\\n‚àë\\ni\\nxizi)2 +\\n‚àë\\ni\\n2xizi + 1\\n=\\n‚àë\\ni'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àë\\ni\\n(xi)2(zi)2 +\\n‚àë\\ni<j\\n2xixjzizj + 1,\\nwhich has O(m2) operations. However, we can obtain dramatic savings by noting that\\n(x ¬∑z + 1)2 = (x ¬∑z)2 + 2(x ¬∑z) + 1\\n= (\\n‚àë\\ni\\nxizi)2 +\\n‚àë\\ni\\n2xizi + 1\\n=\\n‚àë\\ni\\n2xizi +\\n‚àë\\ni\\n(xi)2(zi)2 +\\n‚àë\\ni<j\\n2xixjzizj + 1\\n=œÜ(x)TœÜ(z).\\nIn other words, to compute œÜ(x)TœÜ(z), we can simply compute x ¬∑z + 1 (which only needs m\\noperations) and then square it. Hence, we don‚Äôt need to work directly with the transformations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='=œÜ(x)TœÜ(z).\\nIn other words, to compute œÜ(x)TœÜ(z), we can simply compute x ¬∑z + 1 (which only needs m\\noperations) and then square it. Hence, we don‚Äôt need to work directly with the transformations\\nœÜ(x) to compute their dot products. The function œÜ in this case (7.9) is called a polynomial\\nkernel (of degree 2).\\nThe kernel trick works for higher order polynomials as well. In general, a polynomial of degree\\nd can be computed by ( x ¬∑z + 1)d. Beyond polynomials there are other very high dimensional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 40, 'page_label': '40', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='The kernel trick works for higher order polynomials as well. In general, a polynomial of degree\\nd can be computed by ( x ¬∑z + 1)d. Beyond polynomials there are other very high dimensional\\nbasis functions that can be made practical by Ô¨Ånding the right kernel function, such as Radial\\nbasis kernel function K(x,z ) = exp\\n(\\n‚àí (x‚àíz)2\\n2œÉ2\\n)\\n.\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='7.3.4 Non linearly separable case\\nUsing the Lagrange multiplier method on the optimization function for the primal form‚Äôs non\\nlinearly separable case (Table 7.1), we obtain our dual target function\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±jy(i)y(j)(x(i))T (x(j)) (7.10)\\nsubject to ‚àë\\niŒ±iy(i) = 0 andC >Œ±i ‚â• 0 (so now theŒ±i‚Äôs are bounded above by the regularization\\nconstant). To evaluate a new sample x, we similarly perform the computation as in (7.8).\\nIn summary, we have two optimization problems for two cases:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='constant). To evaluate a new sample x, we similarly perform the computation as in (7.8).\\nIn summary, we have two optimization problems for two cases:\\nSeparable case Non-separable case\\nFind\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j)\\nwhere ‚àë\\niŒ±iy(i) = 0 and Œ±i ‚â• 0 for all i.\\nFind\\nmax\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j)\\nwhere ‚àë\\niŒ±iy(i) = 0 and C >Œ±i ‚â• 0 for all i.\\nTable 7.2: Optimization constraints for separable and non-separable case in dual SVM.\\n7.4 Other topics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='max\\nŒ±\\n‚àë\\ni\\nŒ±i ‚àí 1\\n2\\n‚àë\\ni,j\\nŒ±iŒ±iy(i)y(j)(x(i))Tx(j)\\nwhere ‚àë\\niŒ±iy(i) = 0 and C >Œ±i ‚â• 0 for all i.\\nTable 7.2: Optimization constraints for separable and non-separable case in dual SVM.\\n7.4 Other topics\\n7.4.1 Why do SVMs work?\\nIf we are using huge features spaces (with kernels) why are we not overÔ¨Åtting the data?\\n‚Ä¢ Number of parameters remains the same (and most are set to 0).\\n‚Ä¢ While we have a lot of input values, at the end we only care about the support vectors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Number of parameters remains the same (and most are set to 0).\\n‚Ä¢ While we have a lot of input values, at the end we only care about the support vectors\\nand these are usually a small group of samples.\\n‚Ä¢ The minimization (or the maximizing of the margin) function acts as a sort of regularization\\nterm leading to reduced overÔ¨Åtting.\\n7.4.2 Multi-class classiÔ¨Åcation with SVM\\nIf we have data from more than two classes, most common solution is the one-versus-all approach:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 41, 'page_label': '41', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='term leading to reduced overÔ¨Åtting.\\n7.4.2 Multi-class classiÔ¨Åcation with SVM\\nIf we have data from more than two classes, most common solution is the one-versus-all approach:\\n‚Ä¢ Create a classifer for each class against all other data.\\n‚Ä¢ For a new point use all classiÔ¨Åers and compare the margin for all selected classes. The\\nclass with the largest margin is selected.\\nNote that this is not necessarily valid since this is not what we trained the SVM for, but often\\nworks well in practice.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 8\\nEnsemble Methods and Boosting\\n8.1 Introduction\\nConsider the simple (weak) learners (e.g., naive Bayes, logistic regression, decision tree) - those\\nthat don‚Äôt learn too well but still better than chance, i.e. error < 50% but not close to 0. They\\nare good (low variance, usually don‚Äôt overÔ¨Åt) but also bad (high bias, can‚Äôt solve hard problems).\\nCan we somehow improve them by combining them together? A simple approach is ‚Äúbucket of\\nmodels‚Äù:\\n‚Ä¢ Input:\\n‚Äì Your topT favorite learners L1,...,L T'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Can we somehow improve them by combining them together? A simple approach is ‚Äúbucket of\\nmodels‚Äù:\\n‚Ä¢ Input:\\n‚Äì Your topT favorite learners L1,...,L T\\n‚Äì A dataset D\\n‚Ä¢ Learning algorithm:\\n1. Use 10-fold cross validation to estimate the error of L1,...,L T\\n2. Pick the best (lowest 10-CV error) learner L‚àó\\n3. Train L‚àó on D and return its hypothesis h‚àó\\nThis approach is simple and will give results not much worse than the best of the ‚Äúbase learners‚Äù,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2. Pick the best (lowest 10-CV error) learner L‚àó\\n3. Train L‚àó on D and return its hypothesis h‚àó\\nThis approach is simple and will give results not much worse than the best of the ‚Äúbase learners‚Äù,\\nbut what if there‚Äôs not a single best learner? How do we come up with a method that combines\\nmultiple classiÔ¨Åers? One way is to perform voting (ensemble methods):\\n‚Ä¢ Instead of learning a single (weak) classiÔ¨Åer, learn many weak classiÔ¨Åers that are good at\\ndiÔ¨Äerent parts of the input space.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='multiple classiÔ¨Åers? One way is to perform voting (ensemble methods):\\n‚Ä¢ Instead of learning a single (weak) classiÔ¨Åer, learn many weak classiÔ¨Åers that are good at\\ndiÔ¨Äerent parts of the input space.\\n‚Ä¢ Output class: (Weighted) vote of each classiÔ¨Åer\\n‚Äì ClassiÔ¨Åers that are most ‚Äúsure‚Äù will vote with more conviction\\n‚Äì Each classiÔ¨Åer will be most ‚Äúsure‚Äù about a particular part of the space\\n‚Äì On average, do better than single classiÔ¨Åer!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Äì ClassiÔ¨Åers that are most ‚Äúsure‚Äù will vote with more conviction\\n‚Äì Each classiÔ¨Åer will be most ‚Äúsure‚Äù about a particular part of the space\\n‚Äì On average, do better than single classiÔ¨Åer!\\nThe question, then, is how we can force classiÔ¨Åers to learn about diÔ¨Äerent parts of the input\\nspace and weigh the votes of diÔ¨Äerent classiÔ¨Åers? This leads us to the idea of boosting:\\n‚Ä¢ Idea: given a weak learner, run it multiple times on ( reweighted) training data, then let\\nthe learned classiÔ¨Åers vote.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 42, 'page_label': '42', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Idea: given a weak learner, run it multiple times on ( reweighted) training data, then let\\nthe learned classiÔ¨Åers vote.\\n‚Ä¢ On each iteration t:\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 43, 'page_label': '43', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Äì weigh each training example by how incorrectly it was classiÔ¨Åed\\n‚Äì learn a hypothesis ht and its strength Œ±t\\n‚Ä¢ Final classiÔ¨Åer: a linear combination of the votes of the diÔ¨Äerent classiÔ¨Åers weighted by\\ntheir strength\\nNote the notion of a weighted dataset - in particular, if D(i) be the weight of the i-th training\\nexample (x(i),y (i)), then it counts as D(i) examples. From now, in all calculations, whenever\\nused, the i-th training example counts as D(i) ‚Äúexamples‚Äù.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 43, 'page_label': '43', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='example (x(i),y (i)), then it counts as D(i) examples. From now, in all calculations, whenever\\nused, the i-th training example counts as D(i) ‚Äúexamples‚Äù.\\nWith this in mind, we can now deÔ¨Åne the full boosting algorithm (Shapire, 1998):\\nAlgorithm 17: (AdaBoost)\\nGiven dataset {(x(1),y (1)),..., (x(m),y (m))} where x(i) ‚ààX,y (i) ‚ààY = {¬±1}.\\nInitialize D1(i) = 1\\nm.\\nFort = 1,...,T :\\n‚Ä¢ Train weak learner using distribution Dt.\\n‚Ä¢ Get weak classiÔ¨Åer ht :X ‚Üí R.\\n‚Ä¢ Compute the error œµt = ‚àëm\\ni=1Dt(t)I\\n('),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 43, 'page_label': '43', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Initialize D1(i) = 1\\nm.\\nFort = 1,...,T :\\n‚Ä¢ Train weak learner using distribution Dt.\\n‚Ä¢ Get weak classiÔ¨Åer ht :X ‚Üí R.\\n‚Ä¢ Compute the error œµt = ‚àëm\\ni=1Dt(t)I\\n(\\nht(x(i)) Ã∏=y(i))\\nand strength Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n.\\n‚Ä¢ Update\\nDt+1(i) = Dt(i) exp(‚àíŒ±ty(i)ht(x(i))\\nZt\\nwhere\\nZt =\\nm‚àë\\ni=1\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\nis a normalization factor (chosen so that ‚àëm\\ni=1Dt+1(i) = 1).\\nOutput the Ô¨Ånal classiÔ¨Åer H(x) = sign\\n( T‚àë\\nt=1\\nŒ±tht(x)\\n)\\n.\\nAn example is shown below.\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 44, 'page_label': '44', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 45, 'page_label': '45', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='8.2 Mathematical details\\nWe now examine at each of the above formulas to see how they were derived. First observe that:\\n‚Ä¢ œµt is the fraction of misclassiÔ¨Åed (weighted) samples in iteration t. Recall our earlier\\ndeÔ¨Ånition of a weak learner as one that has error < 50%. Hence we always have œµt < 0.5\\nand therefore Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n> 0.\\n‚Ä¢ If sample i is classiÔ¨Åed correctly then y(i)ht(x(i)) = 1 so\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) = Dt(i)\\nexp(Œ±i) <D t(i),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 45, 'page_label': '45', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='and therefore Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n> 0.\\n‚Ä¢ If sample i is classiÔ¨Åed correctly then y(i)ht(x(i)) = 1 so\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) = Dt(i)\\nexp(Œ±i) <D t(i),\\ni.e., the weight of this sample decreases. On the other hand, if sample i is classiÔ¨Åed\\nincorrectly, then its weight increases.\\nIn other words, in each iteration the classiÔ¨Åer will focus on a diÔ¨Äerent set of points (more\\nspeciÔ¨Åcally, those that were misclassiÔ¨Åed in the previous iteration), and in the end, we hope that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 45, 'page_label': '45', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In other words, in each iteration the classiÔ¨Åer will focus on a diÔ¨Äerent set of points (more\\nspeciÔ¨Åcally, those that were misclassiÔ¨Åed in the previous iteration), and in the end, we hope that\\nthe combination of these classiÔ¨Åers (over all iterations) can classify all points correctly. This is\\nthe idea behind boosting. Now, to prove that it does work (i.e., the error converges to 0 after\\nsome number of iterations), we answer the following questions.\\n8.2.1 What Œ±t to choose for hypothesis ht?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 45, 'page_label': '45', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='some number of iterations), we answer the following questions.\\n8.2.1 What Œ±t to choose for hypothesis ht?\\nNote that the training error of the Ô¨Ånal classiÔ¨Åer H is bounded by\\n1\\nm\\nm‚àë\\ni=1\\nI\\n(\\nH(x(i)) Ã∏=y(i))\\n‚â§ 1\\nm\\nm‚àë\\ni=1\\nexp(‚àíy(i)f(x(i))) =\\nT‚àè\\nt=1\\nZt (8.1)\\nwhere f(x) = ‚àëT\\nt=1Œ±tht(x) and H(x) = sign (f(x)).\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 12 - Upper bound of Ô¨Ånal classiÔ¨Åer‚Äôs training error\\nTo prove the Ô¨Årst inequality, note that:\\n‚Ä¢ Each correctly classiÔ¨Åed sample i contributes 0 to the LHS and 1\\nme to the RHS.\\n‚Ä¢ Each incorrectly classiÔ¨Åed sample i contributes 1\\nm to the LHS and e\\nm to the RHS.\\nIn other words, I\\n(\\nH(x(i)) Ã∏=y(i))\\n< exp(‚àíy(i)f(x(i))) for all i, so ‚àë\\ni I\\n(\\nH(x(i)) Ã∏=y(i))\\n<‚àë\\ni exp(‚àíy(i)f(x(i))).\\nTo prove the second inequality, note that the deÔ¨Ånition of Dt gives us'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In other words, I\\n(\\nH(x(i)) Ã∏=y(i))\\n< exp(‚àíy(i)f(x(i))) for all i, so ‚àë\\ni I\\n(\\nH(x(i)) Ã∏=y(i))\\n<‚àë\\ni exp(‚àíy(i)f(x(i))).\\nTo prove the second inequality, note that the deÔ¨Ånition of Dt gives us\\nDT +1(i) =DT (i) ¬∑ exp(‚àíŒ±Ty(i)hT (x(i)))\\nZT\\n=DT‚àí1(i) ¬∑ exp(‚àíŒ±T‚àí1y(i)hT‚àí1(x(i)))\\nZT‚àí1\\n¬∑ exp(‚àíŒ±Ty(i)hT (x(i)))\\nZT\\n=...\\n= exp(‚àí ‚àë\\ntŒ±ty(i)ht(x(i)))\\nm ‚àè\\ntZt\\n= exp(‚àíy(i)f(x(i)))\\nm ‚àè\\ntZt\\n.\\nOn the other hand, because we deÔ¨Åne the Zt‚Äôs as normalization factors,\\n1 =\\nm‚àë\\ni=1\\nDT +1(i) = 1\\nm ‚àè\\ntZt\\nm‚àë\\ni=1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='ZT\\n=...\\n= exp(‚àí ‚àë\\ntŒ±ty(i)ht(x(i)))\\nm ‚àè\\ntZt\\n= exp(‚àíy(i)f(x(i)))\\nm ‚àè\\ntZt\\n.\\nOn the other hand, because we deÔ¨Åne the Zt‚Äôs as normalization factors,\\n1 =\\nm‚àë\\ni=1\\nDT +1(i) = 1\\nm ‚àè\\ntZt\\nm‚àë\\ni=1\\nexp(‚àíy(i)ht(x(i))),\\nwhich leads to\\n1\\nm\\nm‚àë\\ni=1\\nexp(‚àíy(i)f(x(i))) =\\nT‚àè\\nt=1\\nZt.\\nIn other words, to guarantee low error, we just need to make sure its upper bound ‚àè\\ntZt is small.\\nWe can tighten this bound greedily by choosing Œ±t on each iteration to minimize Zt.\\nTo do so, let‚Äôs deÔ¨Åne the error at iteration t as œµt = ‚àë'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='tZt is small.\\nWe can tighten this bound greedily by choosing Œ±t on each iteration to minimize Zt.\\nTo do so, let‚Äôs deÔ¨Åne the error at iteration t as œµt = ‚àë\\niDt(i)I\\n(\\nht(x(i)) Ã∏=y(i))\\n.\\nIt then follows that\\nZt =\\n‚àë\\nt\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) = (1 ‚àíœµt) exp(‚àíŒ±t) +œµt exp(Œ±t).\\nWe can then choose Œ±t that minimizes Zt by solving ‚àÇZt\\n‚àÇŒ±t\\n= 0, which yields Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n.\\nFurther note that Œ±t and œµt are negatively correlated, so intuitively Œ±t is the ‚Äústrength‚Äù of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We can then choose Œ±t that minimizes Zt by solving ‚àÇZt\\n‚àÇŒ±t\\n= 0, which yields Œ±t = 1\\n2 ln\\n(\\n1‚àíœµt\\nœµt\\n)\\n.\\nFurther note that Œ±t and œµt are negatively correlated, so intuitively Œ±t is the ‚Äústrength‚Äù of\\nclassiÔ¨Åer ht. Hence, we have shown how to minimize ‚àè\\ntZt, but how small can this minimum\\nvalue be?\\n8.2.2 Show that training error converges to 0\\nWe can further derive another upper bound for the training error:\\nerr(H) = 1\\nm\\nm‚àë\\ni=1\\nI\\n(\\nH(x(i)) Ã∏=y(i))\\n‚â§\\nT‚àè\\nt=1\\nZt ‚â§ exp\\n(\\n‚àí2\\n‚àë\\nt\\nŒ≥2\\nt\\n)\\n(8.2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 46, 'page_label': '46', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='8.2.2 Show that training error converges to 0\\nWe can further derive another upper bound for the training error:\\nerr(H) = 1\\nm\\nm‚àë\\ni=1\\nI\\n(\\nH(x(i)) Ã∏=y(i))\\n‚â§\\nT‚àè\\nt=1\\nZt ‚â§ exp\\n(\\n‚àí2\\n‚àë\\nt\\nŒ≥2\\nt\\n)\\n(8.2)\\nwhere Œ≥t = 1\\n2 ‚àíœµt.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 47, 'page_label': '47', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Diving in the Math 13 - Convergence of AdaBoost‚Äôs training error\\nNote that\\nŒ±t = 1\\n2 ln\\n(1 ‚àíœµt\\nœµt\\n)\\n‚áí exp(Œ±t) =\\n‚àö\\n1 ‚àíœµt\\nœµt\\n,\\nand also\\nœµt =\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i); 1 ‚àíœµt =\\n‚àë\\ny(i)=ht(x(i))\\nDt(i).\\nWe now have\\nZt =\\n‚àë\\ni\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i))) +\\n‚àë\\ny(i)=ht(x(i))\\nDt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 47, 'page_label': '47', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Dt(i) exp(‚àíŒ±ty(i)ht(x(i)))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)‚àíy(i)ht(x(i))\\n=\\n‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n(‚àö\\n1 ‚àíœµt\\nœµt\\n)\\n+\\n‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n(‚àö œµt\\n1 ‚àíœµt\\n)\\n=\\n‚àö\\n1 ‚àíœµt\\nœµt\\n¬∑\\n\\uf8eb\\n\\uf8ed ‚àë\\ny(i)Ã∏=ht(x(i))\\nDt(i)\\n\\uf8f6\\n\\uf8f8 +\\n‚àö œµt\\n1 ‚àíœµt\\n¬∑\\n\\uf8eb\\n\\uf8ed ‚àë\\ny(i)=ht(x(i))\\nDt(i)\\n\\uf8f6\\n\\uf8f8\\n=\\n‚àö\\n1 ‚àíœµt\\nœµt\\n¬∑œµt +\\n‚àö œµt\\n1 ‚àíœµt\\n¬∑ (1 ‚àíœµt)\\n= 2\\n‚àö\\nœµt(1 ‚àíœµt).\\nFurthermore, for any x ‚àà R, 1 ‚àíx ‚â§ exp(‚àíx). Substitute x = 4Œ≥2\\nt we see that\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí4Œ≥2\\nt ) ‚áí\\n‚àö\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí2Œ≥2\\nt ).\\nNow we have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 47, 'page_label': '47', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚àö\\n1 ‚àíœµt\\nœµt\\n¬∑œµt +\\n‚àö œµt\\n1 ‚àíœµt\\n¬∑ (1 ‚àíœµt)\\n= 2\\n‚àö\\nœµt(1 ‚àíœµt).\\nFurthermore, for any x ‚àà R, 1 ‚àíx ‚â§ exp(‚àíx). Substitute x = 4Œ≥2\\nt we see that\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí4Œ≥2\\nt ) ‚áí\\n‚àö\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí2Œ≥2\\nt ).\\nNow we have\\nZt = 2\\n‚àö\\nœµt(1 ‚àíœµt) =\\n‚àö\\n1 ‚àí 4Œ≥2\\nt ‚â§ exp(‚àí2Œ≥2\\nt ),\\nhence\\nerr(H) ‚â§\\nT‚àè\\nt=1\\nZt ‚â§\\nT‚àè\\nt=1\\nexp(‚àí2Œ≥2\\nt ) = exp(‚àí2\\n‚àë\\nt\\nŒ≥2\\nt ).\\nIt then follows that, as the number of iterations T increases, exp( ‚àí2 ‚àëT\\nt=1Œ≥2\\nt ) decreases expo-\\nnentially, so the training error also approaches 0 exponentially fast.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 48, 'page_label': '48', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='In practice, this also happens quite quickly. In fact, Schapire (1989) showed that in digit recog-\\nnition, the testing error can still decrease even after the training error reaches 0 1. Boosting is\\nalso robust to overÔ¨Åtting.\\nSome weak learners also have their own ensemble methods apart from AdaBoost. For example,\\nan ensemble of decision tree is called a random forest. For each tree we select a subset of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 48, 'page_label': '48', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Some weak learners also have their own ensemble methods apart from AdaBoost. For example,\\nan ensemble of decision tree is called a random forest. For each tree we select a subset of\\nattributes (recommended subset size = square root of number of total attributes) and build the\\ntree using only the selected attributes. An input sample is the classiÔ¨Åed using majority voting.\\n1Note that the training error reported in this graph is the global training error where each input is weighed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 48, 'page_label': '48', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='1Note that the training error reported in this graph is the global training error where each input is weighed\\nequally as usual. During the iterations of AdaBoost, however, we are concerned with the weighted errors œµt. In\\nthis case, while the global training error is 0, the œµt‚Äôs may still be > 0, so there is room for improvement, and\\ntherefore the test error can still decrease.\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 9\\nPrincipal Component Analysis\\n9.1 Introduction\\nSuppose we are given data points in d-dimensional space and want to project them into a\\nlower dimensional space while preserving as much information as possible (e.g., Ô¨Ånd best planar\\napproximation to 3D data or 10 4D data). In particular, choose an orthogonal projection that\\nminimizes the squared error in reconstructing original data.\\nLike auto-encoding neural networks, PCA learns re-representation of input data that can best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='minimizes the squared error in reconstructing original data.\\nLike auto-encoding neural networks, PCA learns re-representation of input data that can best\\nreconstruct it. However, PCA has some diÔ¨Äerences:\\n‚Ä¢ The learned encoding is a linear function of inputs\\n‚Ä¢ No local minimum problems when training\\n‚Ä¢ Givend-dimensional data X, learns d-dimensional representation where:\\n‚Äì the dimensions are orthogonal\\n‚Äì The top k dimensions are the k-dimensional linear re-representation that minimizes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Givend-dimensional data X, learns d-dimensional representation where:\\n‚Äì the dimensions are orthogonal\\n‚Äì The top k dimensions are the k-dimensional linear re-representation that minimizes\\nreconstruction error (sum of squared errors)\\nIn particular, PCA involves orthogonal projection of the data onto a lower-dimensional linear\\nspace that equivalently:\\n1. minimizes the mean squared distance between data points and projections\\n2. maximizes variance of projected data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='space that equivalently:\\n1. minimizes the mean squared distance between data points and projections\\n2. maximizes variance of projected data\\nPCA has the following properties:\\n‚Ä¢ PCA vectors originate from the center of mass (usually we center the data as the Ô¨Årst step)\\n‚Ä¢ Principal component #1: points in the direction of the largest variance\\n‚Ä¢ Each subsequent principal component:\\n‚Äì is orthogonal to the previous ones, and\\n‚Äì points in the directions of the largest variance of the residual subspace'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 49, 'page_label': '49', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Each subsequent principal component:\\n‚Äì is orthogonal to the previous ones, and\\n‚Äì points in the directions of the largest variance of the residual subspace\\n49'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 50, 'page_label': '50', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='9.2 PCA algorithms\\nHere we present 3 diÔ¨Äerent algorithms for performing PCA.\\nAlgorithm 18: (Sequential PCA)\\nGiven centered data {x(1),x (2),...,x (m)}, compute the principal vectors:\\nw1 = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wTx(i))2\\nw2 = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àíw1wT\\n1x(i)))2\\n...\\nwk = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àí\\nk‚àí1‚àë\\nj=1\\nwjwT\\njx(i)))2\\nIn the Sequential algorithm, to Ô¨Ånd w1, we maximize the variance of projection of x. To'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 50, 'page_label': '50', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àíw1wT\\n1x(i)))2\\n...\\nwk = arg max\\n‚à•w‚à•=1\\n1\\nm\\nm‚àë\\ni=1\\n(wT (x(i) ‚àí\\nk‚àí1‚àë\\nj=1\\nwjwT\\njx(i)))2\\nIn the Sequential algorithm, to Ô¨Ånd w1, we maximize the variance of projection of x. To\\nÔ¨Ånd w2, we maximize the variance of the projection in the residual subspace.\\nThe Sequential algorithm is intuitive and gives a sense of what to look for in a principal vector.\\nHowever, it is slow and not often used in practice unless we only care about the Ô¨Årst principal\\nvector.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 50, 'page_label': '50', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='However, it is slow and not often used in practice unless we only care about the Ô¨Årst principal\\nvector.\\nAlgorithm 19: (Sample covariance matrix PCA)\\nGiven data {x(1),x (2),...,x (m)}, compute covariance matrix\\nŒ£ = 1\\nm\\nm‚àë\\ni=1\\n(x(i) ‚àí ¬Øx)(x(i) ‚àí ¬Øx)T\\nwhere ¬Øx = 1\\nm\\n‚àëm\\ni=1x(i). The principal vectors are the eigenvectors of Œ£, and larger eigenval-\\nues corresponds to more important eigenvectors.\\nWhile straightforward, the computation of Œ£ and its eigenvectors is computationally expensive.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 50, 'page_label': '50', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='ues corresponds to more important eigenvectors.\\nWhile straightforward, the computation of Œ£ and its eigenvectors is computationally expensive.\\nIn practice, the most useful method is by singular value decomposition (SVD), which avoids\\nexplicitly computing Œ£.\\n50'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 20: (SVD PCA)\\nPerform SVD of the centered data matrix X = (x(1),...,x (m)) into\\nX =USV T\\nwhere U and V are orthonormal and S is a diagonal matrix. The columns of V are the\\neigenvectors and the diagonal values of S - which are square roots of the eigenvectors of V -\\ndenote the importance of each eigenvector in descending order. The top k principal vectors\\nare the columns of VT are the leftmost k columns of V .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='denote the importance of each eigenvector in descending order. The top k principal vectors\\nare the columns of VT are the leftmost k columns of V .\\nFormally, the SVD of a matrix A is just the decomposition of A into three other matrices, which\\nwe call U, S, and V . The dimensions of these matrices are given as subscripts in the formula\\nbelow:\\nAn√óm =Un√ónSn√ómVT\\nm√óm.\\nThe columns of U are orthonormal eigenvectors of AAT . The columns of V are orthonormal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='below:\\nAn√óm =Un√ónSn√ómVT\\nm√óm.\\nThe columns of U are orthonormal eigenvectors of AAT . The columns of V are orthonormal\\neigenvectors of ATA. The matrix S is diagonal, with the square roots of the eigenvalues from\\nU (or V ; the eigenvalues of ATA are the same as those of AAT ) in descending order. These\\neigenvalues are called the singular values of A.\\n9.3 PCA applications\\nThe main applications of PCA are:\\n‚Ä¢ Data visualization - by reducing the number of dimensions to 2 or 3, we can plot the data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='eigenvalues are called the singular values of A.\\n9.3 PCA applications\\nThe main applications of PCA are:\\n‚Ä¢ Data visualization - by reducing the number of dimensions to 2 or 3, we can plot the data\\npoints on a graph\\n‚Ä¢ Noise reduction, e.g. eigenfaces\\n‚Ä¢ Data compression\\n9.3.1 Eigenfaces\\nWe want to identify speciÔ¨Åc person, based on facial image, robust to glasses, lighting, facial\\nexpression, ... (i.e., they are considered noise in this case). Each image is 256 √ó 256 pixels so'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='We want to identify speciÔ¨Åc person, based on facial image, robust to glasses, lighting, facial\\nexpression, ... (i.e., they are considered noise in this case). Each image is 256 √ó 256 pixels so\\neach input x is 2562 = 65536 dimensional.\\nSince the number of dimensions is too large, we cannot perform classiÔ¨Åcation directly. Instead,\\nwe use PCA on the whole dataset to get ‚Äúprincipal component‚Äù images (the eigenfaces), then\\nclassify based on projection weights onto these principal component images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 51, 'page_label': '51', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='we use PCA on the whole dataset to get ‚Äúprincipal component‚Äù images (the eigenfaces), then\\nclassify based on projection weights onto these principal component images.\\n51'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Suppose there arem instances each with dimensionN (in this casem = 50,,N = 65536). Given\\na N √óN covariance matrix Œ£, can compute:\\n‚Ä¢ All N eigenvectors / eigenvalues in O(N 3)\\n‚Ä¢ First k eigenvectors / eigenvalues in O(kN 2)\\nBut this is expensive if N = 65536. However, there is a clever workaround, since we note that\\nm ‚â™ 65536. SpeciÔ¨Åcally, we Ô¨Årst compute the eigenvectors v‚Äôs of L = XTX (which is much\\nsmaller, only m √óm), then for each v, Xv would be an eigenvector of XTX.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='m ‚â™ 65536. SpeciÔ¨Åcally, we Ô¨Årst compute the eigenvectors v‚Äôs of L = XTX (which is much\\nsmaller, only m √óm), then for each v, Xv would be an eigenvector of XTX.\\nDiving in the Math 14 - Proof of workaround for eigenfaces\\nWe want to prove that ifv is eigenvector of L =XTX thenXv is eigenvector of Œ£ = XXT .\\nBased on the deÔ¨Ånition of eigenvector, there exists Œ≥ such that\\nLv =Œ≥v\\nXTXv =Œ≥v\\nX(XTXv) =X(Œ≥v) =Œ≥Xv\\n(XXT )Xv =Œ≥(Xv)\\nŒ£(Xv) =Œ≥(Xv)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Based on the deÔ¨Ånition of eigenvector, there exists Œ≥ such that\\nLv =Œ≥v\\nXTXv =Œ≥v\\nX(XTXv) =X(Œ≥v) =Œ≥Xv\\n(XXT )Xv =Œ≥(Xv)\\nŒ£(Xv) =Œ≥(Xv)\\nAgain, using the deÔ¨Ånition of eigenvector, we see that Xv is an eigenvector of Œ£, also with\\neigenvalueŒ≥.\\nIn other words, we do not have to compute the eigenvalues of Œ£ directly from Œ£, but through\\nL. This would reduce the runtime to O(Nm 2) +O(km2), where k is the speciÔ¨Åed number of\\neigenvectors (i.e., the number of dimensions we want to reduce to).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='L. This would reduce the runtime to O(Nm 2) +O(km2), where k is the speciÔ¨Åed number of\\neigenvectors (i.e., the number of dimensions we want to reduce to).\\nWe can then reconstruct the faces using some of the top principal vector. As more eigenvectors\\nare used, we get back more detailed faces but without noises such as lighting, glasses and facial\\nexpression. The below Ô¨Ågures demonstrate we can reconstruct one particular face, starting from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 52, 'page_label': '52', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='are used, we get back more detailed faces but without noises such as lighting, glasses and facial\\nexpression. The below Ô¨Ågures demonstrate we can reconstruct one particular face, starting from\\nusing only one principal vectors, then adding more and more. The circled face denotes the best\\napproximation without noise.\\n52'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 53, 'page_label': '53', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Note: this method is quite old. Nowadays we use deep neural network.\\n9.3.2 Image compression\\nTo compress an image, we can divide it in patches (12 √ó 12 pixels on a grid), so each patch is a\\n144-D vector input. Using PCA on these inputs, reconstructing the patches, then putting them\\nback together again will give us the compressed version. In some cases, using only 13 principal\\nvectors can already reduce the relative error to 5% (i.e., most information is in the top principal\\nvectors).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 53, 'page_label': '53', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='vectors can already reduce the relative error to 5% (i.e., most information is in the top principal\\nvectors).\\n9.4 Shortcomings\\nPCA is unsupervised and doesn‚Äôt care about the labels. It maximizes the variance, independence\\nof class. For example, in the plot below, if we want to reduce the dimension to 1 while preserving\\nclass separations, we would pick the green line. However, PCA would pick the magenta line\\ninstead.\\nFurthermore, PCA can only capture linear relationships.\\n53'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 54, 'page_label': '54', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 10\\nHidden Markov Model\\n10.1 Introduction\\nDeÔ¨Ånition 11: (Hidden Markov Model)\\nA Hidden Markov Model consists of the followings:\\n‚Ä¢ A set of states S = {s1,s 2,...,s n}. At each time t we are in exactly one of these\\nstates, denoted by qt.\\n‚Ä¢ A list œÄ1,œÄ 2,...,œÄ n where œÄi is the probability that we start at state i.\\n‚Ä¢ A transition probability matrix Aj,i = P (qt = si | qt‚àí1 = sj), which denotes the\\nprobability of transitioning from sj to si.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 54, 'page_label': '54', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ A transition probability matrix Aj,i = P (qt = si | qt‚àí1 = sj), which denotes the\\nprobability of transitioning from sj to si.\\n‚Ä¢ A set of possible outputs Œ£, at each time t we emit a symbol œÉt ‚àà Œ£.\\n‚Ä¢ An emission probability matrix Bt,i = P (ot | si), which denotes the probability of\\nemitting symbol œÉt at state si.\\nFor example, a two-state HMM may look like the followings:\\nFigure 10.1: An example Hidden Markov Model with 2 states.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 54, 'page_label': '54', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='emitting symbol œÉt at state si.\\nFor example, a two-state HMM may look like the followings:\\nFigure 10.1: An example Hidden Markov Model with 2 states.\\nNote the Markov property from the above deÔ¨Ånition: given qt, qt+1 is conditionally inde-\\npendent on qt‚àí1 or any earlier time point . In other words, knowing qt is suÔ¨Écient to infer\\nabout the state of qt+1.\\nWith n states and m output symbols, we can see that there are n starting parameters œÄi, n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 54, 'page_label': '54', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='about the state of qt+1.\\nWith n states and m output symbols, we can see that there are n starting parameters œÄi, n2\\ntransition probabilities Aji, and mn emission probabilities Bik, for a total of n2 +mn +n =\\nO(n2 +mn) parameters. We will discuss how to learn these parameters from data later on, but\\nlet‚Äôs Ô¨Årst focus on the inference task. Assuming all of these parameters are already known, what\\nkind of information can we infer?\\n54'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 55, 'page_label': '55', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='10.2 Inference in HMM\\nThere are three big questions that a learned HMM can answer:\\n1. What is P (qt =si)? In other words, what is the probability that we end up in state si at\\ntime t, without observing any output ?\\n2. What is P (qt = si | o1o2...o t)? In other words, given a sequence of output symbols\\no1o2...o t, what is the probability that we end up in state si at time t?\\n3. What is arg max\\nq1q2...qt\\nP (q1q2...q t | o1o2...o t)? In other words, given a sequence of output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 55, 'page_label': '55', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='o1o2...o t, what is the probability that we end up in state si at time t?\\n3. What is arg max\\nq1q2...qt\\nP (q1q2...q t | o1o2...o t)? In other words, given a sequence of output\\nsymbolso1o2...o t, what is the sequence of states q1q2...q t that is most likely to generate\\nthis output?\\nBefore moving on, we show an example application of these questions. A popular technique of\\nmodeling student knowledge in Educational Data Mining is called Bayesian Knowledge Tracing.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 55, 'page_label': '55', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='this output?\\nBefore moving on, we show an example application of these questions. A popular technique of\\nmodeling student knowledge in Educational Data Mining is called Bayesian Knowledge Tracing.\\nThe high-level goal is: given data about the correctness of a student‚Äôs answers on a set of\\nproblems related to a certain skill, can we say whether the student has mastered this skill?\\n55'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='DeÔ¨Ånition 12: (Bayesian Knowledge Tracing)\\nFor each skill, BKT models student knowledge as a binary latent variable in an HMM ,\\nwhich consists of the followings:\\n‚Ä¢ Two states: S = {Mastered, Unmastered}.\\n‚Ä¢ œÄMastered: the probability of the student starting at Mastered, i.e., knowing the skill\\nbeforehand.\\n‚Ä¢ pLearn: the probability of transitioning from Unmastered to Mastered. pForget: the\\nprobability of transitioning from Mastered to Unmastered.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='beforehand.\\n‚Ä¢ pLearn: the probability of transitioning from Unmastered to Mastered. pForget: the\\nprobability of transitioning from Mastered to Unmastered.\\n‚Ä¢ Two possible outputs C (correct) and I (incorrect): whether student‚Äôs answer to a\\nproblem is correct or incorrect.\\n‚Ä¢ pGuess = p(ot = C | qt = Unmastered): the probability of getting a correct answer\\ndespite not mastering the skill, i.e., guessing.\\n‚Ä¢ pSlip = p(ot = I | qt = Mastered): the probability of getting an incorrect answer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='despite not mastering the skill, i.e., guessing.\\n‚Ä¢ pSlip = p(ot = I | qt = Mastered): the probability of getting an incorrect answer\\ndespite mastering the skill, i.e., slipping.\\nMastered Unmastered\\nC I\\npForget\\n1 ‚àípForget\\npLearn\\n1 ‚àípLearn\\n1 ‚àípGuess\\npSlip pGuess\\n1 ‚àípSlip\\nUsing Inference #2, we can ask questions like: if the student submits 5 answers and gets the\\nÔ¨Årst 3 correct but last 2 incorrect, what is the probability that she has mastered the skill? More'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Using Inference #2, we can ask questions like: if the student submits 5 answers and gets the\\nÔ¨Årst 3 correct but last 2 incorrect, what is the probability that she has mastered the skill? More\\nformally, what isP (Mastered | CCCII)? Is this diÔ¨Äerent from, for example,P (Mastered | IICCC)\\n(getting Ô¨Årst 2 incorrect but last 3 correct)?\\n10.2.1 What is P (qt =si)?\\nSince we don‚Äôt have observed data, the emission probabilities can be ignored. Instead, we simply'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 56, 'page_label': '56', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(getting Ô¨Årst 2 incorrect but last 3 correct)?\\n10.2.1 What is P (qt =si)?\\nSince we don‚Äôt have observed data, the emission probabilities can be ignored. Instead, we simply\\nrely on the priors and transition probabilities. For example, in Figure 10.1 we can compute\\nP (q2 =A) as\\nP (q2 =A) =P (q2 =A |q1 =A) ¬∑p(q1 =A) +p(q2 =A |q1 =B) ¬∑p(q1 =B)\\n= AAA ¬∑œÄA + ABA ¬∑œÄB,\\n56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='and then P (q3 =B) as\\nP (q3 =B) =P (q3 =B |q2 =A) ¬∑p(q2 =A) +P (q3 =B |q2 =B) ¬∑p(q2 =B)\\n= AAB ¬∑ (AAA ¬∑œÄA + ABA ¬∑œÄB) + ABB ¬∑ (AAB ¬∑œÄA + ABB ¬∑œÄB).\\nIn general,\\nP (qt =si) =\\n‚àë\\nq1,q2,...,qt‚àí1‚ààS\\nP (si |q1q2...q t‚àí1)P (q1q2...q t‚àí1). (10.1)\\nHowever, this is too costly to compute, with runtime O(2n‚àí1). Instead, an optimization trick is\\nto use dynamic programming:\\nAlgorithm 21: (Computing Ô¨Ånal state without observations)\\nWe perform two steps:\\n‚Ä¢ Base case: P (q1 =si) =œÄi\\n‚Ä¢ Inductive case:\\nP (qt+1 =si) =\\n‚àë'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='to use dynamic programming:\\nAlgorithm 21: (Computing Ô¨Ånal state without observations)\\nWe perform two steps:\\n‚Ä¢ Base case: P (q1 =si) =œÄi\\n‚Ä¢ Inductive case:\\nP (qt+1 =si) =\\n‚àë\\nj‚ààS\\nP (qt+1 =si |qt =sj) ¬∑P (qt =sj) =\\n‚àë\\nj‚ààS\\nAji ¬∑P (qt =sj). (10.2)\\n10.2.2 What is P (qt =si |o1o2...o t)?\\nUsing the chain rule, we Ô¨Årst see that\\nP (qt =si |o1o2...o t) = P (qt =si ‚àßo1o2...o t)\\nP (o1o2...o t) = P (qt =si ‚àßo1o2...o t)‚àë\\nj‚ààS\\nP (qt =sj ‚àßo1o2...o t)\\n(10.3)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Using the chain rule, we Ô¨Årst see that\\nP (qt =si |o1o2...o t) = P (qt =si ‚àßo1o2...o t)\\nP (o1o2...o t) = P (qt =si ‚àßo1o2...o t)‚àë\\nj‚ààS\\nP (qt =sj ‚àßo1o2...o t)\\n(10.3)\\nThis motivates us to deÔ¨ÅneŒ±t(i) =P (qt =si ‚àßo1o2...o t). We can then computeŒ±t(i) as follows.\\nAlgorithm 22: (Computing Ô¨Ånal state given observed output sequence)\\nTo computeŒ±t(i), we perform two steps:\\n‚Ä¢ Base case: Œ±1(i) =P (q1 =si ‚àßo1) =P (o1 |q1 =s1)P (q1 =s1) = B1,i ¬∑œÄi.\\n‚Ä¢ Inductive case:\\nŒ±t+1(i) =\\n‚àë\\nj‚ààS\\nŒ±t(j) ¬∑ Aj,i ¬∑ Bt+1,i. (10.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='To computeŒ±t(i), we perform two steps:\\n‚Ä¢ Base case: Œ±1(i) =P (q1 =si ‚àßo1) =P (o1 |q1 =s1)P (q1 =s1) = B1,i ¬∑œÄi.\\n‚Ä¢ Inductive case:\\nŒ±t+1(i) =\\n‚àë\\nj‚ààS\\nŒ±t(j) ¬∑ Aj,i ¬∑ Bt+1,i. (10.4)\\nIt follows that\\nP (qt =si |o1o2...o t) = Œ±t(i)‚àë\\nj‚ààS\\nŒ±t(j)\\n(10.5)\\n10.2.3 What is arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t)?\\nLet\\nŒ¥t(i) = max\\nq1,...,qt‚àí1‚ààS\\nP (q1...q t‚àí1 ‚àßqt =si ‚àßo1...o t). (10.6)\\nIn other words, Œ¥t(i) is the probability of the most likely path from time 1 to t that produces'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 57, 'page_label': '57', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='P (q1q2...q t |o1o2...o t)?\\nLet\\nŒ¥t(i) = max\\nq1,...,qt‚àí1‚ààS\\nP (q1...q t‚àí1 ‚àßqt =si ‚àßo1...o t). (10.6)\\nIn other words, Œ¥t(i) is the probability of the most likely path from time 1 to t that produces\\noutput o1...o t and ends in si. We can then compute Œ¥t(i) as follows.\\n57'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 58, 'page_label': '58', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Algorithm 23: (Viterbi Algorithm)\\nWe perform two steps:\\n‚Ä¢ Base case: Œ¥1(i) =œÄi ¬∑ B1,i.\\n‚Ä¢ Inductive case:\\nŒ¥t+1(i) = max\\nj\\nŒ¥t(i) ¬∑ Aj,i ¬∑ Bt+1,i. (10.7)\\nIt follows that arg max\\nq1q2...qt\\nP (q1q2...q t |o1o2...o t) is the path deÔ¨Åned by arg max\\nj\\nŒ¥t(j).\\n58'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 59, 'page_label': '59', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 11\\nReinforcement Learning\\n11.1 Markov decision process\\nA Markov decision process is a set of nodes and edges where nodes represent states and edges\\nrepresent transition. In this case, the transitions are only based on the previous state (same as\\nin HMM). Unlike HMM, however:\\n‚Ä¢ We know all the states, each state associated with a reward.\\n‚Ä¢ We can have an inÔ¨Çuence on the transition.\\nAn obvious question for such models: what is the combined expected value for each state? What'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 59, 'page_label': '59', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ We can have an inÔ¨Çuence on the transition.\\nAn obvious question for such models: what is the combined expected value for each state? What\\ncan we expect to earn over our lifetime if we become asst. prof / go to industry? Before we\\nanswer this quesiton, we need to deÔ¨Åne a model for future rewards. In particular, the value of a\\ncurrent award is higher than the value of future awards (inÔ¨Çation, conÔ¨Ådence). This discounted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 59, 'page_label': '59', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='answer this quesiton, we need to deÔ¨Åne a model for future rewards. In particular, the value of a\\ncurrent award is higher than the value of future awards (inÔ¨Çation, conÔ¨Ådence). This discounted\\nreward model is speciÔ¨Åed using a parameter 0 <Œ≥ < 1. Therefore, if we let rt be the reward at\\ntime t, then the total reward is\\nTotal =\\n‚àû‚àë\\nt=0\\nŒ≥trt, (11.1)\\n59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 60, 'page_label': '60', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='which does converge because Œ≥ ‚àà (0, 1).\\nNow, let‚Äôs deÔ¨Åne J‚àó(si) as the expected discounted sum of rewards when starting at state si. It\\nfollows that\\nJ‚àó(si) =ri +Œ≥\\nn‚àë\\nk=1\\npikJ‚àó(sk), (11.2)\\nwherepik is the transition probability from si tosk and the sum represents the expected pay for\\nall possible transitions from si.\\nWe have n equations like (11.2), one for each si, so this is a linear system of equations and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 60, 'page_label': '60', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='all possible transitions from si.\\nWe have n equations like (11.2), one for each si, so this is a linear system of equations and\\na closed form solution can be derived, but may be time consuming. It also doesn‚Äôt generalize\\nto non-linear models. Alternatively, this problem can be solved in an iterative manner: deÔ¨Åne\\nJt(si) as the expected discounted reward after t steps, then J1(si) =ri and\\nJt+1(si) =ri +Œ≥\\n‚àë\\nk\\npikJt(sk). (11.3)\\nThis can be computed via dynamic programming, and we can stop when max'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 60, 'page_label': '60', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Jt(si) as the expected discounted reward after t steps, then J1(si) =ri and\\nJt+1(si) =ri +Œ≥\\n‚àë\\nk\\npikJt(sk). (11.3)\\nThis can be computed via dynamic programming, and we can stop when max\\ni\\n|Jt+1(si)‚àíJt(si)|<\\nœµ for some threshold œµ, which we know will happen because Jt(si) converges.\\n11.2 Reinforcement learning - No action\\nIn reinforcement learning, we use the same Markov model with rewards and actions. But there\\nare a few diÔ¨Äerences:\\n1. We do not assume we know the Markov model\\n60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='2. We adapt to new observation (online vs oÔ¨Ñine)\\nIn other words, we want to learn the expected reward but do not know the model, and in this\\ncase we do so by learning both the reward and model at the same time (e.g., game playing, robot\\ninteracting with environment). Unlike HMM, if we move to a state, we know which state that is\\n(i.e., the states are observed); other than that, however, we don‚Äôt know the reward at that state\\nor the transition probabilities.\\n11.2.1 Supervised RL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(i.e., the states are observed); other than that, however, we don‚Äôt know the reward at that state\\nor the transition probabilities.\\n11.2.1 Supervised RL\\nMore formally, we deÔ¨Åne the scenario in reinforcement learning as follows: we are wandering the\\nworld, and at each time point we see a state and a reward. Our goal is to compute the sum of\\ndiscounted rewards for each state Jest(si). For example, given the following observations\\ns1, 4 s2, 0 s3, 2 s2, 2 s4, 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='discounted rewards for each state Jest(si). For example, given the following observations\\ns1, 4 s2, 0 s3, 2 s2, 2 s4, 0\\nIn general, we have the supervised learning algorithm for RL as follows.\\nAlgorithm 24: (Supervised reinforcement learning)\\nObserve set of states and rewards (s(0),r (0)), (s(1),r (1)),..., (s(T ),r (T )).\\nFort = 0,...,T compute discounted sum\\nJ(t) =\\nT‚àë\\ni=t\\nŒ≥i‚àítr(i).\\nCompute Jest(si) (mean of J(t) for t such that s(t) =si):\\nJest(si) =\\n‚àëT\\nt=0J(t)I (s(t) =si)‚àëT\\nt=0 I (s(t) =si)\\n.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Fort = 0,...,T compute discounted sum\\nJ(t) =\\nT‚àë\\ni=t\\nŒ≥i‚àítr(i).\\nCompute Jest(si) (mean of J(t) for t such that s(t) =si):\\nJest(si) =\\n‚àëT\\nt=0J(t)I (s(t) =si)‚àëT\\nt=0 I (s(t) =si)\\n.\\nHere we assume that we observe each state frequently enough and that we have many observa-\\ntions so that the Ô¨Ånal observations do not have a big impact on our prediction. Each update\\ntakesO(n) where n is the number of states, since we are updating vectors containing entries for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 61, 'page_label': '61', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='tions so that the Ô¨Ånal observations do not have a big impact on our prediction. Each update\\ntakesO(n) where n is the number of states, since we are updating vectors containing entries for\\nall states. Space is also O(n). Convergence to J‚àó can be proven, and the algorithm can be more\\neÔ¨Écient by ignoring states for which discounted factor Œ≥i is very low already.\\nHowever, the supervised learning approach has two problems:\\n61'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 62, 'page_label': '62', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Takes a long time to converge, because we don‚Äôt try to learn the underlying MDP model,\\nbut just focus on Jest.\\n‚Ä¢ Does not use all available data, we can learn transition probabilities as well.\\nIn other words, we want to utilize the fact that there is an underlying model and the transitions\\nare not completely random.\\n11.2.2 Certainty-Equivalence learning\\nAlgorithm 25: (Certainty-Equivalence (CE) learning)\\nWe keep track of 3 vectors:\\n‚Ä¢ Count(s): number of times we visited state s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 62, 'page_label': '62', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='are not completely random.\\n11.2.2 Certainty-Equivalence learning\\nAlgorithm 25: (Certainty-Equivalence (CE) learning)\\nWe keep track of 3 vectors:\\n‚Ä¢ Count(s): number of times we visited state s\\n‚Ä¢ J(s): sum of rewards from state s\\n‚Ä¢ Trans (i,j ): number of time we transitioned from si to sj\\nWhen we visit state si, receive reward r and move to state sj we do the following:\\n‚Ä¢ Counts(si) =Counts(si) + 1\\n‚Ä¢ J(si) =J(si) +r\\n‚Ä¢ Trans (i,j ) =Trans (i,j ) + 1\\nAt any time, we can estimate:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 62, 'page_label': '62', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='When we visit state si, receive reward r and move to state sj we do the following:\\n‚Ä¢ Counts(si) =Counts(si) + 1\\n‚Ä¢ J(si) =J(si) +r\\n‚Ä¢ Trans (i,j ) =Trans (i,j ) + 1\\nAt any time, we can estimate:\\n‚Ä¢ Reward estimate rest(s) =J(s)/Counts(s)\\n‚Ä¢ Transition probability estimate\\n‚Ä¢ pest(j |i) =Trans (i,j )/Counts(si)\\nAfter learning the model, we can now have an estimate which we can solve for all states si:\\nJest(si) =rest(si) +Œ≥\\n‚àë\\nj\\npest(sj |si)Jest(si), i = 1,...,n (11.4)\\n62'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 63, 'page_label': '63', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='The runtime of CE comes from two steps: update (O(1)) and solving MDP (O(n3) using matrix\\ninversion). The space is O(n2) for transition probabilities.\\nTo reduce runtime, we could use the ‚ÄúOne backup‚Äù version, which updatesJest(si) for the current\\nstate si while learning the model , instead of solving n equations after learning like in (11.4). In\\nthis case, the runtime is only O(n), and we can sill prove convergence to J‚àó (but slower than\\nCE). The space remains at O(n2).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 63, 'page_label': '63', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='this case, the runtime is only O(n), and we can sill prove convergence to J‚àó (but slower than\\nCE). The space remains at O(n2).\\n11.2.3 Temporal diÔ¨Äerence learning\\nWe now look at another algorithm with the same eÔ¨Éciency as one backup CE but requires much\\nless space. In particular, we can ignore all the rest and pest and only focus on Jest with a new\\napproximation rule.\\nAlgorithm 26: (Temporal diÔ¨Äerence (TD) learning)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 63, 'page_label': '63', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='less space. In particular, we can ignore all the rest and pest and only focus on Jest with a new\\napproximation rule.\\nAlgorithm 26: (Temporal diÔ¨Äerence (TD) learning)\\nWe only maintain the Jest array. Assume we have Jest(s1),...,J est(sn). If we observe a\\ntransition from state si to state sj and a reward r, we update using the following rule\\nJest(si) = (1 ‚àíŒ±)Jest(si) +Œ±(r +Œ≥J est(sj)), (11.5)\\nwhereŒ± is a hyper-parameter to determine how much weight we place on the current obser-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 63, 'page_label': '63', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Jest(si) = (1 ‚àíŒ±)Jest(si) +Œ±(r +Œ≥J est(sj)), (11.5)\\nwhereŒ± is a hyper-parameter to determine how much weight we place on the current obser-\\nvation (and can change during the algorithm, unlike Œ≥).\\nAs always, choosing a good Œ± is an issue. Nevertheless, it can be proven that TD learning is\\nguaranteed to converge if:\\n‚Ä¢ All states are visited often.\\n‚Ä¢ ‚àë\\ntŒ±t = ‚àû\\n‚Ä¢ ‚àë\\ntŒ±2\\nt < ‚àû\\n63'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='For example,Œ±t = C\\nt for some constant t would satisfy both requirements.\\nNow the runtime of TD is O(1) because there is only one update (11.5) at each iteration, and\\nthe space is O(n) because of the Jest array.\\nHere is a summary so far of the four reinforcement learning algorithms.\\nMethod Time Space\\nSupervised learning O(n) O(n)\\nCE learning O(n3) O(n2)\\nOne backup CE O(n) O(n2)\\nTD learning O(1) O(n)\\n11.3 Reinforcement learning with action - Policy learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Method Time Space\\nSupervised learning O(n) O(n)\\nCE learning O(n3) O(n2)\\nOne backup CE O(n) O(n2)\\nTD learning O(1) O(n)\\n11.3 Reinforcement learning with action - Policy learning\\nSo far we assumed that we cannot impact the outcome transition. In real world situations we\\noften have a choice of actions we take (as we discussed for MDPs). How can we learn the best\\npolicy for such cases?\\nNote the diÔ¨Äerence in the pest table - while the columns are still the states (because we only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='policy for such cases?\\nNote the diÔ¨Äerence in the pest table - while the columns are still the states (because we only\\ntransition from state to state), the rows are now (state, action) pair, because each action leads\\nto a diÔ¨Äerent transition. Our goal is to learn the action that leads to the most reward. In\\nparticular, we can update CE by setting\\nJest(si) =rest(si) + max\\na\\n(\\nŒ≥\\n‚àë\\nj\\npest(sj |si,a )Jest(sj)\\n)\\n. (11.6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 64, 'page_label': '64', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='particular, we can update CE by setting\\nJest(si) =rest(si) + max\\na\\n(\\nŒ≥\\n‚àë\\nj\\npest(sj |si,a )Jest(sj)\\n)\\n. (11.6)\\nAs mentioned above, we can also use TD learning for better eÔ¨Éciency. However, TD is model\\nfree, so in this context, we can adjust TD to learn policies by deÔ¨Åning Q‚àó(si,a ) = expected sum\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 65, 'page_label': '65', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='of future (discounted) rewards if we start at state si and take action a. Then, when we take a\\nspeciÔ¨Åc action a in state si and then transition to state sj we can update\\nQest(si,a ) = (1 ‚àíŒ±)Qest(si,a ) +Œ±(ri +Œ≥ max\\na‚Ä≤\\nQest(sj,a‚Ä≤)). (11.7)\\nInstead of the Jest vector we maintain the Qest matrix, which is a rather sparse n by m matrix\\n(n states and m actions).\\nIn practice, when choosing the next action, we may not necessarily pick the one that results in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 65, 'page_label': '65', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='(n states and m actions).\\nIn practice, when choosing the next action, we may not necessarily pick the one that results in\\nthe highest expected sum of future rewards, because we are only sampling from the distribution\\nof possible outcomes. We do not want to avoid potentially beneÔ¨Åcial actions. Instead, we can\\ntake a more probabilistic approach\\np(a) = 1\\nZ exp\\n(\\n‚àíQest(si,a )\\nf(t)\\n)\\n, (11.8)\\nwhere Z is a normalizing constant and f(t) decreases as time t goes by, to represent that we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 65, 'page_label': '65', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='take a more probabilistic approach\\np(a) = 1\\nZ exp\\n(\\n‚àíQest(si,a )\\nf(t)\\n)\\n, (11.8)\\nwhere Z is a normalizing constant and f(t) decreases as time t goes by, to represent that we\\nare more conÔ¨Ådent in the learned model. We can initialize Q values to be high to increase the\\nlikelihood that we will explore more options. Finally, it can be shown that Q learning converges\\nto optimal policy.\\n65'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='Chapter 12\\nGeneralization and Model Selection\\n12.1 True risk vs Empirical risk\\nDeÔ¨Ånition 13: (True risk)\\nTrue risk is the target performance measure. It is deÔ¨Åned as is the probability of misclassiÔ¨Å-\\ncationP (f(X) Ã∏=Y ) in classiÔ¨Åcation and mean squared error E[(f(X) ‚àíY )2] in regression.\\nMore generally, it is the expected performance on a random test point ( X,Y ).\\nWhile we want to minimize true risk, we do not know that the underlying distribution of X and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='More generally, it is the expected performance on a random test point ( X,Y ).\\nWhile we want to minimize true risk, we do not know that the underlying distribution of X and\\nY is. What we do know are the samples ( Xi,Yi), which give us the empirical risk.\\nDeÔ¨Ånition 14: (Empirical risk)\\nEmpirical risk is the performance on training data. It is deÔ¨Åned as proportion of misclassiÔ¨Åed\\nexamples 1\\nn\\n‚àën\\ni=1 I (f(Xi) Ã∏=Yi) in classiÔ¨Åcation and average squared error 1\\nn\\n‚àën\\ni=1(f(Xi) ‚àí\\nYi)2 in regression.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='examples 1\\nn\\n‚àën\\ni=1 I (f(Xi) Ã∏=Yi) in classiÔ¨Åcation and average squared error 1\\nn\\n‚àën\\ni=1(f(Xi) ‚àí\\nYi)2 in regression.\\nSo we want to minimize the empirical risk and evaluate the true risk, but this may lead to\\noverÔ¨Åtting (i.e., small training error but large generalization error). For instance, the following\\ngraph shows two classiÔ¨Åers for a binary classiÔ¨Åcation problem (football player or not). While\\nthe classiÔ¨Åer on the right has zero training error, we are much less inclined to believe that it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='graph shows two classiÔ¨Åers for a binary classiÔ¨Åcation problem (football player or not). While\\nthe classiÔ¨Åer on the right has zero training error, we are much less inclined to believe that it\\ncaptures the true distribution. Here it is more likely that football players simply have higher\\nheight and weight, which match better with the classiÔ¨Åer on the left.\\nThe question is: when should we not minimize the empirical risk completely? The following'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 66, 'page_label': '66', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='height and weight, which match better with the classiÔ¨Åer on the left.\\nThe question is: when should we not minimize the empirical risk completely? The following\\ngraph shows what the empirical risk and true risk may look like as we increase the model\\ncomplexity. Initially both types of risk would decrease, but after some point (the Best Model\\n66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 67, 'page_label': '67', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='point), we started Ô¨Åtting the noise instead of the true data. In that case, the empirical risk can\\nkeep decreasing while the true risk increases.\\nAgain, we do not know how true risk in practice, which makes this a diÔ¨Écult problem. Can we\\nestimate the true risk in a way better than just using the empirical risk? One way is to use\\nstructural risk minimization.\\nDeÔ¨Ånition 15: (Structural risk minimization)\\nPenalize models using bound on deviation of true and empirical risk\\nÀÜfn = arg min\\nf‚ààF'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 67, 'page_label': '67', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='structural risk minimization.\\nDeÔ¨Ånition 15: (Structural risk minimization)\\nPenalize models using bound on deviation of true and empirical risk\\nÀÜfn = arg min\\nf‚ààF\\n{ ÀÜRn(f) +ŒªC(f)}, (12.1)\\nwhereŒª is a tuning parameter chosen by model selection, andC(f) is the bound on deviation\\nfrom true risk a. In essence, instead of minimizing the unknown true risk directly, we try to\\nminimize an upper bound (with high probability) on the true risk.\\naWe will discuss how to derive these later.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 67, 'page_label': '67', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='minimize an upper bound (with high probability) on the true risk.\\naWe will discuss how to derive these later.\\nIn other words, we penalize models based on prior information (bias) or information criteria\\n(MDL, AIC, BIC). In ML there is a ‚Äúno free lunch‚Äù theorem: given only the data, we cannot\\nlearn anything. We need some kind of prior information (inductive bias); for example, in using\\nlinear regression, our inductive bias is that the data can be Ô¨Åt by a line. The inductive bias'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 67, 'page_label': '67', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='learn anything. We need some kind of prior information (inductive bias); for example, in using\\nlinear regression, our inductive bias is that the data can be Ô¨Åt by a line. The inductive bias\\nin this case, also called Occam‚Äôs Razor, is to seek the simplest explanation (e.g., if a 10-degree\\n67'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 68, 'page_label': '68', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='polynomial and 100-degree polynomial say roughly the same things, pick the former).\\nInductive bias can also come from domain knowledge. For example, the function of oil spill\\ncontamination should be smooth (if one point is contaminated, the points around it should be\\nas well), while the function of photon arrival is not. Therefore, even if we get the same data,\\nthe Ô¨Åt functions may look very diÔ¨Äerent.\\nAn example of penalizing complex models using prior knowledge is regularized linear regres-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-02-01T21:24:46+00:00', 'author': '', 'keywords': '', 'moddate': '2020-02-01T21:24:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\notes.pdf', 'total_pages': 69, 'page': 68, 'page_label': '68', 'source_file': 'notes.pdf', 'file_type': 'pdf'}, page_content='the Ô¨Åt functions may look very diÔ¨Äerent.\\nAn example of penalizing complex models using prior knowledge is regularized linear regres-\\nsion, which uses some norm of regression coeÔ¨Écients as the cost C(f). An example of penal-\\nizing models based on information content is AIC ( C(f) = # parameters) or BIC ( C(f) =\\n# parameters √ó logn). AIC allows # parameters to be inÔ¨Ånite as # of training data n becomes\\nlarge, while BIC penalizes complex models more heavily.\\n12.2 Model Selection\\n68')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b90e7",
   "metadata": {},
   "source": [
    "### Embedding and Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38e278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG Tutorial\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List , Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcc7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model : all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1004.42it/s, Materializing param=pooler.dense.weight]                            \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully . Embedding dimension : 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x18b8fc1d400>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using Sentence Transformer\"\"\"\n",
    "\n",
    "    def __init__(self,model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "            Initialize the embedding manager\n",
    "\n",
    "            Args:\n",
    "               model_name :Hugging face model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the Sentence Transformer Model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model : {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully . Embedding dimension : {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name} : {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self,texts:List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            Generate embeddings for a list of texts\n",
    "\n",
    "            Args : texts : List of txt strings to embed\n",
    "            Returns : numpy arrays of embeddings with shape (len(texsts),embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} textss....\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape : {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "## initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager\n",
    "                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ed0f1",
   "metadata": {},
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromadb vector database\"\"\"\n",
    "\n",
    "    def __init__(self,collection_name:str = \"pdf_documents\",persist_directory:str = \"..data/vector_ddb\"):\n",
    "        \"\"\"\n",
    "            Initialize Vector DB\n",
    "\n",
    "            Args : collection_name => Name of the ChromaDB collection\n",
    "            persis_directory : Dirtectory to persist the vector DB\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_db()\n",
    "\n",
    "    \n",
    "    def _initialize_db(self):\n",
    "        \"\"\" Initialize ChromaDB client and collection \"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata = {\"description\" : \"PDF docuemt embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector DB initialized . Collection : {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection : {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector db : {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def add_documents(self,documents:List[any],embeddings :np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector db\")\n",
    "\n",
    "        # Prepare datas for chromaDB\n",
    "        ids = []\n",
    "        metadata = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            #Generate unique ID\n",
    "            doc_id = f\"doc{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #Prepare Matadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata ['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG Tutorial (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
